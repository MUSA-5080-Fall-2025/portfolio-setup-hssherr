{
  "hash": "af9f7de10e5089eb202e88b720cf9359",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 1: Census Data Quality for Policy Decisions\"\nsubtitle: \"Evaluating Data Reliability for Algorithmic Decision-Making\"\nauthor: \"Henry Sywulak-Herr\"\ndate: today\nformat: \n  html:\n    code-fold: false\n    toc: true\n    toc-location: left\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\n\n# Assignment Overview\n\n## Scenario\n\nYou are a data analyst for the **Vermont Department of Human Services**. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\n\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n## Learning Objectives\n\n- Apply dplyr functions to real census data for policy analysis\n- Evaluate data quality using margins of error \n- Connect technical analysis to algorithmic decision-making\n- Identify potential equity implications of data reliability issues\n- Create professional documentation for policy stakeholders\n\n## Submission Instructions\n\n**Submit by posting your updated portfolio link on Canvas.** Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/`\n\nMake sure to update your `_quarto.yml` navigation to include this assignment under an \"Assignments\" menu.\n\n# Part 1: Portfolio Integration\n\nCreate this assignment in your portfolio repository under an `assignments/assignment_1/` folder structure. Update your navigation menu to include:\n\n```\n- text: Assignments\n  menu:\n    - href: assignments/assignment_1/your_file_name.qmd\n      text: \"Assignment 1: Census Data Exploration\"\n```\nIf there is a special character like comma, you need use double quote mark so that the quarto can identify this as text\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages (hint: you need tidycensus, tidyverse, and knitr)\nlibrary(pacman)\np_load(tidyverse, tidycensus, knitr)\n# Set your Census API key\n\n# I already have it installed and saved in R\nkey <- Sys.getenv(\"CENSUS_API_KEY\")\n\n# Choose your state for analysis - assign it to a variable called my_state\nmy_state <- \"VT\"\n```\n:::\n\n\n**State Selection:** I chose **Vermont (VT)** for this analysis since, as a northern New England state, it has distinctive characteristics when it comes to geography and population distribution. The state's abundant natural resources have lead to scattered patterns of development in the northern portions of the state, with the largest population centers to the west along Lake Champlain in cities such as Burlington.\n\n# Part 2: County-Level Resource Assessment\n\n## 2.1 Data Retrieval\n\n**Your Task:** Use `get_acs()` to retrieve county-level data for your chosen state.\n\n**Requirements:**\n- Geography: county level\n- Variables: median household income (B19013_001) and total population (B01003_001)  \n- Year: 2022\n- Survey: acs5\n- Output format: wide\n\n**Hint:** Remember to give your variables descriptive names using the `variables = c(name = \"code\")` syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write your get_acs() code here\n\nvt_data1 <- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim <- vt_data1 %>% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %>% head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  <chr> <chr>            <dbl>       <dbl>    <dbl>    <dbl>\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA\n```\n\n\n:::\n:::\n\n\n## 2.2 Data Quality Assessment\n\n**Your Task:** Calculate margin of error percentages and create reliability categories.\n\n**Requirements:**\n- Calculate MOE percentage: (margin of error / estimate) * 100\n- Create reliability categories:\n  - High Confidence: MOE < 5%\n  - Moderate Confidence: MOE 5-10%  \n  - Low Confidence: MOE > 10%\n- Create a flag for unreliable estimates (MOE > 10%)\n\n**Hint:** Use `mutate()` with `case_when()` for the categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct <- vt_data1_trim %>%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct < 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct >= 5 & med_hh_inc_Mpct <= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct > 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability <- vt_data1_MOEpct %>% \n  group_by(med_hh_inc_conf) %>% \n  summarize(count = n())\n\nvt_data1_reliability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  <chr>               <int>\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Hint: use count() and mutate() to add percentages\n```\n:::\n\n\n## 2.3 High Uncertainty Counties\n\n**Your Task:** Identify the 5 counties with the highest MOE percentages.\n\n**Requirements:**\n- Sort by MOE percentage (highest first)\n- Select the top 5 counties\n- Display: county name, median income, margin of error, MOE percentage, reliability category\n- Format as a professional table using `kable()`\n\n**Hint:** Use `arrange()`, `slice()`, and `select()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct <- vt_data1_MOEpct %>%\n  arrange(desc(med_hh_inc_Mpct)) %>%\n  slice_head(n = 5) %>%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, col.names = c(\"County\", \"Median Household Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"), format.args = list(round(3)))\n```\n\n::: {.cell-output-display}\n\n\n|County     | Median Household Income| Margin of Error| Total Population| MOE Percent|Confidence Ranking  |\n|:----------|-----------------------:|---------------:|----------------:|-----------:|:-------------------|\n|Grand Isle |                   86639|           10729|             7335|       12.38|Low Confidence      |\n|Lamoille   |                   69886|            5846|            25977|        8.37|Moderate Confidence |\n|Essex      |                   55247|            3679|             5976|        6.66|Moderate Confidence |\n|Franklin   |                   73633|            4436|            50101|        6.02|Moderate Confidence |\n|Windham    |                   65473|            3331|            45857|        5.09|Moderate Confidence |\n\n\n:::\n:::\n\n<br>\n**Data Quality Commentary:**\n\n*[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]* <br>\n<br>\nCounties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate.\n\n# Part 3: Neighborhood-Level Analysis\n\n## 3.1 Focus Area Selection\n\n**Your Task:** Select 2-3 counties from your reliability analysis for detailed tract-level study.\n\n**Strategy:** Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties <- vt_data1_MOEpct %>% \n  group_by(med_hh_inc_conf) %>% \n  slice(1) %>% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %>% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n```\n\n::: {.cell-output-display}\n\n\n|County     | Median HH Income| Margin of Error| Total Population| MOE Percent|Confidence Ranking  |\n|:----------|----------------:|---------------:|----------------:|-----------:|:-------------------|\n|Addison    |            85870|            2958|            37434|        3.44|High Confidence     |\n|Grand Isle |            86639|           10729|             7335|       12.38|Low Confidence      |\n|Essex      |            55247|            3679|             5976|        6.66|Moderate Confidence |\n\n\n:::\n:::\n\n\n**Comment on the output:** [write something :)] <br>\n<br>\nWhile Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability.\n\n## 3.2 Tract-Level Demographics\n\n**Your Task:** Get demographic data for census tracts in your selected counties.\n\n**Requirements:**\n- Geography: tract level\n- Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)\n- Use the same state and year as before\n- Output format: wide\n- **Challenge:** You'll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define your race/ethnicity variables with descriptive names\ndemo_vars <- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties <- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 <- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep <- vt_data2 %>%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %>% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts <- vt_data2_sep %>%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)\n```\n:::\n\n\n## 3.3 Demographic Analysis\n\n**Your Task:** Analyze the demographic patterns in your selected areas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat <- vt_data2_pcts %>% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo <- vt_data2_pcts %>% \n  group_by(COUNTY) %>% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n```\n\n::: {.cell-output-display}\n\n\n|County     | Tract Count| Avg White Pct| Avg Black Pct| Avg Hispanic/Latino Pct|\n|:----------|-----------:|-------------:|-------------:|-----------------------:|\n|Addison    |          10|          91.4|        0.8869|                    2.50|\n|Essex      |           3|          94.0|        0.0335|                    1.54|\n|Grand Isle |           2|          91.0|        1.0907|                    2.00|\n\n\n:::\n:::\n\n\n# Part 4: Comprehensive Data Quality Evaluation\n\n## 4.1 MOE Analysis for Demographic Variables\n\n**Your Task:** Examine margins of error for demographic variables to see if some communities have less reliable data.\n\n**Requirements:**\n- Calculate MOE percentages for each demographic variable\n- Flag tracts where any demographic variable has MOE > 15%\n- Create summary statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct <- vt_data2_sep %>%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability <- vt_data2_MOEpct %>% \n  mutate(race_all_conf = ifelse(race_white_Mpct > 15 | race_black_Mpct > 15 | race_hispLatinoMpct > 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100\n```\n\n\n:::\n:::\n\n\n## 4.2 Pattern Analysis\n\n**Your Task:** Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats <- vt_data2_reliability %>%\n  group_by(race_all_conf) %>% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n```\n\n::: {.cell-output-display}\n\n\n| Confidence Category| Avg Tract Pop| Avg White Pct| Avg Black Pct| Avg Hispanic/Latino Pop|\n|-------------------:|-------------:|-------------:|-------------:|-----------------------:|\n|                   1|          3383|          91.7|         0.816|                    2.31|\n\n\n:::\n:::\n\n\n**Pattern Analysis:** [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\n\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont's low population outside of city centers also contributes to increased unreliability of estimates.\n\n# Part 5: Policy Recommendations\n\n## 5.1 Analysis Integration and Professional Summary\n\n**Your Task:** Write an executive summary that integrates findings from all four analyses.\n\n**Executive Summary Requirements:**\n1. **Overall Pattern Identification**: What are the systematic patterns across all your analyses?\n2. **Equity Assessment**: Which communities face the greatest risk of algorithmic bias based on your findings?\n3. **Root Cause Analysis**: What underlying factors drive both data quality issues and bias risk?\n4. **Strategic Recommendations**: What should the Department implement to address these systematic issues?\n\n**Executive Summary:**\n\n[Your integrated 4-paragraph summary here]\n\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\n\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\n\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract's sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\n\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions.\n\n\n## 6.3 Specific Recommendations\n\n**Your Task:** Create a decision framework for algorithm implementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table <- vt_data1_MOEpct %>%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %>% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n```\n\n::: {.cell-output-display}\n\n\n|County     | Median HH Income| MOE Pct|Confidence Rating   |Algorithm Recommendation                  |\n|:----------|----------------:|-------:|:-------------------|:-----------------------------------------|\n|Addison    |            85870|    3.44|High Confidence     |Safe for algorithmic decisions            |\n|Bennington |            68558|    4.23|High Confidence     |Safe for algorithmic decisions            |\n|Caledonia  |            62964|    4.34|High Confidence     |Safe for algorithmic decisions            |\n|Chittenden |            89494|    2.55|High Confidence     |Safe for algorithmic decisions            |\n|Essex      |            55247|    6.66|Moderate Confidence |Use with caution - monitor outcomes       |\n|Franklin   |            73633|    6.02|Moderate Confidence |Use with caution - monitor outcomes       |\n|Grand Isle |            86639|   12.38|Low Confidence      |Requires manual review or additional data |\n|Lamoille   |            69886|    8.37|Moderate Confidence |Use with caution - monitor outcomes       |\n|Orange     |            74534|    3.64|High Confidence     |Safe for algorithmic decisions            |\n|Orleans    |            63981|    4.16|High Confidence     |Safe for algorithmic decisions            |\n|Rutland    |            62641|    4.17|High Confidence     |Safe for algorithmic decisions            |\n|Washington |            77278|    3.75|High Confidence     |Safe for algorithmic decisions            |\n|Windham    |            65473|    5.09|Moderate Confidence |Use with caution - monitor outcomes       |\n|Windsor    |            69492|    4.24|High Confidence     |Safe for algorithmic decisions            |\n\n\n:::\n:::\n\n\n**Key Recommendations:**\n\n**Your Task:** Use your analysis results to provide specific guidance to the department.\n\n1. **Counties suitable for immediate algorithmic implementation:** [List counties with high confidence data and explain why they're appropriate]\n- Addison\n- Bennington\n- Caledonia\n- Chittenden\n- Orange\n- Orleans\n- Rutland\n- Washington\n- Windsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\n2. **Counties requiring additional oversight:** [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n- Essex\n- Franklin\n- Lamoille\n- Windham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\n3. **Counties needing alternative approaches:** [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n- Grand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates.\n\n\n## Questions for Further Investigation\n\n[List 2-3 questions that your analysis raised that you'd like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\n1. What are the underlying contributing factors for the dominance of white residents in Vermont?\n2. As a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?\n\n# Technical Notes\n\n**Data Sources:** \n- U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates\n- Retrieved via tidycensus R package on 9/28/2025\n\n**Reproducibility:** \n- All analysis conducted in R version 4.4.1\n- Census API key required for replication\n- Complete code and documentation available at: [Henry SH Portfolio Link](https://musa-5080-fall-2025.github.io/portfolio-setup-hssherr/)\n\n**Methodology Notes:**\n[Describe any decisions you made about data processing, county selection, or analytical choices that might affect reproducibility] <br>\nThe selected counties for the racial composition analysis were isolated by slicing the first instance of each confidence ranking.\n\n**Limitations:**\n[Note any limitations in your analysis - sample size issues, geographic scope, temporal factors, etc.] <br>\nThis analysis only considers a single year of data from the ACS5 and does not examine adjacent years to see if they also display similar uncertainties in their estimates for the assessed variables.\n\n## Submission Checklist\n\nBefore submitting your portfolio link on Canvas:\n\n- [ ] All code chunks run without errors\n- [ ] All \"[Fill this in]\" prompts have been completed\n- [ ] Tables are properly formatted and readable\n- [ ] Executive summary addresses all four required components\n- [ ] Portfolio navigation includes this assignment\n- [ ] Census API key is properly set \n- [ ] Document renders correctly to HTML\n\n**Remember:** Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/your_file_name.html`",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}