{
  "hash": "d132996bc2a6398b76c41d7964895a99",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Henry Sywulak-Herr\"\ndate: \"2025-11-30\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\n## Load Packages & Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load all packages for analysis\nlibrary(pacman)\np_load(tidyverse, lubridate, readxl,\n       sf, tigris, tidycensus, osmdata,\n       riem,\n       viridis, gridExtra, knitr, kableExtra, patchwork)\n\ncolors <- c(\"#93d500\", \"#0082ca\", \"#002169\", \"#e6e6e6\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Load Trips Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save path to data files\npath <- \"./data/indego-trips/\"\n\n# load all trip csvs for the four quarters\nindego <- lapply(list.files(path, full.names = T),\n                 function(x) {\n                   data <- read_csv(file = x, show_col_types = F)\n                   data <- data %>%\n                     mutate(quarter = sub(\".*-(q[0-9])\\\\.csv$\", \"\\\\1\", x))\n                   return(data)\n                   }\n                 ) %>% do.call(rbind, .)\n\noriginal_trip_total <- nrow(indego)\n```\n:::\n\n\nFrom past experience working with Indego bikeshare data and referencing\nthe active station table from their data portal\n([link](https://www.rideindego.com/wp-content/uploads/2025/10/indego-stations-2025-10-01.csv)),\nthere are many trips that are logged as starting/ending at what's known\nas a **virtual station**, which \"is used by staff to check in or check\nout a bike remotely for a special event or in a situation in which a\nbike could not otherwise be checked in or out to a station.\" These types\nof trips are edge cases and do not have associated spatial coordinates\nto associate them with a station point, making them useless for a\nspatial analysis. The below code checks for this pattern and eliminates\nthose trips from the dataset.\n\n### Check Virtual Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify where NAs are coming from in the dataset\nna_counts <- colSums((is.na(indego)))\nna_counts[na_counts > 0] %>% \n  as.data.frame() %>%\n  kable(col.names = c(\"Column\", \"Count\"))\n```\n\n::: {.cell-output-display}\n\n\n|Column    | Count|\n|:---------|-----:|\n|start_lat |    93|\n|start_lon |    93|\n|end_lat   | 21562|\n|end_lon   | 21562|\n\n\n:::\n\n```{.r .cell-code}\n# Confirm that station ID 3000 is always associated with NAs for lat/lon\nstation_3000 <- indego %>% \n  filter((start_station == 3000 | end_station == 3000))\n\ncat(\"Count of Station ID 3000 Trips\",\n    nrow(station_3000))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCount of Station ID 3000 Trips 21768\n```\n\n\n:::\n:::\n\n\nThe vast majority of NA coordinates for station 3000 confirms that this\nis primarily a method for Indego to check abandoned or improperly docked\nbikes back into the system. Therefore, all trips that involve station\n3000 will be removed from the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter out trips starting or ending from station ID 3000\nindego <- indego %>% \n  filter(start_station != 3000 & end_station != 3000)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\\n\\n\",\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Percent of Entries Removed: 1.48%\n```\n\n\n:::\n\n```{.r .cell-code}\n# check for nas across all columns within the entire dataset\nifelse(anyNA(indego), \"NAs present\", \"No NAs present in the dataset\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"No NAs present in the dataset\"\n```\n\n\n:::\n:::\n\n\n### Calculate Time Bins\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>% \n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    year = year(interval60),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of trips\ncat(\"Annual Trip Count:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnnual Trip Count: 1449210 \n```\n\n\n:::\n\n```{.r .cell-code}\n# date range\ncat(\"Date range:\", \n    as.character(min(mdy_hm(indego$start_time))), \"to\", \n    as.character(max(mdy_hm(indego$start_time))), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 2024-01-01 00:04:00 to 2025-03-31 23:48:00 \n```\n\n\n:::\n\n```{.r .cell-code}\n# unique station ids across both start and end station columns\nunique_stations <- unique(c(indego$start_station, indego$end_station))\ncat(\"Unique stations:\",  length(unique_stations), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique stations: 283 \n```\n\n\n:::\n\n```{.r .cell-code}\n# trip types\nkable(table(indego$trip_route_category),\n      col.names = c(\"Trip Type\", \"Count\"))\n```\n\n::: {.cell-output-display}\n\n\n|Trip Type  |   Count|\n|:----------|-------:|\n|One Way    | 1357754|\n|Round Trip |   91456|\n\n\n:::\n\n```{.r .cell-code}\n# passholder types\nkable(table(indego$passholder_type),\n      col.names = c(\"Passholder Type\", \"Count\"))\n```\n\n::: {.cell-output-display}\n\n\n|Passholder Type |  Count|\n|:---------------|------:|\n|Day Pass        |  65935|\n|Indego30        | 828446|\n|Indego365       | 516040|\n|IndegoFlex      |      4|\n|NULL            |   1146|\n|Walk-up         |  37639|\n\n\n:::\n\n```{.r .cell-code}\nbiketypes <- round(table(indego$bike_type)*100 / sum(table(indego$bike_type)), 1)\nkable(biketypes,\n      col.names = c(\"Bike Type\", \"Percent\"))\n```\n\n::: {.cell-output-display}\n\n\n|Bike Type | Percent|\n|:---------|-------:|\n|electric  |    59.2|\n|standard  |    40.8|\n\n\n:::\n:::\n\n\n### Trip Duration Analysis (2024)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarize trip duration by week\nweekly_summary <- indego %>%\n  filter(year == 2024) %>% \n  group_by(week, month) %>% \n  summarise(mean_dur = mean(duration),\n            median_dur = median(duration),\n            min_dur = min(duration),\n            max_dur = max(duration)) %>% \n  pivot_longer(cols = colnames(.)[-c(1:2)], names_to = \"var\", values_to = \"value\")\n\n# create a faceted plot of summary statistics\nggplot() +\n  geom_line(data = weekly_summary, \n            mapping = aes(x = week, y = value),\n            color = colors[1],\n            linewidth = 1) +\n  facet_wrap(~var, scales = \"free_y\") +\n  labs(title = \"Weekly Summary Statistics of Trip Duration\", x = \"Week (1-53)\", y = \"Value\") +\n  scale_x_continuous(breaks = seq(min(indego$week), max(indego$week), 4)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/trip-duration-1.png){width=672}\n:::\n:::\n\n\nTrip duration across 2024 ranged from just 1 minute to 1,440 minutes (24\nhours), which likely correspond to the minimum and maximum time the\nbikeshare system is able to register. These trips could be the result of\npeople removing and quickly re-docking bikes at the same station for\nvarious reasons, or people forgetting to dock or incorrectly docking\ntheir bike. This indicates that there are likely some erroneous trip\nlogs in the data that need to be cleaned.\n\nMean and median duration follow the seasonal pattern of trip counts,\nwith longer trips on average during summer weeks and shorter trips\nduring winter weeks. Week 22 in the month of June saw the greatest mean\nduration of bike trips of the year at 21.6 minutes, compared to weeks 2\nand 5 in January which both saw the lowest mean duration of 12.8\nminutes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(log(indego$duration), breaks = 50, col = colors[2], border = colors[4], \n     main = \"Histogram of Log Duration\",\n     xlab = \"Log of Duration in Minutes\",\n     xlim = c(0,8))\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/duration-plots-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# x-value of the right side of distribution is roughly 5\n# e^5 is equal to 150, or 2.5 hours trip duration\n\ncumulative_pcts <- data.frame(hour = seq(1:24),\n                              cumulative_pct = sapply(seq(1:24), function(x){\n                                round(sum(indego$duration <= x*60)/nrow(indego)*100, 2)\n                                }))\n\nggplot(data = cumulative_pcts) +\n  geom_line(aes(x = hour, y = cumulative_pct), \n            colour = colors[3],\n            linewidth = 1) +\n  labs(title = \"Cumulative Percent of Trips Within a Given Duration\",\n       x = \"Duration (hrs)\",\n       y = \"Cumulative Pct\") +\n  scale_x_continuous(breaks = seq(0, 24, 2)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/duration-plots-2.png){width=672}\n:::\n:::\n\n\nIndego bikeshare passes allow for unlimited 30- or 60-minute rides, but\nexceeding an hour in a single trip incurs a cost of 20-30 cents per\nminute extra. Given that bike trips with a duration less than 3 hours\naccount for 99.5% of trips, with 99.9% of trips accounted for at 12\nhours, it would be reasonable to suggest that any trip length beyond 12\nhours (720 minutes) is well beyond the expected use case of Indego's\nbikeshare network and should be considered an outlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove trips greater than 12 hours in duration\nindego <- indego %>% \n  filter(duration < 720)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Percent of Entries Removed: 1.59%\n```\n\n\n:::\n:::\n\n\n### Redockings\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify which trips started and ended at the same station\n# note that this is a sanity check, all should have trip_route_category == \"Round Trip\"\nround_trips <- indego %>% \n  filter(start_station == end_station)\ntable(round_trips$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRound Trip \n     91156 \n```\n\n\n:::\n\n```{.r .cell-code}\nround_trips_short <- round_trips %>% \n  filter(duration < 30)\n\nhist(round_trips_short$duration,\n     breaks = 30,\n     col = colors[2],\n     border = colors[4],\n     main = \"Histogram of Round Trip Durations (<30 min trips only)\",\n     xlab = \"Duration (mins)\")\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/redocking-analysis-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# simpler Empirical Cumulative Density Function (ecdf) plot used here due to lower quantity of data\necdf(round_trips_short$duration) %>%\n  plot(main = \"ECDF Plot of Round Trip Duration\",\n       xlab = \"Duration (mins)\",\n       ylab = \"CDF\")\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/redocking-analysis-2.png){width=672}\n:::\n\n```{.r .cell-code}\ncat(\"Count of Round Trips Lasting 1 Minute: \",\n    sum(round_trips$duration == 1),\n    \"\\n\\nPercent of Round Trips Lasting 1 Minute: \",\n    sum(round_trips$duration == 1)/nrow(round_trips),\n    \"\\n\\n1-Minute Round Trips - Percent of Original Trip Count: \",\n    sum(round_trips$duration == 1)/original_trip_total*100,\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCount of Round Trips Lasting 1 Minute: 32726\n\nPercent of Round Trips Lasting 1 Minute: 0.3590109\n\n1-Minute Round Trips - Percent of Original Trip Count: 2.224778\n```\n\n\n:::\n:::\n\n\nBased on an Empirical Cumulative Density Function (ECDF) plot, round\ntrips lasting one minute or less approximately constitute a majority\n(\\~50%) of trips lasting less than 30 minutes. This category of trip\nalso accounts for approximately **2.2% of the original trips in the\ndataset**. This is a significant amount of trips that could end up\nskewing the final model results, since bike redockings are not\nindicative of actual bikeshare trip behavior. Redockings can often\nresult from rider confusion and can happen more than one minute after\nbeginning a trip, so out of an abundance of caution **all round trips\nless than five minutes will be removed from the dataset**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter out round trips less than 5 minutes long\nindego <- indego %>% \n  filter(!(duration < 5 & trip_route_category == \"Round Trip\"))\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Percent of Entries Removed: 4.29%\n```\n\n\n:::\n:::\n\n\n## Trip Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = colors[1], linewidth = 1) +\n  geom_smooth(se = FALSE, color = colors[3], linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - 2024\",\n    subtitle = \"Annual demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/daily-trip-counts-1.png){width=672}\n:::\n:::\n\n\nAnnual ridership peaks at the end of summer, between the months of July\nand October. The least number of trips occur during winter months, such\nas January, declining sharply between October and January while rising\nat a slower rate from January to April. If multiple years of bike trips\nwere plotted, they would likely demonstrate a sinusoidal pattern, rising\nand falling based on the time of year.\n\nThere are some notable outlying moments of high/low trips, which could\nbe a result of major events (major sporting events or public\ngatherings), unseasonable temperatures (an extremely hot summer day),\netc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/hourly-patterns-1.png){width=672}\n:::\n:::\n\n\nPeak hourly mean bikeshare usage broadly occurs during weekdays around\n7-8am and 5-6pm, demonstrating a correlation with commuting hours.\nMeanwhile, weekend usage has no sharp peaks in usage and instead\nsmoothly peaks around midday hours, while also displaying greater\nbikeshare utilization around midnight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 25,163 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 20,128 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 17,159 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 16,987 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 16,142 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 15,681 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 15,656 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 15,108 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 14,939 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 14,793 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 14,486 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 14,303 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 14,259 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 13,982 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 13,674 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 13,649 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 13,559 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,052 </td>\n   <td style=\"text-align:right;\"> 39.94732 </td>\n   <td style=\"text-align:right;\"> -75.15695 </td>\n   <td style=\"text-align:right;\"> 13,517 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 13,442 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 13,200 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Load Census Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphl_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = F\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 2272)\n```\n:::\n\n\nPerform a spatial analysis to make sure all trips are within Philly to\nmatch spatial data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a unified philadelphia geometry\nphl_boundary <- phl_census %>% select(-everything()) %>% \n  st_union() %>% \n  st_as_sf() %>%\n  st_transform(2272)\n\n# create stations point shapefile \nstn_points <- indego %>% \n  select(start_station, start_lat, start_lon) %>% \n  group_by(start_station) %>% \n  slice_head(n=1) %>% \n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326) %>% \n  st_transform(2272) %>%\n  cbind(in_phl = lengths(st_within(., phl_boundary)))\n\n# one station is not within the boundaries of philadelphia\ntable(stn_points$in_phl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1 \n  1 282 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify the id number of the station outside philly\nnon_phl_station <- stn_points[stn_points$in_phl == 0, \"start_station\"] %>% \n  st_drop_geometry() %>% \n  as.numeric()\n\n# filter out trips to/from this station\nindego <- indego %>%\n  filter(start_station != non_phl_station & end_station != non_phl_station)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Percent of Entries Removed: 4.29%\n```\n\n\n:::\n\n```{.r .cell-code}\n# filter this station from the stn_points sf\nstn_points_filt <- stn_points %>% \n  filter(in_phl > 0)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize stations \nggplot() +\n  geom_sf(data = phl_census, \n          color = \"grey80\",\n          fill = \"grey95\") +\n  geom_sf(data = phl_boundary,\n          color = \"grey60\",\n          fill = \"transparent\") +\n  geom_sf(data = stn_points_filt,\n          color = colors[2],\n          shape = 18,\n          alpha = 0.5) +\n  labs(title = \"Indego Bikeshare Stations in Philadelphia\") +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/visualize-stations-in-phl-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = phl_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_sf(\n    data = stn_points_filt,\n    color = \"white\", size = 0.25\n  ) +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/map-census-vars-1.png){width=672}\n:::\n:::\n\n\nMedian household income is not available for all census tracts. However,\nthose that are unavailable seem to be tracts where a large concentration\nof commercial or industrial activity is taking place and the majority of\nthem have no reported population. This will be explored more in the\nfollowing section.\n\n## Investigate Removing Trips To/From Non-Residential Tracts\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphl_tracts_valid <- phl_census[complete.cases(st_drop_geometry(phl_census)), \"GEOID\"]\n\nstn_valid <- stn_points %>% st_filter(phl_tracts_valid, .predicate = st_within)\n\nggplot() +\n  geom_sf(data = phl_tracts_valid, color = \"grey80\", fill = \"grey95\") +\n  # Stations \n  geom_sf(\n    data = stn_points_filt,\n    aes(color = \"Non-Residential\"),\n    size = 1,\n    alpha = 0.5\n  ) +\n    geom_sf(\n    data = stn_valid,\n    aes(color = \"Residential\"), \n    size = 1,\n    alpha = 0.5\n  ) +\n  scale_color_manual(name = \"Station Type\",\n                     breaks = c(\"Residential\", \"Non-Residential\"),\n                     values = c(\"Residential\" = \"blue\", \"Non-Residential\" = \"red\")) +\n  labs(title = \"Types of Stations Based on Availability of Census Data\") +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/check-stations-removed-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego_valid <- indego %>% \n  filter(start_station %in% stn_valid$start_station & \n           end_station %in% stn_valid$start_station)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego_valid)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCumulative Percent of Entries Removed: 25.88%\n```\n\n\n:::\n\n```{.r .cell-code}\nkable(colSums(is.na(phl_census)), col.names = c(\"Variable\", \"NA Count\"))\n```\n\n::: {.cell-output-display}\n\n\n|Variable               | NA Count|\n|:----------------------|--------:|\n|GEOID                  |        0|\n|NAME                   |        0|\n|Total_Pop              |        0|\n|B01003_001M            |        0|\n|Med_Inc                |       25|\n|B19013_001M            |       25|\n|Total_Commuters        |        0|\n|B08301_001M            |        0|\n|Transit_Commuters      |        0|\n|B08301_010M            |        0|\n|White_Pop              |        0|\n|B02001_002M            |        0|\n|Med_Home_Value         |       32|\n|B25077_001M            |       32|\n|geometry               |        0|\n|Percent_Taking_Transit |       19|\n|Percent_White          |       17|\n\n\n:::\n:::\n\n\nNon-residential tracts are typically located in areas of predominately\ncommercial, business, or industrical activity. This includes several\ncensus tracts around the city such as the Northeast Airport, University\nCity, Center City, and South Philadelphia. In total, 28 stations (10.3%\nof non-virtual stations within Philadelphia) were filtered out of the\ndataset. When filtering out trips that started or ended at these\nstations, the cumulative percent of trips eliminated from the dataset\njumps from 4.23% to 25.8%, a massive increase in trips lost.\n\nAt the risk of predicting residential trips poorer, previously\nidentified problem variables with a large amount of NAs such as median\nhousehold income and median home value will not be included in the model\nas is. Instead, median household income NAs for the 28 tracts in\nquestion will be reassigned values of 0 as an indicator that they are\nnot residential. Percent taking transit and percent white variables will\nalso get assigned zeros, since they were calculated and NAs are a result\nof divide by zero errors. Other economic indicators of an area such as\nbusiness density will be calculated and utilized instead to supplement\nthese in the improved model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select intended census variables and replace NAs across the whole dataframe\nphl_census_select <- phl_census %>% \n  select(GEOID, Total_Pop, Med_Inc, Percent_Taking_Transit, Percent_White) %>% \n  mutate(., across(everything(), ~ifelse(is.na(.), 0, .)))\n```\n:::\n\n\n## Load Additional Variables\n\n### Holidays\n\nDays where bikeshare usage might be higher or lower could correspond\nwith holidays, where people either have time off and don't need to\ncommute or, conversely, major events could draw a lot of people to\nutilize the network to access them. National holidays and observances\nwere taken from\n[timeanddate.com](https://www.timeanddate.com/holidays/us/2024?hol=25)\nwhile ChatGPT was consulted to aggregate multiple festival websites and\ndevelop a list of festivals, sporting events, and concerts that took\nplace within Philadelphia in 2024.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load holiday xlsx sheets\nholidays_ntnl <- read_xlsx(path = \"./data/holidays.xlsx\",\n                           sheet = \"national_holidays\")\nholidays_phl <- read_xlsx(path = \"./data/holidays.xlsx\",\n                          sheet = \"phl_events\")\n\n# combine holiday dates and types into a single dataframe\nholidays_all <- rbind(holidays_ntnl %>% select(date, type),\n                      holidays_phl %>% select(date, type)) %>%\n  mutate(date = as.Date(date)) %>%\n  arrange(date) %>% \n  distinct(date, .keep_all = T)\n```\n:::\n\n\n### Weather\n\nComfort is an incredibly influential factor when choosing to take\ncertain modes of transportation, and poor weather in particular can be a\nprominent reasons people choose to take public transportation over\nactive transportation. Weather data will be acquired from the\nPhiladelphia International Airport (PHL) weather station due to its\nproximity to Center City Philadelphia. Issues with the API call due to a\ncorrupted CSV cell in the \"metar\" column for March 16th, 2024 (confirmed\nby directly downloading the data from [Iowa Environmental\nMesonet](https://mesonet.agron.iastate.edu/request/download.phtml?network=PA_ASOS))\nrequired eliminating that variable from the call and specifying only\nvariables that are intended to be utilized in the modeling process.\n\nSome measurements were taken less than 1 hr apart and had the same\nmeasurements for the selected weather variables when binned to the hour\n(i.e. if two measurements at 12:03pm and 12:58pm had the same\ntemperature, when binned to 12:00pm they would be duplicated). These\nduplicates were removed. While no hours were missing from the dataset,\nsome rows had NA results for wind speed that were inferred from the\nprevious hour. Several rows exist per hour (n = 11127), so mean weather\nconditions for each hour were then calculated for each of the 8784 hours\nin the year (for 2024, a leap year: 366 days \\* 24 hours/day = 8784\nhours).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get weather data from PHL airport station from Jan 1, 2024 to Dec 31, 2024\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-01-01\",\n  date_end = \"2025-04-01\", \n  data = c('tmpf', 'dwpf', 'relh', 'drct',\n           'sknt', 'p01i', 'alti', 'vsby',\n           'gust', 'wxcodes', 'feel')\n  )\n\n\n# bin to the hour, replace nas, and remove duplicated rows\nweather_processed <- weather_data %>% \n  mutate(interval60 = floor_date(valid, unit = \"hour\"),\n         temp = tmpf,\n         prec = ifelse(is.na(p01i), 0, p01i),\n         wspd = sknt,\n         gust = ifelse(is.na(gust), 0, gust)) %>% \n  select(interval60, temp, feel, prec, relh, wspd, gust) %>% \n  distinct()\n\ncat(\"Number of records after processing:\", nrow(weather_processed),\n    \"\\n\\nNumber of expected records:\", 366*24, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of records after processing: 13576 \n\nNumber of expected records: 8784 \n```\n\n\n:::\n\n```{.r .cell-code}\n# check for missing hours and interpolate values if necessary\nweather_complete <- weather_processed %>% \n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>% \n  fill(temp, feel, prec, relh, wspd, gust, .direction = \"down\") %>%\n  group_by(interval60) %>% \n  summarise(across(everything(), mean))\n\n# check summary statistics per column\nsummary(weather_complete %>% select(-interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      temp            feel             prec               relh       \n Min.   :10.00   Min.   : -5.65   Min.   :0.000000   Min.   : 16.64  \n 1st Qu.:40.00   1st Qu.: 34.73   1st Qu.:0.000000   1st Qu.: 45.05  \n Median :53.00   Median : 53.00   Median :0.000000   Median : 60.19  \n Mean   :54.79   Mean   : 52.39   Mean   :0.002949   Mean   : 61.74  \n 3rd Qu.:70.00   3rd Qu.: 70.00   3rd Qu.:0.000000   3rd Qu.: 78.94  \n Max.   :98.00   Max.   :107.23   Max.   :0.850000   Max.   :100.00  \n      wspd            gust       \n Min.   : 0.00   Min.   : 0.000  \n 1st Qu.: 5.00   1st Qu.: 0.000  \n Median : 7.00   Median : 0.000  \n Mean   : 7.84   Mean   : 3.197  \n 3rd Qu.:10.00   3rd Qu.: 0.000  \n Max.   :32.00   Max.   :55.000  \n```\n\n\n:::\n\n```{.r .cell-code}\n# precipitation has a suspiciously low median and mean, isolate and confirm zero count\ncat(\"Percent of precipitation records equal to zero:\",\n    round(sum(weather_complete$prec == 0)/nrow(weather_complete)*100, 1), \"%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPercent of precipitation records equal to zero: 87.9 %\n```\n\n\n:::\n:::\n\n\n87.9% of the hourly weather records have precipitation values equal to\nzero. While a heavy rainstorm would likely reduce bikeshare usage more\nthan a slight drizzle, for the purposes of this modeling exercise\nsomething like relative humidity and wind speed would together provide\nan indication of the strength of a storm, while precipitation can be\nrecoded to a dummy variable indicating whether it is raining or not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a recoded column indicating whether it is raining or not\nweather_complete <- weather_complete %>% \n  mutate(rain = ifelse(prec > 0, 1, 0) %>% as.factor())\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_long <- weather_complete %>%\n  mutate(rain = as.numeric(rain)-1) %>% \n  pivot_longer(cols = -c(\"interval60\", \"prec\"),\n               names_to = \"var\", values_to = \"val\")\n\n# establish labels for faceted plot\nweather_labels <- c(\n  feel = \"Feels Like Temp (°F)\",\n  gust = \"Wind Gust (mph)\",\n  relh = \"Rel Humidity (%)\",\n  temp = \"Temp (°F)\",\n  wspd = \"Wind Speed (mph)\",\n  rain = \"Raining? (1=yes, 0=no)\"\n)\n\nggplot() +\n  geom_line(data = weather_long,\n            mapping = aes(x = interval60, y = val),\n            color = colors[1]) +\n  facet_wrap(facets = ~var, \n             scales = \"free_y\",\n             nrow = 3,\n             labeller = as_labeller(weather_labels)) +\n  labs(title = \"Weather Patterns for Philadelphia (2024)\",\n       x = \"Month\",\n       y = \"Value\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/plot-weather-patterns-1.png){width=672}\n:::\n\n```{.r .cell-code}\nboxplot(relh ~ rain, \n        data = weather_complete,\n        xlab = \"Raining? (1 = yes, 0 = no)\",\n        ylab = \"Relative Humidity (%)\",\n        main = \"Relative Humidity Distributions based on Rain Conditions (2024)\",\n        col = colors[1], border = \"grey30\")\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/plot-weather-patterns-2.png){width=672}\n:::\n\n```{.r .cell-code}\nboxplot(wspd ~ rain, \n        data = weather_complete,\n        xlab = \"Raining? (1 = yes, 0 = no)\",\n        ylab = \"Wind Speed (mph)\",\n        main = \"Wind Speed Distributions based on Rain Conditions (2024)\",\n        col = colors[2], border = \"grey30\")\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/plot-weather-patterns-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(x = weather_complete$relh,\n     y = weather_complete$wspd,\n     pch = 16,\n     xlab = \"Relative Humidity (%)\",\n     ylab = \"Wind Speed (mph)\",\n     main = \"Wind Speed vs. Relative Humidity (2024)\",\n     col = alpha(\"black\", 0.1))\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/plot-weather-patterns-4.png){width=672}\n:::\n:::\n\n\nWeather patterns are typically seasonal, with temperatures rising in the\nsummer months and falling into winter months. Relative humidity, rain,\nand wind gusts do not seems to have any discernible relationship based\non their line plots, but creating box plots of relative humidity and\nwind speed distributions based on rain conditions reveals that relative\nhumidity is generally higher during rain events, while wind speed does\nnot have any significant differences. Wind speed and relative humidity\nsimilarly do not correlate based on a scatter plot.\n\n### OSM Commercial Density\n\nIn lieu of median household income and median home values as economic\nindicators around stations in non-residential census tracts, the density\nof office and shop spaces in a census tract will be used to indicate the\nvibrancy of economic activity within them. In a city such as\nPhiladelphia, while there may be some enclaves of single-family detached\nhomes with higher-than-average incomes, the vast majority of high-value\nhomes (and therefore, high-income residents) will be located closer to\nCenter City.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import a non-exhaustive list of commercial, retail, and office businesses from OSM\noffices <- opq(st_bbox(phl_boundary %>% st_transform(4326))) %>%\n  add_osm_feature(key = \"office\") %>% \n  osmdata_sf(.)\n\nshops <- opq(st_bbox(phl_boundary %>% st_transform(4326))) %>%\n  add_osm_feature(key = \"shop\") %>% \n  osmdata_sf(.)\n\n# extract points from osm object, transform to EPSG 2272, and filter to PHL boundary\noffices_pts <- offices[[\"osm_points\"]] %>%\n  st_transform(2272) %>% \n  st_filter(phl_boundary, .predicate = st_within) %>% \n  mutate(type = \"office\")\n\nshops_pts <- shops[[\"osm_points\"]] %>%\n  st_transform(2272) %>% \n  st_filter(phl_boundary, .predicate = st_within) %>% \n  mutate(type = \"shop\")\n\n# plot business locations\nggplot() +\n  geom_sf(data = phl_census, color = \"grey80\", fill = \"grey95\") +\n  geom_sf(data = offices_pts, aes(color = \"Offices\"), alpha = 0.25, size = 0.5) +\n  geom_sf(data = shops_pts, aes(color = \"Shops\"), alpha = 0.25, size = 0.5) +\n  scale_color_manual(name = \"Business Types\", values = c(\"Offices\" = colors[1], \"Shops\" = colors[2])) +\n  labs(title = \"Businesses in Philadelphia by Type\") +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/import-osm-data-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine businesses into one dataset and isolate census tract geometries/GEOIDs\nbusinesses <- rbind(offices_pts %>% select(osm_id, name, type),\n                  shops_pts %>% select(osm_id, name, type))\nphl_tracts <- phl_census %>% select(GEOID)\n\n# calculate business counts and densities per census tract\nbusinesses_dens <- phl_tracts %>% \n  mutate(business_cnt = lengths(st_intersects(., businesses)),\n         business_dens = as.numeric(business_cnt/st_area(.))*2.78784e+7)\n\n# plot business densitites spatially\nggplot() +\n  geom_sf(data = businesses_dens,\n          aes(fill = business_dens),\n          color = NA) +\n  scale_fill_viridis(name = \"Business Density (#/sqmi)\") +\n  labs(title = \"Census Tract Business Density in Philadelphia (2025)\") +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/plot-business-density-1.png){width=672}\n:::\n:::\n\n\nBusiness density is confirmed to be a decent metric for highlighting\nareas of high commercial activity in the city, with hot spots in Center\nCity, Passyunk, Island Ave in West Philly, and multiple shopping\ndistricts in Northeast Philly.\n\n## Create Space-Time Panel for Trips\n\n### Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego %>%\n  group_by(interval60, start_station, start_lat, start_lon) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\ncat(\"Original count of rows in Trips Panel:\", nrow(trips_panel))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal count of rows in Trips Panel: 777371\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique stations in the dataset:\", length(unique(trips_panel$start_station)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique stations in the dataset: 282\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\ncat(\"Unique hours in the dataset:\", length(unique(trips_panel$interval60)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique hours in the dataset: 10913\n```\n\n\n:::\n:::\n\n\n### Create Complete Panel Structure\n\nIn order to model over time, we need to include every hour in between\nthe start and end date of the data. This can be accomplished by figuring\nout how many stations are represented in the grouped dataframe of trip\ncounts as well as the total number of hours over the time frame (1\nyear).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# find the number of stations and hours we need to represent\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(seq(min(trips_panel$interval60), max(trips_panel$interval60), by = \"hour\"))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 3,086,208 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 777,371 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 2,308,837 \n```\n\n\n:::\n\n```{.r .cell-code}\n# join trip counts to expanded grid\nstudy_panel <- \n  expand.grid(interval60 = seq(min(trips_panel$interval60),\n                               max(trips_panel$interval60),\n                               by = \"hour\"),\n              start_station = unique(trips_panel$start_station)\n              ) %>%\n  left_join(., trips_panel %>% select(-c(start_lat, start_lon)),\n            by = c(\"interval60\", \"start_station\")) %>% \n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# get station lat and lon columns from indego df\nstn_coords <- indego %>% \n  select(start_station, start_lat, start_lon) %>% \n  group_by(start_station) %>% \n  slice_head(n=1) %>% \n  ungroup()\n  \n# get the census tract each station is in\nstn_points_filt_panel <- st_join(stn_points_filt, phl_tracts, join = st_within) %>% \n  left_join(., stn_coords,\n            by = \"start_station\") %>% \n  select(-in_phl) %>% \n  st_drop_geometry()\n\n# fill in station-level attributes and variables\nstudy_panel <- study_panel %>% \n  left_join(., stn_points_filt_panel,\n            by = \"start_station\")\n```\n:::\n\n\n### Create Time Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    dotw_simple = factor(dotw,\n                         levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0) %>% as.factor(),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0) %>% as.factor()\n  )\n```\n:::\n\n\n### Add Census Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>% \n  left_join(., phl_census_select %>% st_drop_geometry(), by = \"GEOID\")\n```\n:::\n\n\n### Add Business Density Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>% \n  left_join(., businesses_dens %>% select(-business_cnt) %>% st_drop_geometry(),\n            by = \"GEOID\")\n```\n:::\n\n\n### Add Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>% \n  left_join(weather_complete, by = \"interval60\")\n```\n:::\n\n\n### Join Holiday Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>% \n  left_join(., holidays_all, by = \"date\") %>% \n  rename(holiday = type) %>% \n  mutate(holiday = replace_na(holiday, \"none\")) %>% \n  mutate(holiday = factor(holiday,\n                          levels = c(\"none\",\n                                     \"holiday\",\n                                     \"festival\",\n                                     \"concert\",\n                                     \"sporting\")))\n```\n:::\n\n\n## Create Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags: \",\n    format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\\n\",\n    \"Percent data loss relative to original panel dataset after removing NA lags: \",\n    round((1-nrow(study_panel_complete)/nrow(study_panel))*100, 2), \"%\", sep = \"\", \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 3,079,440\n\nPercent data loss relative to original panel dataset after removing NA lags: 0.22%\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# isolate data from one week in a high ridership week to view lag trends\nexample_station <- study_panel_complete %>%\n  filter(start_station == 3328 & week == 25)\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/lag-correlations-1.png){width=672}\n:::\n:::\n\n\nRegions of the plot where overlap occurs with the current (dark blue)\nline are where lagged demand overlaps and is a good indicator of current\ndemand. For example, between June 19th and 20th has significant overlap\nbetween 1 hour and 24 hour lagged demand, while between June 20th and\n21st, 24-hour lagged demand is not a good predictor while 1-hour lagged\ndemand could be.\n\n## Temporal Train/Test Split\n\nIn order to train a model and have it predict for future data, the\ntraining dataset must have all its data points come from before the\ntesting dataset in time. Since this is an annual dataset, and we want to\ntrain/test the model on an approximate 75/25 data split, we need to\nselect all data from weeks 1-39 for the training dataset and from weeks\n40-53 (one extra due to a leap year) in 2024. The trips from Q1 2025\nalso need to be isoolated in a separate dataset to test the model's\npredictive ability and compare model statistics. In order for this to\nwork, however, the datasets cannot have stations that only have trips in\nonly one of them, and so all trip counts for these stations need to be\nfiltered out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspc_2024 <- study_panel_complete %>% filter(year(interval60) == 2024)\nspc_2025 <- study_panel_complete %>% filter(year(interval60) == 2025)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- spc_2024 %>%\n  filter(week < 40) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- spc_2024 %>%\n  filter(week >= 40) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nnew_stations <- spc_2025 %>%\n  filter(Trip_Count > 0) %>% \n  distinct(start_station) %>% \n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n# eliminate trip counts from stations that only have trips in either the train/test data\nstudy_panel_complete_filt <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n  \ncat(\"Percent data loss relative to original panel dataset after removing time-limited station counts: \",\n    round((1-nrow(study_panel_complete_filt)/nrow(study_panel_complete))*100, 2), \n    \"%\", \n    sep = \"\",\n    \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPercent data loss relative to original panel dataset after removing time-limited station counts: 10.64%\n```\n\n\n:::\n\n```{.r .cell-code}\nspc_2024_filt <- study_panel_complete_filt %>% filter(year(interval60) == 2024)\nspc_2025_filt <- study_panel_complete_filt %>% filter(year(interval60) == 2025)\n\n# NOW create train/test split\ntrain <- spc_2024_filt %>%\n  filter(week < 40)\n\ntest <- spc_2024_filt %>%\n  filter(week >= 40)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\",\n    \"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\",\n    \"Training date range:\", as.character(min(train$date)), \"to\", as.character(max(train$date)), \"\\n\",\n    \"Testing date range:\", as.character(min(test$date)), \"to\", as.character(max(test$date)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 1,645,056 \n Testing observations: 562,464 \n Training date range: 2024-01-02 to 2024-09-29 \n Testing date range: 2024-09-30 to 2024-12-31 \n```\n\n\n:::\n:::\n\n\n## Building Predictive Models\n\nThis section is devoted to crafting five linear models of increasing\ncomplexity, while also exploring the addition of additional variables\nand conversion to a poisson regression.\n\n### Model 1 - Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6636 -0.6086 -0.2272  0.1873 29.6544 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -4.381e-01  5.544e-03 -79.033  < 2e-16 ***\nas.factor(hour)1  -4.870e-02  5.831e-03  -8.351  < 2e-16 ***\nas.factor(hour)2  -6.890e-02  5.831e-03 -11.816  < 2e-16 ***\nas.factor(hour)3  -8.988e-02  5.833e-03 -15.408  < 2e-16 ***\nas.factor(hour)4  -7.797e-02  5.834e-03 -13.365  < 2e-16 ***\nas.factor(hour)5   2.170e-02  5.835e-03   3.718 0.000201 ***\nas.factor(hour)6   2.319e-01  5.837e-03  39.737  < 2e-16 ***\nas.factor(hour)7   4.666e-01  5.838e-03  79.936  < 2e-16 ***\nas.factor(hour)8   7.697e-01  5.839e-03 131.820  < 2e-16 ***\nas.factor(hour)9   5.678e-01  5.840e-03  97.232  < 2e-16 ***\nas.factor(hour)10  4.792e-01  5.839e-03  82.070  < 2e-16 ***\nas.factor(hour)11  5.066e-01  5.836e-03  86.811  < 2e-16 ***\nas.factor(hour)12  5.759e-01  5.833e-03  98.730  < 2e-16 ***\nas.factor(hour)13  5.783e-01  5.830e-03  99.183  < 2e-16 ***\nas.factor(hour)14  5.923e-01  5.830e-03 101.589  < 2e-16 ***\nas.factor(hour)15  6.818e-01  5.832e-03 116.918  < 2e-16 ***\nas.factor(hour)16  8.577e-01  5.834e-03 147.006  < 2e-16 ***\nas.factor(hour)17  1.114e+00  5.836e-03 190.926  < 2e-16 ***\nas.factor(hour)18  8.434e-01  5.838e-03 144.483  < 2e-16 ***\nas.factor(hour)19  6.192e-01  5.838e-03 106.069  < 2e-16 ***\nas.factor(hour)20  3.869e-01  5.838e-03  66.279  < 2e-16 ***\nas.factor(hour)21  2.600e-01  5.836e-03  44.562  < 2e-16 ***\nas.factor(hour)22  1.906e-01  5.834e-03  32.664  < 2e-16 ***\nas.factor(hour)23  8.924e-02  5.831e-03  15.305  < 2e-16 ***\ndotw_simple2       4.842e-02  3.164e-03  15.304  < 2e-16 ***\ndotw_simple3       4.205e-02  3.166e-03  13.284  < 2e-16 ***\ndotw_simple4       5.635e-02  3.165e-03  17.805  < 2e-16 ***\ndotw_simple5       2.052e-03  3.163e-03   0.649 0.516608    \ndotw_simple6      -7.793e-02  3.169e-03 -24.596  < 2e-16 ***\ndotw_simple7      -9.705e-02  3.166e-03 -30.652  < 2e-16 ***\ntemp               9.781e-03  4.923e-05 198.692  < 2e-16 ***\nprec              -1.112e+00  3.396e-02 -32.731  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 1645024 degrees of freedom\nMultiple R-squared:  0.1186,\tAdjusted R-squared:  0.1186 \nF-statistic:  7142 on 31 and 1645024 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThis model utilizes 0:00 (12:00am, or midnight) as the reference\ncategory for hour, with coefficients representing the relative\ndifferences in count based on whether the station is being modeled at\nthat hour. 17:00 (5:00pm) has the greatest positive difference relative\nto midnight at +1.114 trips, while 04:00 (4:00am) has the greatest\nnegative difference at -0.078 trips, indicating that midnight is an hour\nthat does not see a lot of trips to begin with. With Monday (dotw = 1)\nas the reference category for days of the week, weekdays have positive\ncoefficients while weekend days have negative coefficients, reflecting\nthe reduction in trips that takes place over weekends on average. A\n1^o^F increase in temperature corresponds with a 0.0098 unit increase in\ntrips, while a 1\" increase in precipitation results in a 1.11 unit\ndecrease in trips. Every coefficient is significant except for the\nFriday factor variable (dotw = 5). The model's adjusted R^2^ for this\nanalysis, which will serve as a baseline value, is equal to 0.1186,\nindicating that 11.9% of the variation in the data can be explained by\nthis model.\n\n### Model 2 - Add Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7703 -0.4046 -0.1197  0.0930 29.6973 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.1776479  0.0048165 -36.883  < 2e-16 ***\nas.factor(hour)1  -0.0038790  0.0050512  -0.768   0.4425    \nas.factor(hour)2   0.0082033  0.0050526   1.624   0.1045    \nas.factor(hour)3   0.0085469  0.0050560   1.690   0.0909 .  \nas.factor(hour)4   0.0298602  0.0050585   5.903 3.57e-09 ***\nas.factor(hour)5   0.1003697  0.0050607  19.833  < 2e-16 ***\nas.factor(hour)6   0.2266914  0.0050669  44.740  < 2e-16 ***\nas.factor(hour)7   0.3384933  0.0050778  66.662  < 2e-16 ***\nas.factor(hour)8   0.4821459  0.0050966  94.602  < 2e-16 ***\nas.factor(hour)9   0.2382506  0.0050891  46.816  < 2e-16 ***\nas.factor(hour)10  0.2036970  0.0050720  40.161  < 2e-16 ***\nas.factor(hour)11  0.2171165  0.0050719  42.808  < 2e-16 ***\nas.factor(hour)12  0.2850913  0.0050685  56.248  < 2e-16 ***\nas.factor(hour)13  0.2819232  0.0050683  55.624  < 2e-16 ***\nas.factor(hour)14  0.2887709  0.0050687  56.972  < 2e-16 ***\nas.factor(hour)15  0.3405637  0.0050745  67.113  < 2e-16 ***\nas.factor(hour)16  0.4411107  0.0050896  86.668  < 2e-16 ***\nas.factor(hour)17  0.5746980  0.0051173 112.304  < 2e-16 ***\nas.factor(hour)18  0.3033183  0.0051136  59.316  < 2e-16 ***\nas.factor(hour)19  0.1926855  0.0050901  37.855  < 2e-16 ***\nas.factor(hour)20  0.0536820  0.0050890  10.549  < 2e-16 ***\nas.factor(hour)21  0.0506393  0.0050688   9.990  < 2e-16 ***\nas.factor(hour)22  0.0543914  0.0050588  10.752  < 2e-16 ***\nas.factor(hour)23  0.0254161  0.0050512   5.032 4.86e-07 ***\ndotw_simple2       0.0013992  0.0027415   0.510   0.6098    \ndotw_simple3      -0.0160545  0.0027437  -5.851 4.88e-09 ***\ndotw_simple4      -0.0069138  0.0027428  -2.521   0.0117 *  \ndotw_simple5      -0.0456290  0.0027425 -16.638  < 2e-16 ***\ndotw_simple6      -0.0840093  0.0027471 -30.581  < 2e-16 ***\ndotw_simple7      -0.0715501  0.0027438 -26.077  < 2e-16 ***\ntemp               0.0033068  0.0000436  75.845  < 2e-16 ***\nprec              -0.5530430  0.0294434 -18.783  < 2e-16 ***\nlag1Hour           0.2588848  0.0007472 346.456  < 2e-16 ***\nlag3Hours          0.1100757  0.0007230 152.251  < 2e-16 ***\nlag1day            0.2894345  0.0007297 396.630  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9348 on 1645021 degrees of freedom\nMultiple R-squared:  0.3388,\tAdjusted R-squared:  0.3388 \nF-statistic: 2.479e+04 on 34 and 1645021 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe adjusted R^2^ value for model 2 is 0.3388, a significant increase\nover the previous model. This was likely due to the addition of the lag\nvariables, which represent the influence of past demand on future\nconditions (i.e. the demand at a station from 1 hour ago could be\nroughly equivalent to the demand at the station in the present).\n\n### Model 3 - Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day + \n    Med_Inc + Percent_Taking_Transit + Percent_White,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec + lag1Hour + lag3Hours + lag1day + Med_Inc + Percent_Taking_Transit + \n    Percent_White, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4881 -0.4097 -0.1200  0.1309 29.7623 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -2.448e-01  5.185e-03 -47.214  < 2e-16 ***\nas.factor(hour)1       -5.429e-03  5.037e-03  -1.078   0.2811    \nas.factor(hour)2        5.230e-03  5.038e-03   1.038   0.2992    \nas.factor(hour)3        4.514e-03  5.042e-03   0.895   0.3706    \nas.factor(hour)4        2.527e-02  5.044e-03   5.009 5.48e-07 ***\nas.factor(hour)5        9.644e-02  5.047e-03  19.110  < 2e-16 ***\nas.factor(hour)6        2.249e-01  5.053e-03  44.509  < 2e-16 ***\nas.factor(hour)7        3.401e-01  5.063e-03  67.158  < 2e-16 ***\nas.factor(hour)8        4.885e-01  5.083e-03  96.115  < 2e-16 ***\nas.factor(hour)9        2.468e-01  5.076e-03  48.628  < 2e-16 ***\nas.factor(hour)10       2.120e-01  5.058e-03  41.910  < 2e-16 ***\nas.factor(hour)11       2.273e-01  5.059e-03  44.927  < 2e-16 ***\nas.factor(hour)12       2.941e-01  5.055e-03  58.187  < 2e-16 ***\nas.factor(hour)13       2.906e-01  5.055e-03  57.480  < 2e-16 ***\nas.factor(hour)14       2.977e-01  5.055e-03  58.888  < 2e-16 ***\nas.factor(hour)15       3.509e-01  5.061e-03  69.323  < 2e-16 ***\nas.factor(hour)16       4.535e-01  5.077e-03  89.324  < 2e-16 ***\nas.factor(hour)17       5.905e-01  5.106e-03 115.662  < 2e-16 ***\nas.factor(hour)18       3.196e-01  5.102e-03  62.652  < 2e-16 ***\nas.factor(hour)19       2.069e-01  5.078e-03  40.749  < 2e-16 ***\nas.factor(hour)20       6.681e-02  5.076e-03  13.161  < 2e-16 ***\nas.factor(hour)21       5.910e-02  5.055e-03  11.692  < 2e-16 ***\nas.factor(hour)22       5.984e-02  5.045e-03  11.861  < 2e-16 ***\nas.factor(hour)23       2.775e-02  5.037e-03   5.509 3.60e-08 ***\ndotw_simple2            2.933e-03  2.734e-03   1.073   0.2833    \ndotw_simple3           -1.425e-02  2.736e-03  -5.209 1.90e-07 ***\ndotw_simple4           -4.909e-03  2.735e-03  -1.795   0.0727 .  \ndotw_simple5           -4.429e-02  2.735e-03 -16.194  < 2e-16 ***\ndotw_simple6           -8.418e-02  2.739e-03 -30.729  < 2e-16 ***\ndotw_simple7           -7.265e-02  2.736e-03 -26.552  < 2e-16 ***\ntemp                    3.533e-03  4.354e-05  81.153  < 2e-16 ***\nprec                   -5.771e-01  2.936e-02 -19.655  < 2e-16 ***\nlag1Hour                2.519e-01  7.486e-04 336.535  < 2e-16 ***\nlag3Hours               1.019e-01  7.259e-04 140.416  < 2e-16 ***\nlag1day                 2.816e-01  7.322e-04 384.589  < 2e-16 ***\nMed_Inc                 3.287e-07  2.729e-08  12.045  < 2e-16 ***\nPercent_Taking_Transit -2.739e-03  6.799e-05 -40.295  < 2e-16 ***\nPercent_White           1.759e-03  4.181e-05  42.071  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9322 on 1645018 degrees of freedom\nMultiple R-squared:  0.3425,\tAdjusted R-squared:  0.3425 \nF-statistic: 2.316e+04 on 37 and 1645018 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe improvements in adjusted R2 after including census variables was\nminimal, increasing to 0.3425. Despite their statistical significance in\nthe model, the small coefficient values for these census variables\nindicate that they have lesser explanatory power in the model compared\nto other time- and weather-based predictors.\n\n### Model 4 - Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.3637612 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.3636509 \n```\n\n\n:::\n:::\n\n\nSlightly better improvements in adjusted R^2^ indicate that station\nfixed effects could have a meaningful influence on the model, though\nthese small benefits might be outweighed by how computationally\nexpensive such a model is to run.\n\n### Model 5 - Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day + rush_hour +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.367391 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.367281 \n```\n\n\n:::\n:::\n\n\nDue to the expanded temporal range of data, the test dataset could not\ninclude all months that are represented in the train dataset. Therefore,\nthe month predictor present in the sample code had to be removed from\nconsideration for this analysis.\n\nThis model proves to not be much of an improvement (Adjusted R^2^ =\n0.367 in model 5, compared to 0.364 in model 4). It may be reasonable to\nleave this interaction term between the rush hour and weekend dummy\nvariables in the model, however, since it controls for a specific\nphenomenon that has a strong theoretical backing (rush hour effects on\nbikeshare usage are not present on the weekends).\n\n### Model 6 - Backtracking: Additional Variables\n\nTo discern the potential influence of additional variables, model 6 will\nbe an enhancement of model 3, prior to the addition of station fixed\neffects that make the model summary table difficult to parse.\n\nThe additional variables (and some variable substitutions) are as\nfollows:\n\n-   Business Density (business_dens) - the density of shops and offices\n    of various types per square mile\n-   Wind Speed (wspd) - windier days make cycling more difficult, and\n    will likely push people to other modes.\n-   Raining Indicator (rain) - substitution for precipitation, since any\n    amount of rain in the day's forecast will likely coincide with a\n    significant reduction in bikeshare usage as people choose\n    alternative modes.\n-   Event Type (holiday) - as previously discussed, holidays could\n    drastically reduce cycling usage as less people are commuting, while\n    other events such as festivals and sporting events could increase it\n    as people choose it to bypass crowded public transit or congested\n    roads. This variable was converted to a factor to analyze the effect\n    of each event type.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel6 <- lm(\n  Trip_Count ~ \n    as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +\n    temp + wspd + rain +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    business_dens + holiday,\n  data = train\n)\n\nsummary(model6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + lag1Hour + \n    lag3Hours + lag1day + temp + wspd + rain + Med_Inc + Percent_Taking_Transit + \n    Percent_White + business_dens + holiday, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4893 -0.4105 -0.1196  0.1378 29.7541 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -2.222e-01  5.487e-03 -40.503  < 2e-16 ***\nas.factor(hour)1       -5.637e-03  5.034e-03  -1.120  0.26274    \nas.factor(hour)2        5.741e-03  5.037e-03   1.140  0.25445    \nas.factor(hour)3        1.302e-03  5.044e-03   0.258  0.79635    \nas.factor(hour)4        2.286e-02  5.048e-03   4.529 5.92e-06 ***\nas.factor(hour)5        9.706e-02  5.051e-03  19.214  < 2e-16 ***\nas.factor(hour)6        2.250e-01  5.061e-03  44.453  < 2e-16 ***\nas.factor(hour)7        3.395e-01  5.070e-03  66.960  < 2e-16 ***\nas.factor(hour)8        4.887e-01  5.090e-03  96.008  < 2e-16 ***\nas.factor(hour)9        2.488e-01  5.083e-03  48.947  < 2e-16 ***\nas.factor(hour)10       2.160e-01  5.063e-03  42.662  < 2e-16 ***\nas.factor(hour)11       2.284e-01  5.058e-03  45.150  < 2e-16 ***\nas.factor(hour)12       2.961e-01  5.051e-03  58.626  < 2e-16 ***\nas.factor(hour)13       2.938e-01  5.051e-03  58.167  < 2e-16 ***\nas.factor(hour)14       3.006e-01  5.052e-03  59.496  < 2e-16 ***\nas.factor(hour)15       3.545e-01  5.060e-03  70.059  < 2e-16 ***\nas.factor(hour)16       4.588e-01  5.079e-03  90.340  < 2e-16 ***\nas.factor(hour)17       5.951e-01  5.112e-03 116.412  < 2e-16 ***\nas.factor(hour)18       3.270e-01  5.115e-03  63.926  < 2e-16 ***\nas.factor(hour)19       2.141e-01  5.092e-03  42.049  < 2e-16 ***\nas.factor(hour)20       7.333e-02  5.090e-03  14.406  < 2e-16 ***\nas.factor(hour)21       6.531e-02  5.067e-03  12.890  < 2e-16 ***\nas.factor(hour)22       6.377e-02  5.049e-03  12.629  < 2e-16 ***\nas.factor(hour)23       2.986e-02  5.036e-03   5.930 3.04e-09 ***\ndotw_simple2            7.647e-04  2.781e-03   0.275  0.78335    \ndotw_simple3           -1.149e-02  2.774e-03  -4.143 3.42e-05 ***\ndotw_simple4           -4.536e-03  2.773e-03  -1.636  0.10193    \ndotw_simple5           -4.416e-02  2.807e-03 -15.733  < 2e-16 ***\ndotw_simple6           -8.201e-02  2.787e-03 -29.428  < 2e-16 ***\ndotw_simple7           -6.846e-02  2.770e-03 -24.710  < 2e-16 ***\nlag1Hour                2.499e-01  7.489e-04 333.722  < 2e-16 ***\nlag3Hours               9.981e-02  7.263e-04 137.428  < 2e-16 ***\nlag1day                 2.813e-01  7.323e-04 384.157  < 2e-16 ***\ntemp                    3.376e-03  4.454e-05  75.791  < 2e-16 ***\nwspd                   -1.720e-04  1.749e-04  -0.983  0.32536    \nrain1                  -9.627e-02  2.196e-03 -43.847  < 2e-16 ***\nMed_Inc                 2.357e-07  2.744e-08   8.590  < 2e-16 ***\nPercent_Taking_Transit -3.068e-03  6.868e-05 -44.663  < 2e-16 ***\nPercent_White           1.644e-03  4.197e-05  39.176  < 2e-16 ***\nbusiness_dens           8.262e-05  2.685e-06  30.773  < 2e-16 ***\nholidayholiday         -4.024e-02  3.107e-03 -12.950  < 2e-16 ***\nholidayfestival        -8.546e-03  3.103e-03  -2.754  0.00589 ** \nholidayconcert          2.214e-03  3.637e-03   0.609  0.54259    \nholidaysporting         3.700e-02  4.163e-03   8.889  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9313 on 1645012 degrees of freedom\nMultiple R-squared:  0.3437,\tAdjusted R-squared:  0.3437 \nF-statistic: 2.003e+04 on 43 and 1645012 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nUnfortunately, the results of adding additional variables to model 3 did\nnot result in a strong increase in model performance, according to R2\nvalues (model 6 R2 = 0.3437, model 3 R2 = 0.3425). The rain indicator\nvariable, which was theorized to be an improvement on the continuous\nprecipitation variable, had a significantly reduced coefficient value\n(model 6 \"rain\" estimate = -0.096, model 3 \"prec\" estimate = -0.57),\nthough both are significant. The holiday variables demonstrated some\npredictive value, though the coefficient for concert dates was not a\nsignificant predictor (p = 0.54).\n\n### Model 7 - Poisson Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel7 <- glm(\n  Trip_Count ~ \n    as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +\n    temp + wspd + rain +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    business_dens + holiday,\n  family = \"poisson\",\n  data = train)\n\nsummary(model7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + lag1Hour + \n    lag3Hours + lag1day + temp + wspd + rain + Med_Inc + Percent_Taking_Transit + \n    Percent_White + business_dens + holiday, family = \"poisson\", \n    data = train)\n\nCoefficients:\n                         Estimate Std. Error  z value Pr(>|z|)    \n(Intercept)            -2.863e+00  1.150e-02 -248.853  < 2e-16 ***\nas.factor(hour)1       -4.520e-01  1.543e-02  -29.299  < 2e-16 ***\nas.factor(hour)2       -7.527e-01  1.734e-02  -43.402  < 2e-16 ***\nas.factor(hour)3       -1.350e+00  2.207e-02  -61.168  < 2e-16 ***\nas.factor(hour)4       -1.202e+00  2.102e-02  -57.195  < 2e-16 ***\nas.factor(hour)5       -1.793e-02  1.404e-02   -1.277  0.20144    \nas.factor(hour)6        8.510e-01  1.150e-02   73.971  < 2e-16 ***\nas.factor(hour)7        1.276e+00  1.073e-02  118.867  < 2e-16 ***\nas.factor(hour)8        1.556e+00  1.034e-02  150.523  < 2e-16 ***\nas.factor(hour)9        1.254e+00  1.057e-02  118.621  < 2e-16 ***\nas.factor(hour)10       1.164e+00  1.070e-02  108.748  < 2e-16 ***\nas.factor(hour)11       1.172e+00  1.062e-02  110.317  < 2e-16 ***\nas.factor(hour)12       1.293e+00  1.048e-02  123.419  < 2e-16 ***\nas.factor(hour)13       1.291e+00  1.044e-02  123.625  < 2e-16 ***\nas.factor(hour)14       1.287e+00  1.040e-02  123.727  < 2e-16 ***\nas.factor(hour)15       1.350e+00  1.030e-02  131.162  < 2e-16 ***\nas.factor(hour)16       1.431e+00  1.018e-02  140.590  < 2e-16 ***\nas.factor(hour)17       1.458e+00  1.013e-02  143.935  < 2e-16 ***\nas.factor(hour)18       1.239e+00  1.025e-02  120.928  < 2e-16 ***\nas.factor(hour)19       1.118e+00  1.038e-02  107.658  < 2e-16 ***\nas.factor(hour)20       8.558e-01  1.072e-02   79.864  < 2e-16 ***\nas.factor(hour)21       7.671e-01  1.098e-02   69.838  < 2e-16 ***\nas.factor(hour)22       6.671e-01  1.127e-02   59.186  < 2e-16 ***\nas.factor(hour)23       4.103e-01  1.194e-02   34.347  < 2e-16 ***\ndotw_simple2           -1.994e-02  3.913e-03   -5.096 3.47e-07 ***\ndotw_simple3           -4.258e-02  3.918e-03  -10.867  < 2e-16 ***\ndotw_simple4           -1.134e-02  3.892e-03   -2.914  0.00356 ** \ndotw_simple5           -5.819e-02  4.016e-03  -14.489  < 2e-16 ***\ndotw_simple6           -1.560e-01  4.191e-03  -37.214  < 2e-16 ***\ndotw_simple7           -1.555e-01  4.203e-03  -36.984  < 2e-16 ***\nlag1Hour                1.342e-01  5.601e-04  239.634  < 2e-16 ***\nlag3Hours               1.021e-01  6.439e-04  158.632  < 2e-16 ***\nlag1day                 1.467e-01  5.290e-04  277.311  < 2e-16 ***\ntemp                    1.134e-02  7.112e-05  159.431  < 2e-16 ***\nwspd                   -2.969e-03  2.640e-04  -11.243  < 2e-16 ***\nrain1                  -2.407e-01  3.844e-03  -62.609  < 2e-16 ***\nMed_Inc                 3.578e-07  3.746e-08    9.552  < 2e-16 ***\nPercent_Taking_Transit -1.432e-02  1.291e-04 -110.938  < 2e-16 ***\nPercent_White           8.373e-03  6.214e-05  134.749  < 2e-16 ***\nbusiness_dens           2.844e-04  3.424e-06   83.045  < 2e-16 ***\nholidayholiday         -9.309e-02  4.960e-03  -18.767  < 2e-16 ***\nholidayfestival        -2.018e-03  4.798e-03   -0.420  0.67413    \nholidayconcert         -8.411e-03  4.929e-03   -1.706  0.08796 .  \nholidaysporting         1.419e-01  6.206e-03   22.866  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2600460  on 1645055  degrees of freedom\nResidual deviance: 1663486  on 1645012  degrees of freedom\nAIC: 2831298\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test)\n  )\n\nspc_2025_filt <- spc_2025_filt %>% \n  mutate(\n    pred2 = predict(model2, newdata = spc_2025_filt),\n    pred6 = predict(model6, newdata = spc_2025_filt)\n  )\n  \n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Model 3 + Additional Variables\",\n    \"7. Poisson Regression of Model 6 Formula\",\n    \"8. Predicting for Q1 2025 - Model 2\",\n    \"9. Predicting for Q1 2025 - Model 6\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred2), na.rm = TRUE),\n    mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred6), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 6. Model 3 + Additional Variables </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 7. Poisson Regression of Model 6 Formula </td>\n   <td style=\"text-align:right;\"> 1.80 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 8. Predicting for Q1 2025 - Model 2 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 9. Predicting for Q1 2025 - Model 6 </td>\n   <td style=\"text-align:right;\"> 0.43 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# check MAE for Q1 2025\n```\n:::\n\n\nFor 2024-data-based predictions, the poisson regression model\nimmediately emerges as the worst in terms of Mean Absolute Error (MAE),\nwhich is likely due to the assumption of poisson regressions that the\nmean of the distribution equals its variance. This is not true for\nbikeshare trip counts, where the median and mean are close to zero but\nthe dataset fluctuates significantly over time. The introduction of\ntemporal lags in model 2 represent the greatest reduction in MAE across\nall linear model iterations. The addition of station fixed effects\nslightly increased MAE, highlighting how introducing complexity into the\nmodel, while increasing R2, might be a detriment to the model overall.\n\nModel 6 (model 3 + additional variables) was chosen as the model to\npredict for Q1 2025 and compare to model 2, the final model generated\nfrom the original procedure. The MAE values for both model 2 and model 6\nwere lower than when analyzing their respective predictions from the\ntest dataset for 2024. It is possible that this could be a result of the\nlimited time frame that the 2025 data represents (544,320 rows), which\nreduces the sample size of errors compared to the 2024 MAE values\n(2,207,520 rows). If either model better at predicting for late\nwinter/early spring months due to data availability patterns that were\nnot explored in this report, then this will be reflected in performing\nbetter for just those isolated months for 2025.\n\n## Space-Time Error Analysis\n\n### Observed vs. Predicted\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/observed-predicted-1.png){width=672}\n:::\n:::\n\n\nModel 2 performs best for PM Rush and Evening hours, and generally\nbetter for weekdays when compared to weekends. It tends to predict worse\nfor AM Rush and Mid-Day trips.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat, start_lon) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat), !is.na(start_lon))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = phl_census %>% st_transform(4326), fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.2\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = phl_census %>% st_transform(4326), fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = avg_demand),\n    size = 3.5,\n    alpha = 0.2\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/spatial-errors-1.png){width=672}\n:::\n:::\n\n\nClusters of high MAE values occur towards Center City, where the average\ndemand is greater. This is likely influenced by the large number of\nzeros within the dataset preventing the clean estimation of the much\nhigher ridership stations in Center City.\n\n### Temporal Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/temporal-errors-1.png){width=672}\n:::\n:::\n\n\nFor the AM Rush, Evening, and PM Rush times during weekdays, model 2 has\na higher MAE value. The same is true Mid-Day and Overnight times during\nweekends. Overnight times have the lowest MAE of all time periods, which\nis also likely due to the high amount of zeros present in the data (late\nnights have the least number of trips on average).\n\n### Error and Demographic Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(stn_points_filt_panel %>% \n              select(start_station, GEOID),\n            by = \"start_station\") %>% \n  left_join(phl_census_select %>% \n              select(GEOID, Med_Inc, Percent_Taking_Transit, Percent_White), \n            by = \"GEOID\") %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Sywulak-Herr_Henry_assignment5_files/figure-html/demographic-errors-1.png){width=672}\n:::\n:::\n\n\nWhile there are some trends when considering MAEs vs. demographic\nvariables, it's important to keep in mind that, for the most part, these\nrelationships are very slight. For median income and percent white, high\nvalues of both tended to result in higher errors, while for stations in\ncensus tracts with lower percentages of transit users error tended to\ndecrease.\n\n## Final Report\n\n**In general, these models - if used at all - should be used with\nextreme caution.** A low adjusted R2 value of 0.3388 combined with\nrelatively high MAE values of 0.53 for the top model (model 2) indicate\nthat this model does not explain the vast majority of the existing data,\nnor does it predict particularly well on average. System rebalancing of\nthe Indego bikeshare network requires accurate knowledge of where demand\nis highest and where supply of bikes is weakest. Prediction errors when\nmodeling these aspects of the network would result in an incorrect\ndistribution of bikes throughout the network. If this produces a\nparticularly bad use experience for bikeshare users, this could push\npeople away from utilizing the system.\n\nPrediction errors are loosely worse in areas with lower bikeshare usage,\nwhich here are most clearly represented as affluent, white neighborhoods\nthat likely use other modes of transportation more frequently. This is\nsupported by the analysis of spatial errors performed, highlighting\nincreased prediction errors in Center City, a far more affluent and\nwhite-dominant area of the Philadelphia. Given that Indego is\nnotoriously bad at expanding into majority-black regions of\nPhiladelphia, and therefore this model has a deficit of input data from\nthose communities, it's highly likely that they are also being\nincorrectly modeled. This could worsen bikeshare access disparities by\nfocusing resources in locations that wouldn't even use bikeshare if they\nhad access to it, while ignoring communities that could benefit from\nincreased bikeshare access much more. A future safeguard could include\nweighting any models to give more emphasis to stations in or near these\ncommunities (adding a spatial lag variable to accomplish this).\n",
    "supporting": [
      "Sywulak-Herr_Henry_assignment5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}