[
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#data-visualization",
    "href": "weekly-notes/week-03-notes.html#data-visualization",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "href": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\nWhen making a ggplot, the elements go as follows:\n\nData → what is your dataset?\nAesthetics → what variables map to visual properties?\nGeometries → how do you want to display the data?\nAdditional Layers → scales, themes, facets, annotations, etc.\nBroadly:\n\nggplot(data = your_data) +\n\naes(x = variable1, y = variable2) +\ngeom_something() +\nadditional_layers()\n\n\n\nArguments\n\nx, y → position\ncolor → point/line color\nfill → area fill color\nsize → point/line size\nshape → point shape\nalpha → transparency\nAethetics go inside aes(), constants go outside\nAesthetics are not for decorating, they are for changing data-related elements of your plot (WILL BE ON QUIZ)"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "href": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nIt’s like detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "href": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "R Tips from This Week",
    "text": "R Tips from This Week\n\nThe regex() function can generate regex expressions based on a string input."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nNew R functions or approaches\n\nfilter → subset rows\nselect → subset columns\nmutate → create new variable columns\nsummarize → perform operations on a grouped dataframe\ngroup_by → create a grouped dataframe based on a column\nrename → rename columns (use `newname`)\n%&gt;% (Piping) → tidyverse method of passing function outputs to another function\n\nQuarto features learned\n\nVarious markdown style shortcuts:\n\nBold = **\nItalic = *\nBold/Italic = ***\nCode Text = `\nLists = -\nHeaders = # (x2 or x3)\nLinks = [Link text] (link.com)\n\nRendering the webpage as we make edits\n\nChanging Global Options in RStudio to render on save\nUsing the “Render” button to force rendering updates"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nThe nuances of GitHub, but I think I’m getting most of it\n\nAreas needing more practice\n\nUsing GitHub"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nData manipulation in R will be useful for taking in data from web pages and restructuring it into a format that’ll be useful for modeling"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nLearning how to use GitHub and publish our pages\n\nHow I’ll apply this knowledge\n\nUsing GitHub for collaborative projects in the future"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables.\n[write the code below - first add a code chunk]\n\nage_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*age\"))\n\nage_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Age\"))\n\n# Median Age by Sex: B01002_001\n\n\n\n0.5 Advanced Search Techniques\nYour Task: Learn more sophisticated search methods.\n\n# Search for multiple terms at once\nhousing_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*(rent|value)\"))\n\nprint(\"Housing cost variables:\")\n\n[1] \"Housing cost variables:\"\n\nhead(housing_vars, 10)\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B07002PR_004 Estimate!!Median age --!!Total:!!Moved from d… Median… &lt;NA&gt;     \n 2 B07002_004   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 3 B07002_005   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 4 B07011PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n 5 B07011_004   Estimate!!Median income in the past 12 months… Median… tract    \n 6 B07011_005   Estimate!!Median income in the past 12 months… Median… tract    \n 7 B07402PR_004 Estimate!!Median age --!!Total living in area… Median… &lt;NA&gt;     \n 8 B07402_004   Estimate!!Median age --!!Total living in area… Median… county   \n 9 B07402_005   Estimate!!Median age --!!Total living in area… Median… county   \n10 B07411PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n\n# Search excluding certain terms\nincome_not_family &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\") & \n         !str_detect(label, \"family\"))\n\nprint(\"Income variables (not family income):\")\n\n[1] \"Income variables (not family income):\"\n\nhead(income_not_family)\n\n# A tibble: 6 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n6 B06011_001   Estimate!!Median income in the past 12 months … Median… tract    \n\n# Case-insensitive search using regex\neducation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, regex(\"bachelor\", ignore_case = TRUE)))\n\nprint(\"Education variables:\")\n\n[1] \"Education variables:\"\n\nhead(education_vars, 5)\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06009PR_005 Estimate!!Total:!!Bachelor's degree             Place … &lt;NA&gt;     \n2 B06009PR_011 Estimate!!Total:!!Born in Puerto Rico:!!Bachel… Place … &lt;NA&gt;     \n3 B06009PR_017 Estimate!!Total:!!Born in the United States:!!… Place … &lt;NA&gt;     \n4 B06009PR_023 Estimate!!Total:!!Native; born elsewhere:!!Bac… Place … &lt;NA&gt;     \n5 B06009PR_029 Estimate!!Total:!!Foreign born:!!Bachelor's de… Place … &lt;NA&gt;     \n\n\n\n\n0.6 Interactive Exploration\nYour Task: Use RStudio’s viewer for easier searching.\n\n# Open the full variable list in RStudio viewer\n# This opens a searchable data table\nView(acs_vars_2022)\n\n# Pro tip: You can also search specific table groups\n# B01 = Age and Sex\n# B19 = Income  \n# B25 = Housing\ntable_b19 &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(name, \"^B19\"))  # ^ means \"starts with\"\n\nprint(\"All B19 (Income) table variables:\")\n\n[1] \"All B19 (Income) table variables:\"\n\nhead(table_b19, 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n\n0.7 Verify Your Variable Choices\nYour Task: Test your variables by getting a small sample of data.\n\n# Test the variables you found\ntest_vars &lt;- c(\n  total_pop = \"B01003_001\",      # Total population\n  median_income = \"B19013_001\",  # Median household income\n  median_age = \"B01002_001\"      # Median age\n)\n\n# Get data for just one state to test\ntest_data &lt;- get_acs(\n  geography = \"state\",\n  variables = test_vars,\n  state = \"PA\",\n  year = 2022\n)\n\n# Check that you got what you expected\ntest_data\n\n# A tibble: 3 × 5\n  GEOID NAME         variable        estimate   moe\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 42    Pennsylvania median_age          40.8   0.1\n2 42    Pennsylvania total_pop     12989208    NA  \n3 42    Pennsylvania median_income    73170   347  \n\n\n\n\n0.8 Common Variable Patterns\nReference guide for future use:\n\n# Common patterns to remember:\ncommon_variables &lt;- tribble(\n  ~concept, ~typical_code, ~description,\n  \"Total Population\", \"B01003_001\", \"Total population\",\n  \"Median Age\", \"B01002_001\", \"Median age of population\", \n  \"Median HH Income\", \"B19013_001\", \"Median household income\",\n  \"White Population\", \"B03002_003\", \"White alone population\",\n  \"Black Population\", \"B03002_004\", \"Black/African American alone\",\n  \"Hispanic Population\", \"B03002_012\", \"Hispanic or Latino population\",\n  \"Bachelor's Degree\", \"B15003_022\", \"Bachelor's degree or higher\",\n  \"Median Rent\", \"B25058_001\", \"Median contract rent\",\n  \"Median Home Value\", \"B25077_001\", \"Median value owner-occupied\"\n)\n\nprint(\"Common Census Variables:\")\n\n[1] \"Common Census Variables:\"\n\ncommon_variables\n\n# A tibble: 9 × 3\n  concept             typical_code description                  \n  &lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;                        \n1 Total Population    B01003_001   Total population             \n2 Median Age          B01002_001   Median age of population     \n3 Median HH Income    B19013_001   Median household income      \n4 White Population    B03002_003   White alone population       \n5 Black Population    B03002_004   Black/African American alone \n6 Hispanic Population B03002_012   Hispanic or Latino population\n7 Bachelor's Degree   B15003_022   Bachelor's degree or higher  \n8 Median Rent         B25058_001   Median contract rent         \n9 Median Home Value   B25077_001   Median value owner-occupied  \n\n\nKey Tips for Variable Hunting:\n\nStart with concepts - search for the topic you want (income, age, housing)\nLook for “Median” vs “Mean” - median is usually more policy-relevant\nCheck the universe - some variables are for “households,” others for “population”\nTest with small data before running large queries\nBookmark useful variables for future projects (type them in your weekly notes!)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", \", state_choice)))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name                     median_incomeE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Bucks County, Pennsylvania              107826\n2 Chester County, Pennsylvania            118574\n3 Montgomery County, Pennsylvania         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n# histogram of total population\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 50, fill = \"#73956F\", colour = \"grey25\", alpha = 0.9) +\n  labs(\n    title = \"Distribution of Total Population\",\n    x = \"Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_boxplot(fill = \"#73956F\", , width = 0.1) +\n  labs(\n    title = \"Total Population Distribution with Outliers\",\n    x = \"Population\"\n  ) +\n  ylim(-0.25, 0.25) +\n  theme_minimal() +\n  scale_x_continuous()\n\n\n\n\n\n\n\n# Identify the outlier counties\npop_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(total_popE, 0.25, na.rm = TRUE),\n    Q3 = quantile(total_popE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = total_popE &lt; (Q1 - 1.5 * IQR) | total_popE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, total_popE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\npop_outliers\n\n# A tibble: 7 × 2\n  county_name                       total_popE\n  &lt;chr&gt;                                  &lt;dbl&gt;\n1 Allegheny County, Pennsylvania       1245310\n2 Bucks County, Pennsylvania            645163\n3 Chester County, Pennsylvania          536474\n4 Delaware County, Pennsylvania         575312\n5 Lancaster County, Pennsylvania        553202\n6 Montgomery County, Pennsylvania       856399\n7 Philadelphia County, Pennsylvania    1593208"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship? - Answer → Yes, it condenses the data and produces a much more visual trend\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\nggplot(data = county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(fill = \"red\", alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = T, color = \"grey95\") +\n  labs(title = \"Median Age vs. Median Income of Pennsylvania Counties\", \n       x = \"Median Income (k$)\",\n       y = \"Median Age (yrs)\") +\n  theme_dark() +\n  scale_x_continuous(labels = dollar)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\ncounty_reliability &lt;- county_reliability %&gt;% \n  mutate(median_ageM_pct = (median_ageM/median_ageE)*100)\n\nggplot(data = county_reliability) +\n  aes(x = total_popE, y = median_ageM_pct) +\n  geom_point(color = \"purple3\", size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 5, colour = \"lightblue3\", linetype = \"dashed\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = median_ageM_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile?\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\n\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-well-cover",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-well-cover",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Algorithmic Decision Making\n\nWhat are algorithms in public policy?\nWhen algorithmic decision making goes wrong\nCurrent policy responses\n\nPart 2: Active Learning\n\nSmall group scenarios: designing ethical algorithms\nDiscussion and reflection\n\nPart 3: Census Data Foundations\n\nUnderstanding census data for policy analysis\nGeography and data availability\n\nPart 4: Hands-On Census Data with R\n\nLive demonstration of key functions\nPractice exercises"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#opening-question",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#opening-question",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Opening Question",
    "text": "Opening Question\nDiscuss with your table (1 minutes):\nWhat is an algorithm?\nThink beyond just computer code - how do you make decisions in daily life?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-is-an-algorithm",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-is-an-algorithm",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "What Is An Algorithm?",
    "text": "What Is An Algorithm?\nDefinition: A set of rules or instructions for solving a problem or completing a task\nExamples:\n\nRecipe for cooking\nDirections to get somewhere\n\nDecision tree for hiring\nComputer program that processes data to make predictions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#algorithmic-decision-making-in-government",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#algorithmic-decision-making-in-government",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Algorithmic Decision Making in Government",
    "text": "Algorithmic Decision Making in Government\nSystems used to assist or replace human decision-makers\nBased on predictions from models that process historical data containing:\n\nInputs (“features”, “predictors”, “independent variables”, “x”)\nOutputs (“labels”, “outcome”, “dependent variable”, “y”)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#real-world-examples",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#real-world-examples",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Real-World Examples",
    "text": "Real-World Examples\n\n\nCriminal Justice Recidivism risk scores for bail and sentencing decisions\n\nHousing & Finance\nMortgage lending and tenant screening algorithms\n\nHealthcare Patient care prioritization and resource allocation"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#clarifying-key-terms",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#clarifying-key-terms",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Clarifying Key Terms",
    "text": "Clarifying Key Terms\nData Science → Computer science/engineering focus on algorithms and methods\nData Analytics → Application of data science methods to other disciplines\nMachine Learning → Algorithms for classification & prediction that learn from data\nAI → Algorithms that adjust and improve across iterations (neural networks, etc.)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#public-sector-context",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#public-sector-context",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Public Sector Context",
    "text": "Public Sector Context\nLong history of government data collection:\n\nCivic registration systems\n\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental”)\nFocus on prediction rather than explanation\nHarder to interpret and explain"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-government-uses-algorithms",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-government-uses-algorithms",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Why Government Uses Algorithms",
    "text": "Why Government Uses Algorithms\nGovernments have limited budgets and need to serve everyone\nAlgorithmic decision making is especially appealing because it promises:\n\nEfficiency - process more cases faster\nConsistency - same rules applied to everyone\n\nObjectivity - removes human bias\nCost savings - fewer staff needed\n\nBut does it deliver on these promises?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#remember-data-analytics-is-subjective",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#remember-data-analytics-is-subjective",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Remember: Data Analytics Is Subjective",
    "text": "Remember: Data Analytics Is Subjective\nEvery step involves human choices:\n\nData cleaning decisions\nData coding or classification\n\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you put in the model\n\nThese choices embed human values and biases"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#section",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#section",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Healthcare Algorithm Bias\nThe Problem:\nAlgorithm used to identify high-risk patients for additional care systematically discriminated against Black patients\nWhat Went Wrong:\n\nAlgorithm used healthcare costs as a proxy for need\nBlack patients typically incur lower costs due to systemic inequities in access\n\nResult: Black patients under-prioritized despite equivalent levels of illness\n\nScale: Used by hospitals and insurers for over 200 million people annually\nSource: Obermeyer et al. (2019), Science"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#criminal-justice-algorithm-bias",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#criminal-justice-algorithm-bias",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Criminal Justice Algorithm Bias",
    "text": "Criminal Justice Algorithm Bias\nCOMPAS Recidivism Prediction:\nThe Problem:\n\nAlgorithm 2x as likely to falsely flag Black defendants as high risk\nWhite defendants often rated low risk even when they do reoffend\n\nWhy This Happens:\n\nHistorical arrest data reflects biased policing patterns\nSocioeconomic proxies correlate with race\n“Objective” data contains subjective human decisions\n\nSource: ProPublica investigation"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#dutch-welfare-fraud-detection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#dutch-welfare-fraud-detection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Dutch Welfare Fraud Detection",
    "text": "Dutch Welfare Fraud Detection\nThe Problem:\n\n“Black box” system operated in secrecy\nImpossible for individuals to understand or challenge decisions\nDisproportionately targeted vulnerable populations\n\nCourt Ruling:\n\nBreached privacy rights under European Convention on Human Rights\nHighlighted unfair profiling and discrimination\nSystem eventually shut down"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#small-group-challenge-10-minutes-we-keep-a-tight-ship-around-here.",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#small-group-challenge-10-minutes-we-keep-a-tight-ship-around-here.",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Small Group Challenge (10 Minutes! We keep a tight ship around here.)",
    "text": "Small Group Challenge (10 Minutes! We keep a tight ship around here.)\nAt your table, pick one scenario and answer three prompts.\nPrompts (plain English, no tech):\n\nProxy: What would you use to stand in for what you want?\nBlind spot: What data gap or historical bias could skew results?\nHarm + Guardrail: Who could be harmed, and one simple safeguard?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#pick-one-scenario",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#pick-one-scenario",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Pick one scenario",
    "text": "Pick one scenario\n\nEmergency response prioritization (natural disasters)\n\nSchool enrollment assignment\n\nAutomated traffic enforcement (red-light cameras)\n\nHousing assistance allocation\n\nPredictive policing"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#example-so-you-see-the-level",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#example-so-you-see-the-level",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Example (so you see the level)",
    "text": "Example (so you see the level)\nScenario: Emergency response\n\nProxy: 911 call volume → stand-in for “need”\n\nBlind spot: Under-calling where trust/connectivity is low\n\nHarm + Guardrail: Wealthier areas over-prioritized → add a vulnerability boost (age/disability) and a minimum-service floor per zone"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#discuss-at-your-table-8-minutes",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#discuss-at-your-table-8-minutes",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Discuss at your table (8 minutes)",
    "text": "Discuss at your table (8 minutes)\nAnswer these out loud and on one device or notepad:\n\nProxy → “We’d use ____ as a stand-in for ____.”\nBlind spot → “This could miss/undercount ____ because ____.”\nHarm + Guardrail → “Group ____ could be hurt by ____. We’d add ____ (one safeguard).”\n\nChoose ONE guardrail type:\n\nPrioritize vulnerable groups\n\nCap disparities across areas (simple rule)\n\nHuman review + appeals for edge cases\n\nReplace a bad proxy (collect the right thing)\n\nPublish criteria & run a periodic bias check"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#lightning-shares-23-tables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#lightning-shares-23-tables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Lightning shares (2–3 tables)",
    "text": "Lightning shares (2–3 tables)\nIn ≤20 seconds, say:\n\nScenario, one proxy, one harm, one guardrail\n\nClass quick poll: Would that guardrail help?\n\n👍 Green light\n👎 Red light"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-census-data-matters",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-census-data-matters",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Why Census Data Matters",
    "text": "Why Census Data Matters\nCensus data is the foundation for:\n\nUnderstanding community demographics\nAllocating government resources\n\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nConnection: The same demographic data used in census goes into many of the algorithms we analyzed"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-vs.-american-community-survey",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-vs.-american-community-survey",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Census vs. American Community Survey",
    "text": "Census vs. American Community Survey\n\n\nDecennial Census (2020)\n\nEveryone counted every 10 years\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS)\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced the old “long form” in 2005\nA big source of data we’ll use this semester"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#acs-estimates-what-you-need-to-know",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#acs-estimates-what-you-need-to-know",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "ACS Estimates: What You Need to Know",
    "text": "ACS Estimates: What You Need to Know\n1-Year Estimates (areas &gt; 65,000 people)\n\nMost current data, smallest sample\n\n5-Year Estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nWhat you’ll use most often\n\nKey Point: All ACS data comes with margins of error - we’ll learn to work with uncertainty"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-geography-hierarchy",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-geography-hierarchy",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Census Geography Hierarchy",
    "text": "Census Geography Hierarchy\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at:\n\nCounty level - state and regional planning\nCensus tract level - neighborhood analysis\nBlock group level - very local analysis (tempting, but big MOEs)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-innovation-differential-privacy",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-innovation-differential-privacy",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "2020 Census Innovation: Differential Privacy",
    "text": "2020 Census Innovation: Differential Privacy\nThe Challenge: Modern computing can “re-identify” individuals from census data\nThe Solution: Add mathematical “noise” to protect privacy while preserving overall patterns\nThe Controversy: Some places now show populations living “underwater” or other impossible results\nWhy This Matters: Even “objective” data involves subjective choices about privacy vs. accuracy. Also, the errors."
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#accessing-census-data-in-r",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#accessing-census-data-in-r",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Accessing Census Data in R",
    "text": "Accessing Census Data in R\nTraditional approach: Download CSV files from Census website\nModern approach: Use R packages to access data directly\nBenefits of programmatic access:\n\nAlways get latest data\nReproducible workflows\n\nAutomatic geographic boundaries\nBuilt-in error handling\n\nWe’ll use the tidycensus package starting in Lab 1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-acs-data-structure",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-acs-data-structure",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Understanding ACS Data Structure",
    "text": "Understanding ACS Data Structure\nData organized in tables:\n\nB19013 - Median Household Income\nB25003 - Housing Tenure (Own/Rent)\n\nB15003 - Educational Attainment\nB08301 - Commuting to Work\n\nEach table has multiple variables:\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\nYou’ll learn to find the right variables for your research questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-margins-of-error",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-margins-of-error",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Working with Margins of Error",
    "text": "Working with Margins of Error\nEvery ACS estimate comes with uncertainty\nRule of thumb:\n\nLarge MOE relative to estimate = less reliable\nSmall MOE relative to estimate = more reliable\n\nIn your analysis:\n\nAlways report MOE alongside estimates\nBe cautious comparing estimates with overlapping error margins\nConsider using 5-year estimates for greater reliability"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#two-types-of-census-data",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#two-types-of-census-data",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Two Types of Census Data",
    "text": "Two Types of Census Data\n\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\n\nPUMS - Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#when-new-data-comes-out",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#when-new-data-comes-out",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "When New Data Comes Out",
    "text": "When New Data Comes Out\nACS 1-year estimates: Released in September (previous year’s data)\nACS 5-year estimates: Released in December\nDecennial Census: Released on rolling schedule over 2-3 years\nFor Lab 1: We’ll use 2018-2022 ACS 5-year estimates (latest available)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-sources-youll-use",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-sources-youll-use",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Data Sources You’ll Use",
    "text": "Data Sources You’ll Use\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org) - Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database - Track changes over time"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#live-demo-setup",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#live-demo-setup",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Live Demo Setup",
    "text": "Live Demo Setup\nLet’s see tidycensus in action with some basic examples:\n\nlibrary(tidycensus)\n\nWarning: package 'tidycensus' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n# Set API key (you'll get yours for Lab 1)\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n\nFollow along: We’ll work through examples together, then you’ll practice in Lab 1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#basic-get_acs-function",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#basic-get_acs-function",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Basic get_acs() Function",
    "text": "Basic get_acs() Function\nMost important function you’ll use:\n\n# Get state-level population data\nstate_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",  # Total population\n  year = 2022,\n  survey = \"acs5\"\n)\n\nGetting data from the 2018-2022 5-year ACS\n\nglimpse(state_pop)\n\nRows: 52\nColumns: 5\n$ GEOID    &lt;chr&gt; \"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"…\n$ NAME     &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Co…\n$ variable &lt;chr&gt; \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B010…\n$ estimate &lt;dbl&gt; 5028092, 734821, 7172282, 3018669, 39356104, 5770790, 3611317…\n$ moe      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nKey parameters: geography, variables, year, survey"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-the-output",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-the-output",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nEvery ACS result includes:\n\nGEOID - Geographic identifier\nNAME - Human-readable location name\n\nvariable - Census variable code\nestimate - The actual value\nmoe - Margin of error\n\nThis is the foundation for all your analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-multiple-variables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-multiple-variables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Working with Multiple Variables",
    "text": "Working with Multiple Variables\n\n# Get income and population for Pennsylvania counties\npa_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"  # Makes analysis easier\n)\n\nGetting data from the 2018-2022 5-year ACS\n\nhead(pa_data)\n\n# A tibble: 6 × 6\n  GEOID NAME                 total_popE total_popM median_incomeE median_incomeM\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 42001 Adams County, Penns…     104604         NA          78975           3334\n2 42003 Allegheny County, P…    1245310         NA          72537            869\n3 42005 Armstrong County, P…      65538         NA          61011           2202\n4 42007 Beaver County, Penn…     167629         NA          67194           1531\n5 42009 Bedford County, Pen…      47613         NA          58337           2606\n6 42011 Berks County, Penns…     428483         NA          74617           1191\n\n\nNote: output = \"wide\" gives you one row per place, multiple columns for variables"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-cleaning-essentials",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-cleaning-essentials",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Data Cleaning Essentials",
    "text": "Data Cleaning Essentials\nClean up messy geographic names:\n\npa_clean &lt;- pa_data %&gt;%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    # Remove \"County\" word\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Compare before and after\nselect(pa_clean, NAME, county_name)\n\n# A tibble: 67 × 2\n   NAME                           county_name\n   &lt;chr&gt;                          &lt;chr&gt;      \n 1 Adams County, Pennsylvania     Adams      \n 2 Allegheny County, Pennsylvania Allegheny  \n 3 Armstrong County, Pennsylvania Armstrong  \n 4 Beaver County, Pennsylvania    Beaver     \n 5 Bedford County, Pennsylvania   Bedford    \n 6 Berks County, Pennsylvania     Berks      \n 7 Blair County, Pennsylvania     Blair      \n 8 Bradford County, Pennsylvania  Bradford   \n 9 Bucks County, Pennsylvania     Bucks      \n10 Butler County, Pennsylvania    Butler     \n# ℹ 57 more rows\n\n\nFunctions you’ll use: str_remove(), str_extract(), str_replace()"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#calculating-data-reliability",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#calculating-data-reliability",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Calculating Data Reliability",
    "text": "Calculating Data Reliability\nThis is crucial for policy work:\n\npa_reliability &lt;- pa_clean %&gt;%\n  mutate(\n    # Calculate MOE as percentage of estimate\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 2),\n    \n    # Create reliability categories\n    reliability = case_when(\n      moe_percentage &lt; 5 ~ \"High Confidence\",\n      moe_percentage &gt;= 5 & moe_percentage &lt;= 10 ~ \"Moderate\",\n      moe_percentage &gt; 10 ~ \"Low Confidence\"\n    )\n  )\n\ncount(pa_reliability, reliability)\n\n# A tibble: 2 × 2\n  reliability         n\n  &lt;chr&gt;           &lt;int&gt;\n1 High Confidence    57\n2 Moderate           10\n\n\nKey functions: case_when() for categories, MOE calculations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#finding-patterns-with-dplyr",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#finding-patterns-with-dplyr",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Finding Patterns with dplyr",
    "text": "Finding Patterns with dplyr\n\nFind counties with highest uncertainty\nSummarize by reliability category\n\n\n# Find counties with highest uncertainty\nhigh_uncertainty &lt;- pa_reliability %&gt;%\n  filter(moe_percentage &gt; 8) %&gt;%\n  arrange(desc(moe_percentage)) %&gt;%\n  select(county_name, median_incomeE, moe_percentage)\n\n# Summary by reliability category  \nreliability_summary &lt;- pa_reliability %&gt;%\n  group_by(reliability) %&gt;%\n  summarize(\n    counties = n(),\n    avg_income = round(mean(median_incomeE, na.rm = TRUE), 0)\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#professional-tables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#professional-tables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Professional Tables",
    "text": "Professional Tables\nMaking results presentation-ready:\n\n# Create formatted table\nkable(high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"MOE %\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nCounties with Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE %\n\n\n\n\nForest\n46,188\n9.99\n\n\nSullivan\n62,910\n9.25\n\n\n\n\n\nKey points:\n\nUse kable() for professional formatting\nAdd descriptive column names and captions\n\nFormat numbers appropriately"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#quick-practice",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#quick-practice",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Quick Practice",
    "text": "Quick Practice\nTry this with a neighbor (5 minutes):\nUsing the pa_reliability data we just created:\n\nFilter for counties with “High Confidence” data\nArrange by median income (highest first)\n\nSelect county name and median income\nSlice the top 3 counties\n\nWe’ll share answers before moving on"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#policy-connection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#policy-connection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Policy Connection",
    "text": "Policy Connection\nWhy this matters:\n\nAlgorithmic fairness: Unreliable data can bias automated decisions\nResource allocation: Know which areas need extra attention\nEquity analysis: Some communities may be systematically under-counted\nProfessional credibility: Always assess your data quality\n\nThis connects directly to our algorithmic bias discussion"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#from-algorithms-to-analysis",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#from-algorithms-to-analysis",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "From Algorithms to Analysis",
    "text": "From Algorithms to Analysis\nToday’s key connections:\nAlgorithmic Decision Making → Understanding why your analysis matters for real policy decisions\nData Subjectivity → Why we emphasize transparent, reproducible methods in this class\nCensus Data → The foundation for most urban planning and policy analysis\nR Skills → The tools to do this work professionally and ethically"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions-for-reflection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions-for-reflection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Questions for Reflection",
    "text": "Questions for Reflection\nAs you work with data this semester, ask:\n\nWhat assumptions am I making in my data choices?\nWho might be excluded from my analysis?\nHow could my findings be misused if taken out of context?\nWhat would I want policymakers to understand about my methods?\n\nThese questions will make you a more thoughtful analyst and better future policymaker"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#before-next-class",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#before-next-class",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Before Next Class",
    "text": "Before Next Class\n\nComplete Lab 0 if you haven’t finished\nPost your weekly notes - reflect on today’s discussion\nStart Lab 1 - census data exploration (begins today!)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-1-course-foundation",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-1-course-foundation",
    "title": "Welcome to MUSA 5080",
    "section": "Part 1: Course Foundation",
    "text": "Part 1: Course Foundation\n\nCourse overview and philosophy\nProfessional tools we’ll use this semester\nAssessment approach and portfolio development"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-2-github-version-control",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-2-github-version-control",
    "title": "Welcome to MUSA 5080",
    "section": "Part 2: GitHub & Version Control",
    "text": "Part 2: GitHub & Version Control\n\nGit fundamentals for data science\nGitHub Classroom workflow\nCollaborative coding practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-3-reproducible-research-tools",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-3-reproducible-research-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Part 3: Reproducible Research Tools",
    "text": "Part 3: Reproducible Research Tools\n\nQuarto for professional documentation\nMarkdown basics for clear communication\nRStudio settings for reproducibility"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-4-r-project-workflow",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-4-r-project-workflow",
    "title": "Welcome to MUSA 5080",
    "section": "Part 4: R Project Workflow",
    "text": "Part 4: R Project Workflow\n\nProject organization best practices\nFile management and relative paths\nWeekly workflow you’ll follow all semester"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-5-data-analysis-with-tidyverse",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-5-data-analysis-with-tidyverse",
    "title": "Welcome to MUSA 5080",
    "section": "Part 5: Data Analysis with tidyverse",
    "text": "Part 5: Data Analysis with tidyverse\n\ndplyr fundamentals and function patterns\nPipes for readable code\ngroup_by() and summarize() for policy analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-6-hands-on-setup",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-6-hands-on-setup",
    "title": "Welcome to MUSA 5080",
    "section": "Part 6: Hands-On Setup",
    "text": "Part 6: Hands-On Setup\n\nPortfolio repository creation\nLive demonstration of complete workflow\nYour first analysis in professional format"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-this-course-is-about",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-this-course-is-about",
    "title": "Welcome to MUSA 5080",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nAdvanced spatial analysis for urban planning and public policy\nData science tools within policy context\nFocus on understanding concepts rather than just completing code\nProfessional portfolio development using modern tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#unlike-private-sector-data-science",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#unlike-private-sector-data-science",
    "title": "Welcome to MUSA 5080",
    "section": "Unlike Private Sector Data Science",
    "text": "Unlike Private Sector Data Science\n\nNot just about optimization\nPublic goods, governance, equity considerations\nTransparency and interpretability are crucial\nAlgorithmic bias has real consequences for communities"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#this-semesters-innovation",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#this-semesters-innovation",
    "title": "Welcome to MUSA 5080",
    "section": "This Semester’s Innovation",
    "text": "This Semester’s Innovation\nProblem: AI tools making it easy to complete code without understanding\nSolution:\n\n40% weekly in-class quizzes (test conceptual understanding)\nLow-stakes portfolio assignments (focus on learning, not grades)\nGitHub-based workflow (professional skills)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-tools",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Tools?",
    "text": "Why These Tools?\nGitHub: Industry standard for version control and collaboration\nQuarto: Modern approach to reproducible research and documentation\nR: Powerful for spatial analysis and policy-focused statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#professional-development",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#professional-development",
    "title": "Welcome to MUSA 5080",
    "section": "Professional Development",
    "text": "Professional Development\nThese aren’t just “class tools” - they’re career tools:\n\nPortfolio employers can see\nVersion control skills for any data job\nProfessional documentation practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-git",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-git",
    "title": "Welcome to MUSA 5080",
    "section": "What is Git?",
    "text": "What is Git?\nVersion control system that tracks changes in files\nThink of it as:\n\n“Track changes” for code projects\nTime machine for your work\nCollaboration tool for teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nCloud hosting for Git repositories\n\nBackup your work in the cloud\n\nShare projects with others\nDeploy websites (like our portfolios)\nCollaborate on code projects"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#key-github-concepts",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#key-github-concepts",
    "title": "Welcome to MUSA 5080",
    "section": "Key GitHub Concepts",
    "text": "Key GitHub Concepts\nRepository (repo): Folder containing your project files\nCommit: Snapshot of your work at a point in time\nPush: Send your changes to GitHub cloud\nPull: Get latest changes from GitHub cloud"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#github-in-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#github-in-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "GitHub in This Course",
    "text": "GitHub in This Course\nYour workflow each week:\n1. Edit files in RStudio\n2. Commit changes with descriptive message  \n3. Push to GitHub\n4. Your portfolio website updates automatically\nThis becomes second nature by mid-semester!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github-classroom",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github-classroom",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nEducational tool that:\n\nCreates individual repositories for each student\nDistributes assignments automatically\n\nEnables efficient feedback and grading\nTeaches professional Git workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-it-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-it-works",
    "title": "Welcome to MUSA 5080",
    "section": "How It Works",
    "text": "How It Works\n\nDr. Delmelle creates assignment with starter code\nYou accept assignment via special link\nGitHub creates your personal repository\nYou complete work in your repository\nTAs provide feedback through GitHub tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#benefits-for-you",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#benefits-for-you",
    "title": "Welcome to MUSA 5080",
    "section": "Benefits for You",
    "text": "Benefits for You\n\nIndividual workspace that’s yours to customize\nProfessional portfolio you can show employers\nVersion control practice for future jobs\nDirect feedback from instructors on your code"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nPublishing system that combines:\n\nCode (R, Python, etc.)\nText (explanations, analysis)\nOutput (plots, tables, results)\n\nInto professional documents"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nReproducible research:\n\nCode and explanation in one place\nOthers can re-run your analysis\nProfessional presentation\n\nCareer relevance:\n\nIndustry standard for data science communication\nCreates websites, PDFs, presentations\nUsed at major tech companies and government agencies"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-vs.-r-markdown",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-vs.-r-markdown",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto vs. R Markdown",
    "text": "Quarto vs. R Markdown\nIf you know R Markdown:\n\nQuarto is the “next generation”\nBetter website creation\nWorks with multiple programming languages\nSame basic concept, improved features"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-document-structure",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-document-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto Document Structure",
    "text": "Quarto Document Structure\nYAML header:\n---\ntitle: \"My Analysis\" \nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nR code chunk:\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/car_sales_data.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#text-formatting",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#text-formatting",
    "title": "Welcome to MUSA 5080",
    "section": "Text Formatting",
    "text": "Text Formatting\n**Bold text**\n*Italic text*\n***Bold and italic***\n`code text`\n~~Strikethrough~~\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#headers",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#headers",
    "title": "Welcome to MUSA 5080",
    "section": "Headers",
    "text": "Headers\n# Main Header\n## Section Header  \n### Subsection Header\nUse headers to organize your analysis sections."
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#lists",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#lists",
    "title": "Welcome to MUSA 5080",
    "section": "Lists",
    "text": "Lists\n## Unordered List\n- Item 1\n- Item 2\n  - Sub-item A\n  - Sub-item B\n\n## Ordered List  \n1. First item\n2. Second item\n3. Third item"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#links-and-images",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#links-and-images",
    "title": "Welcome to MUSA 5080",
    "section": "Links and Images",
    "text": "Links and Images\n[Link text](https://example.com)\n[Link to another page](about.qmd)\n![Alt text](path/to/image.png)\nEssential for professional portfolios:\n\nLink to data sources\nReference course materials\n\nInclude relevant images/plots"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-r-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-r-for-policy-analysis",
    "title": "Welcome to MUSA 5080",
    "section": "Why R for Policy Analysis?",
    "text": "Why R for Policy Analysis?\n\nFree and open source\nExcellent for spatial data\nStrong statistical capabilities\nLarge community in urban planning/policy\nReproducible research workflows"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-projects-essential-habit",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-projects-essential-habit",
    "title": "Welcome to MUSA 5080",
    "section": "RStudio Projects: Essential Habit",
    "text": "RStudio Projects: Essential Habit\nAlways work within projects for:\n\nOrganized file structure - data, scripts, outputs in one place\nRelative file paths - \"data/cars.csv\" works for everyone\n\nVersion control integration - Git works seamlessly\nReproducible workflow - others can run your code\n\nProfessional standard - employers expect this!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-benefits-for-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-benefits-for-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "Project Benefits for This Course",
    "text": "Project Benefits for This Course\nYour GitHub portfolio IS a project:\n# This works reliably in projects:\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# This breaks when shared:\ncar_data &lt;- read_csv(\"/Users/yourname/Desktop/cars.csv\")\nWe’ll work in projects all semester - builds good habits!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#creating-your-project",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#creating-your-project",
    "title": "Welcome to MUSA 5080",
    "section": "Creating Your Project",
    "text": "Creating Your Project\nStep 1: Clone your GitHub repository\ngit clone https://github.com/username/musa5080-portfolio.git\ncd musa5080-portfolio\nStep 2: Open in RStudio\n\nOpen RStudio\nFile → Open Project\nNavigate to your cloned folder\nSelect the .Rproj file"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-file-structure",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-file-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Project File Structure",
    "text": "Project File Structure\nOrganized structure from day one:\nmusa5080-portfolio/\n├── .Rproj\n├── .gitignore\n├── data/\n│   ├── raw/\n│   └── processed/\n├── scripts/\n├── docs/\n├── outputs/\n│   ├── figures/\n│   └── tables/\n└── week01/\n    ├── index.qmd\n    └── data/"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-this-structure-matters",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-this-structure-matters",
    "title": "Welcome to MUSA 5080",
    "section": "Why This Structure Matters",
    "text": "Why This Structure Matters\nProfessional habit:\n\nAnyone can understand your project layout\nScripts know where to find data files\nEasy to maintain as projects grow\nIndustry standard for data science teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#file-naming-conventions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#file-naming-conventions",
    "title": "Welcome to MUSA 5080",
    "section": "File Naming Conventions",
    "text": "File Naming Conventions\nBe consistent and descriptive:\n# Good examples:\nweek01_exploratory_analysis.qmd\n2025-09-08_census_data_cleaning.R\nphiladelphia_housing_2020-2024.csv\n\n# Avoid these:\nanalysis.qmd\ntemp.R\ndata.csv\nnew_version_final.qmd"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#working-with-data-files",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#working-with-data-files",
    "title": "Welcome to MUSA 5080",
    "section": "Working with Data Files",
    "text": "Working with Data Files\nBest practices for this course:\n# Raw data (never edit these!)\nraw_census &lt;- read_csv(here(\"data\", \"raw\", \"acs_2022_philadelphia.csv\"))\n\n# Process and save cleaned versions\nclean_census &lt;- raw_census %&gt;%\n  clean_names() %&gt;%\n  filter(!is.na(median_income))\n\nwrite_csv(clean_census, here(\"data\", \"processed\", \"acs_2022_clean.csv\"))\n\n# Use processed data in analysis\nanalysis_data &lt;- read_csv(here(\"data\", \"processed\", \"acs_2022_clean.csv\"))"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-settings-for-reproducibility",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-settings-for-reproducibility",
    "title": "Welcome to MUSA 5080",
    "section": "RStudio Settings for Reproducibility",
    "text": "RStudio Settings for Reproducibility\nCritical settings to change RIGHT NOW:\nTools → Global Options → General:\n\nUncheck “Restore most recently opened project at startup”\nUncheck “Restore previously opened source documents”\n\nTools → Global Options → Workspace:\n\nUncheck “Restore .RData into workspace at startup”\n\nSet “Save workspace to .RData on exit” to “Never”"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-settings-matter",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-settings-matter",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Settings Matter",
    "text": "Why These Settings Matter\nWithout these changes:\n\nOld objects stick around between sessions\nCode appears to work but fails for others\nHidden dependencies break reproducibility\nYour portfolio assignments might not run for TAs!\n\nWith these settings:\n\nFresh environment every time\nCode must be complete and self-contained\nTrue reproducibility\nProfessional habits from day one"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#managing-r-environment",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#managing-r-environment",
    "title": "Welcome to MUSA 5080",
    "section": "Managing R Environment",
    "text": "Managing R Environment\nKeep your environment clean:\n# Start each session fresh\nrm(list = ls())\n\n# Use projects instead of setwd()\n# NEVER use setwd() in your code!\n\n# Check your working directory\ngetwd()  # Should be your project root\n\n# Use here() for all file paths\nhere(\"data\", \"my_file.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#tibbles-vs-data-frames",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#tibbles-vs-data-frames",
    "title": "Welcome to MUSA 5080",
    "section": "Tibbles vs Data Frames",
    "text": "Tibbles vs Data Frames\nTidyverse uses “tibbles” - enhanced data frames:\n# Traditional data frame\nclass(data)\n# [1] \"data.frame\"\n\n# Convert to tibble  \ncar_data &lt;- as_tibble(data)\nclass(car_data)\n# [1] \"tbl_df\" \"tbl\" \"data.frame\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-tibbles-are-better",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-tibbles-are-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why Tibbles Are Better",
    "text": "Why Tibbles Are Better\nSmarter printing:\n\nShows first 10 rows by default\nDisplays column types\nFits nicely on screen\n\nWe’ll see the difference with our car data…"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#essential-dplyr-functions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#essential-dplyr-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Essential dplyr Functions",
    "text": "Essential dplyr Functions\nWe’ll use these constantly:\n\nselect() - choose columns\nfilter() - choose rows\n\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#dplyr-function-rules",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#dplyr-function-rules",
    "title": "Welcome to MUSA 5080",
    "section": "dplyr Function Rules",
    "text": "dplyr Function Rules\nAll dplyr functions follow the same pattern:\n\nFirst argument is always a data frame\nSubsequent arguments describe which columns to operate on (using variable names without quotes)\nOutput is always a new data frame\n\nThis consistency makes dplyr predictable and easy to learn!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#function-pattern-examples",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#function-pattern-examples",
    "title": "Welcome to MUSA 5080",
    "section": "Function Pattern Examples",
    "text": "Function Pattern Examples\n# Rule 1: Data frame first\nselect(car_data, Manufacturer, Price)\nfilter(car_data, Price &gt; 20000)\nmutate(car_data, price_k = Price / 1000)\n\n# Rule 2: Column names without quotes\nselect(car_data, Manufacturer, Model, Price)  # Not \"Manufacturer\"\nfilter(car_data, Year &gt;= 2020, Mileage &lt; 50000)\n\n# Rule 3: Always returns a new data frame\nnew_data &lt;- select(car_data, Manufacturer, Price)\n# car_data is unchanged, new_data contains selected columns"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#live-demo-basic-dplyr",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#live-demo-basic-dplyr",
    "title": "Welcome to MUSA 5080",
    "section": "Live Demo: Basic dplyr",
    "text": "Live Demo: Basic dplyr\n\nlibrary(tidyverse)\n\n# Load car sales data\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# Basic exploration\nglimpse(car_data)\nnames(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#data-manipulation-pipeline",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#data-manipulation-pipeline",
    "title": "Welcome to MUSA 5080",
    "section": "Data Manipulation Pipeline",
    "text": "Data Manipulation Pipeline\nPipes (%&gt;%) are the magic of dplyr:\n# The power of pipes - read as \"then\"\ncar_summary &lt;- data %&gt;%\n  filter(`Year of manufacture` &gt;= 2020) %&gt;%      # Recent models only\n  select(Manufacturer, Model, Price, Mileage) %&gt;% # Key variables\n  mutate(price_k = Price / 1000) %&gt;%             # Convert to thousands\n  filter(Mileage &lt; 50000) %&gt;%                    # Low mileage cars\n  group_by(Manufacturer) %&gt;%                     # Group by brand\n  summarize(                                     # Calculate statistics\n    avg_price = mean(price_k, na.rm = TRUE),\n    count = n()\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-pipes",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-pipes",
    "title": "Welcome to MUSA 5080",
    "section": "Understanding Pipes",
    "text": "Understanding Pipes\nWhat is %&gt;%?\n\nTakes the output from the left side\nFeeds it as the first argument to the function on the right side\nThink: “and then…”\n\nWithout pipes (nested functions):\n# Hard to read - inside out!\ncar_summary &lt;- summarize(\n  group_by(\n    filter(\n      mutate(\n        select(filter(data, `Year of manufacture` &gt;= 2020), \n               Manufacturer, Model, Price, Mileage),\n        price_k = Price / 1000),\n      Mileage &lt; 50000),\n    Manufacturer),\n  avg_price = mean(price_k, na.rm = TRUE),\n  count = n()\n)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#without-pipes-multiple-objects",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#without-pipes-multiple-objects",
    "title": "Welcome to MUSA 5080",
    "section": "Without Pipes (Multiple Objects)",
    "text": "Without Pipes (Multiple Objects)\nAlternative: create many intermediate objects\n# Clutters your environment\nrecent_cars &lt;- filter(data, `Year of manufacture` &gt;= 2020)\nkey_vars &lt;- select(recent_cars, Manufacturer, Model, Price, Mileage)\nprice_thousands &lt;- mutate(key_vars, price_k = Price / 1000)\nlow_mileage &lt;- filter(price_thousands, Mileage &lt; 50000)\ngrouped_cars &lt;- group_by(low_mileage, Manufacturer)\ncar_summary &lt;- summarize(grouped_cars, \n                        avg_price = mean(price_k, na.rm = TRUE),\n                        count = n())\nProblems: Lots of temporary objects, hard to follow the logic"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-pipes-are-better",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-pipes-are-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why Pipes Are Better",
    "text": "Why Pipes Are Better\nReadable: Follow the logical flow of analysis\nEfficient: No temporary objects cluttering environment\nDebuggable: Easy to run line-by-line\nProfessional: Industry standard for data science"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#reading-pipes-aloud",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#reading-pipes-aloud",
    "title": "Welcome to MUSA 5080",
    "section": "Reading Pipes Aloud",
    "text": "Reading Pipes Aloud\ncar_data %&gt;%\n  filter(Price &gt; 15000) %&gt;%\n  select(Manufacturer, Price) %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price))\nRead as:\n“Take car_data, then filter for cars over $15,000, then select manufacturer and price columns, then group by manufacturer, then calculate average price”"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-group_by-and-summarize",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-group_by-and-summarize",
    "title": "Welcome to MUSA 5080",
    "section": "Understanding group_by() and summarize()",
    "text": "Understanding group_by() and summarize()\nThese functions work as a team:\n\ngroup_by() - sets up grouping for subsequent operations\nsummarize() - collapses rows into summary statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-group_by-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-group_by-works",
    "title": "Welcome to MUSA 5080",
    "section": "How group_by() Works",
    "text": "How group_by() Works\n# group_by() doesn't change what you see...\ncar_data %&gt;% \n  group_by(Manufacturer)\n\n# ...but it sets up invisible grouping for next operations\n# Look for: \"Groups: Manufacturer [5]\" in the output\nKey insight: group_by() prepares the data, doesn’t transform it yet"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-summarize-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-summarize-works",
    "title": "Welcome to MUSA 5080",
    "section": "How summarize() Works",
    "text": "How summarize() Works\n# Without grouping - one row of results\ncar_data %&gt;%\n  summarize(\n    avg_price = mean(Price, na.rm = TRUE),\n    total_cars = n()\n  )\n# Result: 1 row with overall averages\n\n# With grouping - one row per group\ncar_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\n    avg_price = mean(Price, na.rm = TRUE),\n    total_cars = n()\n  )\n# Result: 5 rows (one per manufacturer)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#before-and-after-example",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#before-and-after-example",
    "title": "Welcome to MUSA 5080",
    "section": "Before and After Example",
    "text": "Before and After Example\nOriginal data (imagine this):\nManufacturer  Price   Mileage\nToyota       25000    30000\nToyota       28000    15000  \nHonda        22000    45000\nHonda        30000    20000\nFord         35000    10000\nAfter group_by(Manufacturer) %&gt;% summarize(…):\nManufacturer  avg_price  total_cars\nToyota       26500      2\nHonda        26000      2  \nFord         35000      1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#common-summarize-functions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#common-summarize-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Common summarize() Functions",
    "text": "Common summarize() Functions\nEssential summary functions:\ncar_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\n    count = n(),                          # Number of rows\n    avg_price = mean(Price, na.rm = TRUE), # Average\n    med_price = median(Price, na.rm = TRUE), # Median  \n    min_price = min(Price, na.rm = TRUE),   # Minimum\n    max_price = max(Price, na.rm = TRUE),   # Maximum\n    std_dev = sd(Price, na.rm = TRUE)       # Standard deviation\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#policy-analysis-applications",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#policy-analysis-applications",
    "title": "Welcome to MUSA 5080",
    "section": "Policy Analysis Applications",
    "text": "Policy Analysis Applications\nPerfect for policy questions like:\n\nAverage household income by neighborhood\nCrime rates by police district\n\nHousing prices by year\nTransportation usage by demographic group\nEducational outcomes by school district\n\nPattern: group_by(category) %&gt;% summarize(metric)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#weekly-pattern",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#weekly-pattern",
    "title": "Welcome to MUSA 5080",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nMonday Class: - New concepts and methods - Hands-on coding practice - Lab work with TA support\nDuring Week: - Complete portfolio assignments - Weekly notes and reflection - Office hours for help"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#assessment-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#assessment-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "Assessment Philosophy",
    "text": "Assessment Philosophy\nFocus on understanding, not perfect code:\n\nWeekly quizzes test concepts\nPortfolio assignments build skills\nLow stakes encourage experimentation\nProfessional development throughout"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-development",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-development",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Development",
    "text": "Portfolio Development\nYour GitHub portfolio will include:\n\nWeekly learning reflections\nCompleted lab analyses\nProfessional documentation\nWork you can show employers"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-setup-process",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-setup-process",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Setup Process",
    "text": "Portfolio Setup Process\n\nAccept GitHub Classroom assignment\nClone repository to your computer\n\nCustomize with your information\nEnable GitHub Pages\nComplete first analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-well-accomplish",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-well-accomplish",
    "title": "Welcome to MUSA 5080",
    "section": "What We’ll Accomplish",
    "text": "What We’ll Accomplish\nBy end of today:\n\nWorking portfolio repository\nLive website with your work\nFirst R analysis in professional format\nFamiliarity with workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#support-available",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#support-available",
    "title": "Welcome to MUSA 5080",
    "section": "Support Available",
    "text": "Support Available\n\nDr. Delmelle and TAs circulating during hands-on time\nOffice hours starting this week\nGitHub Issues for technical questions\nCanvas discussion for course questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#ready-to-get-started",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#ready-to-get-started",
    "title": "Welcome to MUSA 5080",
    "section": "Ready to Get Started?",
    "text": "Ready to Get Started?\nNext: Portfolio setup with GitHub Classroom\nRemember: This is a learning process - ask for help when you need it!"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html",
    "href": "assignments/assignment1/Assignment1_BlankClone.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#scenario",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#learning-objectives",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#submission-instructions",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#data-retrieval",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\nvt_data1 &lt;- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim &lt;- vt_data1 %&gt;% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %&gt;% head(5)\n\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#data-quality-assessment",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct &lt;- vt_data1_trim %&gt;%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct &lt; 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct &gt;= 5 & med_hh_inc_Mpct &lt;= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct &gt; 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  summarize(count = n())\n\nvt_data1_reliability\n\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  &lt;chr&gt;               &lt;int&gt;\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct &lt;- vt_data1_MOEpct %&gt;%\n  arrange(desc(med_hh_inc_Mpct)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, format = \"html\")\n\n\n\n\nNAME\nmed_hh_incE\nmed_hh_incM\ntot_popE\nmed_hh_inc_Mpct\nmed_hh_inc_conf\n\n\n\n\nGrand Isle\n86639\n10729\n7335\n12.383569\nLow Confidence\n\n\nLamoille\n69886\n5846\n25977\n8.365052\nModerate Confidence\n\n\nEssex\n55247\n3679\n5976\n6.659185\nModerate Confidence\n\n\nFranklin\n73633\n4436\n50101\n6.024473\nModerate Confidence\n\n\nWindham\n65473\n3331\n45857\n5.087593\nModerate Confidence\n\n\n\n\n\n Data Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]   Counties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#focus-area-selection",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %&gt;% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nAddison\n85870\n2958\n37434\n3.44\nHigh Confidence\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\n\n\n\nComment on the output: [write something :)]   While Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#tract-level-demographics",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\ndemo_vars &lt;- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties &lt;- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 &lt;- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep &lt;- vt_data2 %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts &lt;- vt_data2_sep %&gt;%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#demographic-analysis",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat &lt;- vt_data2_pcts %&gt;% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo &lt;- vt_data2_pcts %&gt;% \n  group_by(COUNTY) %&gt;% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTract Count\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pct\n\n\n\n\nAddison\n10\n91.4\n0.8869\n2.50\n\n\nEssex\n3\n94.0\n0.0335\n1.54\n\n\nGrand Isle\n2\n91.0\n1.0907\n2.00"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct &lt;- vt_data2_sep %&gt;%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability &lt;- vt_data2_MOEpct %&gt;% \n  mutate(race_all_conf = ifelse(race_white_Mpct &gt; 15 | race_black_Mpct &gt; 15 | race_hispLatinoMpct &gt; 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#pattern-analysis",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats &lt;- vt_data2_reliability %&gt;%\n  group_by(race_all_conf) %&gt;% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nConfidence Category\nAvg Tract Pop\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pop\n\n\n\n\n1\n3383\n91.7\n0.816\n2.31\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont’s low population outside of city centers also contributes to increased unreliability of estimates."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract’s sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#specific-recommendations",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table &lt;- vt_data1_MOEpct %&gt;%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %&gt;% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMOE Pct\nConfidence Rating\nAlgorithm Recommendation\n\n\n\n\nAddison\n85870\n3.44\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBennington\n68558\n4.23\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCaledonia\n62964\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChittenden\n89494\n2.55\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n55247\n6.66\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n73633\n6.02\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrand Isle\n86639\n12.38\nLow Confidence\nRequires manual review or additional data\n\n\nLamoille\n69886\n8.37\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOrange\n74534\n3.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n63981\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRutland\n62641\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n77278\n3.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWindham\n65473\n5.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWindsor\n69492\n4.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algoLarithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\n\nAddison\nBennington\nCaledonia\nChittenden\nOrange\nOrleans\nRutland\nWashington\nWindsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\n\nEssex\nFranklin\nLamoille\nWindham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n\nGrand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nWhat are the underlying contributing factors for the dominance of white residents in Vermont?\nAs a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#submission-checklist",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html",
    "href": "assignments/assignment1/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#scenario",
    "href": "assignments/assignment1/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "href": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "href": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "href": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\nvt_data1 &lt;- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim &lt;- vt_data1 %&gt;% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %&gt;% head(5)\n\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct &lt;- vt_data1_trim %&gt;%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct &lt; 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct &gt;= 5 & med_hh_inc_Mpct &lt;= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct &gt; 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  summarize(count = n())\n\nvt_data1_reliability\n\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  &lt;chr&gt;               &lt;int&gt;\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct &lt;- vt_data1_MOEpct %&gt;%\n  arrange(desc(med_hh_inc_Mpct)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, col.names = c(\"County\", \"Median Household Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"), format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian Household Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nLamoille\n69886\n5846\n25977\n8.37\nModerate Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\nFranklin\n73633\n4436\n50101\n6.02\nModerate Confidence\n\n\nWindham\n65473\n3331\n45857\n5.09\nModerate Confidence\n\n\n\n\n\n Data Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]   Counties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %&gt;% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nAddison\n85870\n2958\n37434\n3.44\nHigh Confidence\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\n\n\n\nComment on the output: [write something :)]   While Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\ndemo_vars &lt;- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties &lt;- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 &lt;- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep &lt;- vt_data2 %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts &lt;- vt_data2_sep %&gt;%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat &lt;- vt_data2_pcts %&gt;% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo &lt;- vt_data2_pcts %&gt;% \n  group_by(COUNTY) %&gt;% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTract Count\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pct\n\n\n\n\nAddison\n10\n91.4\n0.8869\n2.50\n\n\nEssex\n3\n94.0\n0.0335\n1.54\n\n\nGrand Isle\n2\n91.0\n1.0907\n2.00"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct &lt;- vt_data2_sep %&gt;%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability &lt;- vt_data2_MOEpct %&gt;% \n  mutate(race_all_conf = ifelse(race_white_Mpct &gt; 15 | race_black_Mpct &gt; 15 | race_hispLatinoMpct &gt; 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats &lt;- vt_data2_reliability %&gt;%\n  group_by(race_all_conf) %&gt;% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nConfidence Category\nAvg Tract Pop\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pop\n\n\n\n\n1\n3383\n91.7\n0.816\n2.31\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont’s low population outside of city centers also contributes to increased unreliability of estimates."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract’s sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table &lt;- vt_data1_MOEpct %&gt;%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %&gt;% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMOE Pct\nConfidence Rating\nAlgorithm Recommendation\n\n\n\n\nAddison\n85870\n3.44\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBennington\n68558\n4.23\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCaledonia\n62964\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChittenden\n89494\n2.55\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n55247\n6.66\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n73633\n6.02\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrand Isle\n86639\n12.38\nLow Confidence\nRequires manual review or additional data\n\n\nLamoille\n69886\n8.37\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOrange\n74534\n3.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n63981\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRutland\n62641\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n77278\n3.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWindham\n65473\n5.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWindsor\n69492\n4.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\n\nAddison\nBennington\nCaledonia\nChittenden\nOrange\nOrleans\nRutland\nWashington\nWindsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\n\nEssex\nFranklin\nLamoille\nWindham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n\nGrand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nWhat are the underlying contributing factors for the dominance of white residents in Vermont?\nAs a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "href": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-this-course-is-about",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-this-course-is-about",
    "title": "Welcome to MUSA 5080",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nAdvanced spatial analysis for urban planning and public policy\nData science tools within policy context\nFocus on understanding concepts rather than just completing code\nProfessional portfolio development using modern tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#unlike-private-sector-data-science",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#unlike-private-sector-data-science",
    "title": "Welcome to MUSA 5080",
    "section": "Unlike Private Sector Data Science",
    "text": "Unlike Private Sector Data Science\n\nNot just about optimization\nPublic goods, governance, equity considerations\nTransparency and interpretability are crucial\nAlgorithmic bias has real consequences for communities"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-semesters-innovation",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-semesters-innovation",
    "title": "Welcome to MUSA 5080",
    "section": "This Semester’s Innovation",
    "text": "This Semester’s Innovation\nProblem: AI tools making it easy to complete code without understanding\nSolution:\n\n40% weekly in-class quizzes (test conceptual understanding)\nLow-stakes portfolio assignments (focus on learning, not grades)\nGitHub-based workflow (professional skills)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-these-tools",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-these-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Tools?",
    "text": "Why These Tools?\nGitHub: Industry standard for version control and collaboration\nQuarto: Modern approach to reproducible research and documentation\nR: Powerful for spatial analysis and policy-focused statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#professional-development",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#professional-development",
    "title": "Welcome to MUSA 5080",
    "section": "Professional Development",
    "text": "Professional Development\nThese aren’t just “class tools” - they’re career tools:\n\nPortfolio employers can see\nVersion control skills for any data job\nProfessional documentation practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-git",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-git",
    "title": "Welcome to MUSA 5080",
    "section": "What is Git?",
    "text": "What is Git?\nVersion control system that tracks changes in files\nThink of it as:\n\n“Track changes” for code projects\nTime machine for your work\nCollaboration tool for teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nCloud hosting for Git repositories\n\nBackup your work in the cloud\n\nShare projects with others\nDeploy websites (like our portfolios)\nCollaborate on code projects"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#key-github-concepts",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#key-github-concepts",
    "title": "Welcome to MUSA 5080",
    "section": "Key GitHub Concepts",
    "text": "Key GitHub Concepts\nRepository (repo): Folder containing your project files\nCommit: Snapshot of your work at a point in time\nPush: Send your changes to GitHub cloud\nPull: Get latest changes from GitHub cloud"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#github-in-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#github-in-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "GitHub in This Course",
    "text": "GitHub in This Course\nYour workflow each week:\n\nEdit files in RStudio\nCommit changes with descriptive message\n\nPush to GitHub\nYour portfolio website updates automatically"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-will-become-second-nature-soon",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-will-become-second-nature-soon",
    "title": "Welcome to MUSA 5080",
    "section": "This will become second nature soon!",
    "text": "This will become second nature soon!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github-classroom",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github-classroom",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nEducational tool that:\n\nCreates individual repositories for each student\nDistributes assignments automatically\n\nEnables efficient feedback and grading\nTeaches professional Git workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#how-it-works",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#how-it-works",
    "title": "Welcome to MUSA 5080",
    "section": "How It Works",
    "text": "How It Works\n\nDr. Delmelle creates assignment with starter code\nYou accept assignment via special link\nGitHub creates your personal repository\nYou complete work in your repository\nTAs provide feedback through GitHub tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#benefits-for-you",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#benefits-for-you",
    "title": "Welcome to MUSA 5080",
    "section": "Benefits for You",
    "text": "Benefits for You\n\nIndividual workspace that’s yours to customize\nProfessional portfolio you can show employers\nVersion control practice for future jobs\nDirect feedback from instructors on your code"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nPublishing system that combines:\n\nCode (R, Python, etc.)\nText (explanations, analysis)\nOutput (plots, tables, results)\n\nInto professional documents"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nReproducible research:\n\nCode and explanation in one place\nOthers can re-run your analysis\nProfessional presentation\n\nCareer relevance:\n\nIndustry standard for data science communication\nCreates websites, PDFs, presentations\nUsed at major tech companies and government agencies"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-vs.-r-markdown",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-vs.-r-markdown",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto vs. R Markdown",
    "text": "Quarto vs. R Markdown\nIf you know R Markdown:\n\nQuarto is the “next generation”\nBetter website creation\nWorks with multiple programming languages\nSame basic concept, improved features"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-document-structure",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-document-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto Document Structure",
    "text": "Quarto Document Structure\nYAML header:\n---\ntitle: \"My Analysis\" \nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nR code chunk:\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/car_sales_data.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#text-formatting",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#text-formatting",
    "title": "Welcome to MUSA 5080",
    "section": "Text Formatting",
    "text": "Text Formatting\n**Bold text**\n*Italic text*\n***Bold and italic***\n`code text`\n~~Strikethrough~~\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#headers",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#headers",
    "title": "Welcome to MUSA 5080",
    "section": "Headers",
    "text": "Headers\n# Main Header\n## Section Header  \n### Subsection Header\nUse headers to organize your analysis sections."
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#lists",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#lists",
    "title": "Welcome to MUSA 5080",
    "section": "Lists",
    "text": "Lists\n## Unordered List\n- Item 1\n- Item 2\n  - Sub-item A\n  - Sub-item B\n\n## Ordered List  \n1. First item\n2. Second item\n3. Third item"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#links-and-images",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#links-and-images",
    "title": "Welcome to MUSA 5080",
    "section": "Links and Images",
    "text": "Links and Images\n[Link text](https://example.com)\n[Link to another page](about.qmd)\n![Alt text](path/to/image.png)\nEssential for professional portfolios:\n\nLink to data sources\nReference course materials\n\nInclude relevant images/plots"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-r-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-r-for-policy-analysis",
    "title": "Welcome to MUSA 5080",
    "section": "Why R for Policy Analysis?",
    "text": "Why R for Policy Analysis?\n\nFree and open source\nExcellent for spatial data\nStrong statistical capabilities\nLarge community in urban planning/policy\nReproducible research workflows"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#rstudio-projects-essential-habit",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#rstudio-projects-essential-habit",
    "title": "Welcome to MUSA 5080",
    "section": "Rstudio Projects: Essential Habit",
    "text": "Rstudio Projects: Essential Habit\nAlways work within projects\n\norganized file structure - data, scripts, outputs in one place\nRelative file paths - `“data/cars.csv”’"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tidyverse-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tidyverse-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "tidyverse Philosophy",
    "text": "tidyverse Philosophy\nCollection of packages designed for data science:\n\nConsistent syntax across functions\nReadable code that tells a story\nEfficient workflows for common tasks"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tibbles-vs.-data-frames",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tibbles-vs.-data-frames",
    "title": "Welcome to MUSA 5080",
    "section": "Tibbles vs. Data Frames",
    "text": "Tibbles vs. Data Frames\nTidyverse uses “tibbles” - enhanced data frames.\n#Traditional Data Frame\nclass(data)\n# Convert to tibble\ncar_data &lt;- as_tibble(data)\nclass(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-are-tibbles-better",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-are-tibbles-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why are Tibbles Better?",
    "text": "Why are Tibbles Better?\nSmarter Printing:\n\nShows first 10 rows by default\nDisplays column names\nfits nicely on a screen"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#essential-dplyr-functions",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#essential-dplyr-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Essential dplyr Functions",
    "text": "Essential dplyr Functions\nWe’ll use these constantly:\n\nselect() - choose columns\nfilter() - choose rows\n\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#live-demo-basic-dplyr",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#live-demo-basic-dplyr",
    "title": "Welcome to MUSA 5080",
    "section": "Live Demo: Basic dplyr",
    "text": "Live Demo: Basic dplyr\nlibrary(tidyverse)\n\n# Load car sales data\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# Basic exploration\nglimpse(car_data)\nnames(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#data-manipulation-pipeline",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#data-manipulation-pipeline",
    "title": "Welcome to MUSA 5080",
    "section": "Data Manipulation Pipeline",
    "text": "Data Manipulation Pipeline\n\n# The power of pipes - read as \"then\"\ncar_summary &lt;- data %&gt;%\n  filter(`Year of manufacture` &gt;= 2020) %&gt;%      # Recent models only\n  select(Manufacturer, Model, Price, Mileage) %&gt;% # Key variables\n  mutate(price_k = Price / 1000) %&gt;%             # Convert to thousands\n  filter(Mileage &lt; 50000) %&gt;%                    # Low mileage cars\n  group_by(Manufacturer) %&gt;%                     # Group by brand\n  summarize(                                     # Calculate statistics\n    avg_price = mean(price_k, na.rm = TRUE),\n    count = n()\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#policy-applications",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#policy-applications",
    "title": "Welcome to MUSA 5080",
    "section": "Policy Applications",
    "text": "Policy Applications\nThis semester we’ll use these skills for:\n\nCensus data analysis\nNeighborhood change studies\nPredictive modeling for resource allocation\nHousing market analysis\nTransportation equity assessment"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#weekly-pattern",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#weekly-pattern",
    "title": "Welcome to MUSA 5080",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nMonday Class: - New concepts and methods - Hands-on coding practice - Lab work with TA support\nDuring Week: - Complete portfolio assignments - Weekly notes and reflection - Office hours for help"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#assessment-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#assessment-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "Assessment Philosophy",
    "text": "Assessment Philosophy\nFocus on understanding, not perfect code:\n\nWeekly quizzes test concepts\nPortfolio assignments build skills\nLow stakes encourage experimentation\nProfessional development throughout"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-development",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-development",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Development",
    "text": "Portfolio Development\nYour GitHub portfolio will include:\n\nWeekly learning reflections\nCompleted lab analyses\nProfessional documentation\nWork you can show employers"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-setup-process",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-setup-process",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Setup Process",
    "text": "Portfolio Setup Process\n\nAccept GitHub Classroom assignment\nClone repository to your computer\n\nCustomize with your information\nEnable GitHub Pages\nComplete first analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-well-accomplish",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-well-accomplish",
    "title": "Welcome to MUSA 5080",
    "section": "What We’ll Accomplish",
    "text": "What We’ll Accomplish\nBy end of today: - Working portfolio repository - Live website with your work - First R analysis in professional format - Familiarity with workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#support-available",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#support-available",
    "title": "Welcome to MUSA 5080",
    "section": "Support Available",
    "text": "Support Available\n\nDr. Delmelle and TAs circulating during hands-on time\nOffice hours starting this week\nGitHub Issues for technical questions\nCanvas discussion for course questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#ready-to-get-started",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#ready-to-get-started",
    "title": "Welcome to MUSA 5080",
    "section": "Ready to Get Started?",
    "text": "Ready to Get Started?\nNext: Portfolio setup with GitHub Classroom\nRemember: This is a learning process - ask for help when you need it!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#what-well-cover",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#opening-question",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-policy-implications",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tidycensus)\n\nWarning: package 'tidycensus' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n# Get some census data for demonstration\ndemo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(county_name = str_remove(NAME, \", Pennsylvania\"))\n\nGetting data from the 2018-2022 5-year ACS\n\n# Basic scatter plot\nggplot(demo_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point()"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes\n\nggplot(demo_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Income vs Population in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau ACS\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_x_continuous(labels = scales::comma)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-eda-mindset",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#understanding-distributions",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\n# Histogram of median income\nggplot(demo_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Median Income\",\n       x = \"Median Household Income\", \n       y = \"Number of Counties\")\n\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#boxplots",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!\n\n# Boxplot to see outliers and quartiles  \nggplot(demo_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Income Distribution with Outliers\",\n       y = \"Median Household Income\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\n# Visualize margins of error - ESSENTIAL for ethical practice\ndemo_data %&gt;%\n  mutate(moe_pct = median_incomeM / median_incomeE * 100) %&gt;%\n  ggplot() +\n  aes(x = total_popE, y = moe_pct) +\n  geom_point() +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Data Reliability by Population Size\",\n       x = \"Population\", \n       y = \"Margin of Error (%)\",\n       caption = \"Red line = 10% threshold for reliability\")\n\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#why-join-data",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n# Get income data\nincome_data &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"PA\", \n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, NAME, median_income = B19013_001E, income_moe = B19013_001M)\n\nGetting data from the 2018-2022 5-year ACS\n\n# Get education data  \neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B15003_022\",  # Bachelor's degree or higher\n  state = \"PA\",\n  year = 2022, \n  output = \"wide\"\n) %&gt;%\n  select(GEOID, college_pop = B15003_022E, college_moe = B15003_022M)\n\nGetting data from the 2018-2022 5-year ACS\n\n# Join them together\ncombined_data &lt;- income_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\nhead(combined_data)\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n# Check dimensions\ncat(\"Income data rows:\", nrow(income_data), \"\\n\")\n\nIncome data rows: 67 \n\ncat(\"Education data rows:\", nrow(education_data), \"\\n\") \n\nEducation data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\n# Check for missing values after join\ncombined_data %&gt;%\n  summarize(\n    missing_income = sum(is.na(median_income)),\n    missing_education = sum(is.na(college_pop))\n  )\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n# IMPORTANT: Assess reliability of joined data\ncombined_data %&gt;%\n  mutate(\n    income_cv = (income_moe / median_income) * 100,\n    college_cv = (college_moe / college_pop) * 100\n  ) %&gt;%\n  select(NAME, income_cv, college_cv) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#skills-youll-practice",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)\n\n\n\n\n\nEmail: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "labs/lab0/lab0_template.html",
    "href": "labs/lab0/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7 - Variable types: Character (chr), Double (dbl) - Problematic names: The columns that have spaces in their names require quotes around them while scripting"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\ncar_df %&gt;% head(10)\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: In Markdown, the identifier icon in the top left changes from “A tibble: 50,000 x 7” to “Description: df [50,000 x 7]” for tibbles and dfs, respectively. When rendering the website, printing a data frame prints all rows and does not allow for scrolling across columns. Instead, it prints extra columns in another row."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#selecting-columns",
    "href": "labs/lab0/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nselect(.data = car_data, Model, Mileage)\n\n# A tibble: 50,000 × 2\n   Model      Mileage\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Fiesta      127300\n 2 718 Cayman   57850\n 3 Mondeo       39190\n 4 RAV4        210814\n 5 Polo        127869\n 6 Focus        33603\n 7 Mondeo       86686\n 8 Prius        30663\n 9 Polo         73470\n10 Focus       262514\n# ℹ 49,990 more rows\n\n# Select Manufacturer, Price, and Fuel type\ncar_data %&gt;% select(Manufacturer, Price, `Fuel type`)\n\n# A tibble: 50,000 × 3\n   Manufacturer Price `Fuel type`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n 1 Ford          3074 Petrol     \n 2 Porsche      49704 Petrol     \n 3 Ford         24072 Diesel     \n 4 Toyota        1705 Hybrid     \n 5 VW            4101 Petrol     \n 6 Ford         29204 Petrol     \n 7 Ford         14350 Diesel     \n 8 Toyota       30297 Hybrid     \n 9 VW            9977 Petrol     \n10 Ford          1049 Diesel     \n# ℹ 49,990 more rows\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data %&gt;% select(-`Engine size`)\n\n# A tibble: 50,000 × 6\n   Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta     Petrol                       2002  127300  3074\n 2 Porsche      718 Cayman Petrol                       2016   57850 49704\n 3 Ford         Mondeo     Diesel                       2014   39190 24072\n 4 Toyota       RAV4       Hybrid                       1988  210814  1705\n 5 VW           Polo       Petrol                       2006  127869  4101\n 6 Ford         Focus      Petrol                       2018   33603 29204\n 7 Ford         Mondeo     Diesel                       2010   86686 14350\n 8 Toyota       Prius      Hybrid                       2015   30663 30297\n 9 VW           Polo       Petrol                       2012   73470  9977\n10 Ford         Focus      Diesel                       1992  262514  1049\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#renaming-columns",
    "href": "labs/lab0/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;% \n  rename(year = `Year of manufacture`)\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: year no longer has spaces"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;%\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\ncar_data %&gt;% select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorize-cars",
    "href": "labs/lab0/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is luxury (use case_when)\ncar_data &lt;- car_data %&gt;% \n  mutate(price_category = case_when(Price &lt; 15000 ~ \"budget\",\n                                     Price &gt;= 15000 & Price &lt; 30000 ~ \"midrange\",\n                                     .default = \"luxury\"))\n\n# Check your categories select the new column and show it\ncar_data %&gt;% select(Price, price_category)\n\n# A tibble: 50,000 × 2\n   Price price_category\n   &lt;dbl&gt; &lt;chr&gt;         \n 1  3074 budget        \n 2 49704 luxury        \n 3 24072 midrange      \n 4  1705 budget        \n 5  4101 budget        \n 6 29204 midrange      \n 7 14350 budget        \n 8 30297 luxury        \n 9  9977 budget        \n10  1049 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-filtering",
    "href": "labs/lab0/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;% filter(Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;% filter(Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter(price_category == \"luxury\" & Mileage &lt; 30000)\n\n# A tibble: 3,257 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,247 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Ford OR Porsche\ncar_data %&gt;% filter(Manufacturer %in% c(\"Ford\", \"Porsche\"))\n\n# A tibble: 17,568 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta               1   Petrol       2002  127300  3074    23\n 2 Porsche      718 Cayman           4   Petrol       2016   57850 49704     9\n 3 Ford         Mondeo               1.6 Diesel       2014   39190 24072    11\n 4 Ford         Focus                1.4 Petrol       2018   33603 29204     7\n 5 Ford         Mondeo               1.8 Diesel       2010   86686 14350    15\n 6 Ford         Focus                2   Diesel       1992  262514  1049    33\n 7 Ford         Mondeo               1.6 Diesel       1996   77584  5667    29\n 8 Porsche      911                  2.6 Petrol       2009   66273 41963    16\n 9 Porsche      911                  3.5 Petrol       2005  151556 19747    20\n10 Ford         Focus                1   Hybrid       2010   85131 12472    15\n# ℹ 17,558 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\ncar_data %&gt;% filter(Price &gt; 20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter(`Fuel type` == \"Diesel\" & age &lt; 10)\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2,040 vehicles"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-summaries",
    "href": "labs/lab0/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel &lt;- car_data %&gt;% \n  group_by(`Fuel type`) %&gt;% \n  summarize(avg_mileage = mean(Mileage, na.rm = T))\n\navg_mileage_by_fuel\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncount_by_manufacturer &lt;- car_data %&gt;% \n  group_by(Manufacturer) %&gt;% \n  summarize(count = n())\n\ncount_by_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\nprice_cat_freq_table &lt;- car_data %&gt;%\n  group_by(price_category) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(freq = count / sum(count))\n\nprice_cat_freq_table\n\n# A tibble: 3 × 3\n  price_category count  freq\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 budget         34040 0.681\n2 luxury          6179 0.124\n3 midrange        9781 0.196"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "href": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#public-sector-context",
    "href": "weekly-notes/week-02-notes.html#public-sector-context",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Public Sector Context",
    "text": "Public Sector Context\n\nHow Governments Use Data\n\nLong history of government data collection and a massive amount of public data has become available over the past couple decades\nA desire for algorithmic efficiency often results in some actions that are not typically considered desireable (i.e. mass firings to save costs)\n\nData Analytics Subjectivity\n\nEvery step involves human choices that are inherently subjective\n\nEx. Removal/ommission, recoding, how results are interpreted\n\nHealthcare Algorithm Bias Example\n\nAn algorithm to identify high-risk patients systematically excluded Black patients\nThe model used healthcare costs as a proxy for need, but Black patients typically incurred lower costs due to “systemic inequities in access” to healthcare\n\ni.e. Black patients were not treated as often or as extensively, resulting in less healthcare costs overall\n\nTherefore, patients who were more likely to seek/afford treatment were prioritized over those who could not\n\nCOMPAS Recidivism Preciction Example\n\nBiased policing patterns producing more Black arrests resulted in an algorithm 2x as likely to falsely flag Black defendants as high risk\n\nDutch Welfare Fraud Detection Example\n\nDisproportionately targeted vulnerable populations, violated EU privacy laws\n\n\nActivity in class\n\nTopic: Graduation Probability\nProxy: GPA\nBlind Spot:\nHarm: High-functioning students might encounter a major health event/family-related issue that prevents graduation. Students doing poorly in classes might have a strong support network and simply graduate with a lower GPA.\nGuardrail: Potential survey of source of support to judge student health and support network."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#census-data-foundations",
    "href": "weekly-notes/week-02-notes.html#census-data-foundations",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Census Data Foundations",
    "text": "Census Data Foundations\n\nCensus mandated in the constitution by James Madison to ensure accurate representation in government\nDecennial Census vs. American Community Survey\n\nDecennial Census → Every 10 years, 9 basic questions (incl. age, race, sex, housing), it’s a constitutional requirement, determines political representation\nACS → 3% of households sampled every year, detailed questions (income, education, employment, housing costs), replaced the old “long form” in 2005\n\n1-year estimates (areas &gt; 65,000 people) have the most current data, smallest sample size\n5-year estimates (all areas) have more reliable data since it is aggregated across multiple years and has a larger sample\nThe ACS has a Margin of Error (MOE) that is important to keep an eye on\n\nLarge MOE’s relative to estimate indicate less reliable values\n\n\n\nCensus Geographic Hierarchy\n\nTract &gt; Block Group &gt; Block\nAt smaller geographies (i.e. blocks) mathematical noise is added to protect privacy\n\n“Even objective data involves subjective choices about privacy vs. accuracy”\n\nCensus boundaries change over time, and historical census data doesn’t always match up with modern boundaries\n\nNHGIS provides historical census data with consistent boundaries over time, good for longitudinal studies"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "href": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Tidycensus Tips",
    "text": "Tidycensus Tips\n\nget_acs\n\nCreating a list of c(var_name = “var_code”) automatically renames the columns in the resulting df"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nI still have lingering technical quirks with Quarto\n\nAreas needing more practice\n\nQuarto"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nWhile algorithms are incredibly useful for streamlining policy-making by creating a standard set of rules and criteria for various forms of decision-making, it’s important to keep in mind that the data used to build these models/algorithms and the ways in which they can be deployed can be inherently biased and lead to discriminatory results."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nDiving more deeply into the terminology of the world of algorithms, as well as learning some more census-related tips that’ll be helpful in the future\n\nHow I’ll apply this knowledge\n\nImproving how I approach the process of model-building and looking more deeply into where sources of potential bias might lie."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "href": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "href": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\nThe Earth is round, maps are flat → we can’t preserve area, distance, and angles simultaneously\nAll projections cause some distortions in the true shape of the Earth\nYou cannot use a projection system that uses lat/lon to calculate area\nThe shape of the Earth is a geoid, an imperfect sphere due to terrain\n\nStep 1: Approximate Earth’s shape with an ellipsoid with a uniform surface\nStep 2: Tie the ellipsoid to the real Earth (create a datum)\nStep 3: Overlay your lat/lon grid\n\nNorth American Datum 1927 (NAD27) → centered on Meades Ranch, Kansas\nNAD83 → earth centered\nWGS84 → also earth centered, but uses a different ellipsoid approximation\n*All of these are Geographic (geodetic) coordinate systems that utilize lat/lon\n\nProjecting a 3D Shape onto a 2D shape\n\nCylindrical Projection → wrapping a rectangular sheet around the ellipsoid with its cylindrical axis parallel to the poles (i.e. “Mercator Projection”)\n\nLine of Tangency refers to where a continuous line on the 3D shape aligns entirely with the x axis of the sheet\n\nTransverse Cylindrical → wrapping the cylinder around the ellipsoid, but now the cylindrical axis is parallel to the equatorial plane\nConic projections → wrapping the sheet around in a cone shape with the line of tangency running through the locations you want to study (reduces distortions)\n\nSADD Acronym → things that can be distorted when projecting\n\nShape, Area, Distance, Direction\n\nProjected Coordinate Systems\n\nUTM (Universal Transverse Mercator) → developed by the military by dividing a Mercator Projection into 6 degree segments (60 total)\n\nTypically denoted by a False Northing and a False Easting\n\nState Plane → each state has its own projection method, highly localized (on a global scale) to minimize distortions\n\nTypically conic\nPA has two, one for northern PA and one for southern PA (both in feet/meters)"
  }
]