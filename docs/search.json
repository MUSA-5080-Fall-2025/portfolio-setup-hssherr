[
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Remove all progress bars and cluttered outputs from assignment documents\n\nFor tidycensus/tigris → “progress_bar = F” or something similar"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#homework-feedback-tips",
    "href": "weekly-notes/week-07-notes.html#homework-feedback-tips",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Remove all progress bars and cluttered outputs from assignment documents\n\nFor tidycensus/tigris → “progress_bar = F” or something similar"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#understanding-spatial-patterns-in-errors",
    "href": "weekly-notes/week-07-notes.html#understanding-spatial-patterns-in-errors",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Understanding Spatial Patterns in Errors",
    "text": "Understanding Spatial Patterns in Errors\n\nRandom errors are good, clustered errors are bad\n\nWe want to see models randomly predict poorly, not see patterns in how they predict poorly\nThe latter is an indicator of spatial autocorrelation\n\nSpatial Lag Plot\n\nChecking the relationship of variables to their 5 nearest neighbors, for example\nUse the “spdep” package in R"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#morans-i",
    "href": "weekly-notes/week-07-notes.html#morans-i",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Moran’s I",
    "text": "Moran’s I\n\nRange from -1 to +1\n\n+1 → perfect positive correlation (clustering)\n0 → random spatial pattern\n-1 → perfect negative correlation (dispersion)\n\n\n\\[I = \\frac{n \\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i \\sum_j w_{ij} \\sum_i (x_i - \\bar{x})^2}\\] - The Moran’s I formula exaggerates values that are both similarly placed relative to the mean - If there are large positive errors next to each other or large negative errors close to each other → positive numerator - If there are errors above and below the mean next to each other, multiplying them together yields a negative value - This works to bring Moran’s I back closer to 0 - Neighbors & Calculating Spatial Lag - Queen → all neighbors surrounding a feature - If a 3x3 grid, all 8 cells surrounding the central cell) - Rook → the neighbors in the four cardinal directions relative to a feature - If a 3x3 grid, the four cells touching the faces of the central cell - Spatial Lag could be simply the average value of neighbors - Computing Moran’s I - moran.mc() in R - If Moran’s I is high (errors are clustered): - Add more spatial features - Try introducing spatial fixed effects"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes - Linear Regression",
    "section": "",
    "text": "Root Mean Squared Error →\nMAE\nCross Validation\nTrain/Testing Dataset\nLinear Regression Basics\nIndependent Variables NOT collinear\nVIF (&lt;5-10)\nConstant Variance (Heteroscedasticity)\nOutliers (Cooks Distance)\nNormality of Residuals"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#topics-discussed",
    "href": "weekly-notes/week-05-notes.html#topics-discussed",
    "title": "Week 5 Notes - Linear Regression",
    "section": "",
    "text": "Root Mean Squared Error →\nMAE\nCross Validation\nTrain/Testing Dataset\nLinear Regression Basics\nIndependent Variables NOT collinear\nVIF (&lt;5-10)\nConstant Variance (Heteroscedasticity)\nOutliers (Cooks Distance)\nNormality of Residuals"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#data-visualization",
    "href": "weekly-notes/week-03-notes.html#data-visualization",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "href": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\nWhen making a ggplot, the elements go as follows:\n\nData → what is your dataset?\nAesthetics → what variables map to visual properties?\nGeometries → how do you want to display the data?\nAdditional Layers → scales, themes, facets, annotations, etc.\nBroadly:\n\nggplot(data = your_data) +\n\naes(x = variable1, y = variable2) +\ngeom_something() +\nadditional_layers()\n\n\n\nArguments\n\nx, y → position\ncolor → point/line color\nfill → area fill color\nsize → point/line size\nshape → point shape\nalpha → transparency\nAethetics go inside aes(), constants go outside\nAesthetics are not for decorating, they are for changing data-related elements of your plot (WILL BE ON QUIZ)"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "href": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nIt’s like detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "href": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "R Tips from This Week",
    "text": "R Tips from This Week\n\nThe regex() function can generate regex expressions based on a string input."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nNew R functions or approaches\n\nfilter → subset rows\nselect → subset columns\nmutate → create new variable columns\nsummarize → perform operations on a grouped dataframe\ngroup_by → create a grouped dataframe based on a column\nrename → rename columns (use `newname`)\n%&gt;% (Piping) → tidyverse method of passing function outputs to another function\n\nQuarto features learned\n\nVarious markdown style shortcuts:\n\nBold = **\nItalic = *\nBold/Italic = ***\nCode Text = `\nLists = -\nHeaders = # (x2 or x3)\nLinks = [Link text] (link.com)\n\nRendering the webpage as we make edits\n\nChanging Global Options in RStudio to render on save\nUsing the “Render” button to force rendering updates"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nThe nuances of GitHub, but I think I’m getting most of it\n\nAreas needing more practice\n\nUsing GitHub"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nData manipulation in R will be useful for taking in data from web pages and restructuring it into a format that’ll be useful for modeling"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nLearning how to use GitHub and publish our pages\n\nHow I’ll apply this knowledge\n\nUsing GitHub for collaborative projects in the future"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html",
    "href": "labs/lab_2/assignment2_template.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#assignment-overview",
    "href": "labs/lab_2/assignment2_template.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab_2/assignment2_template.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\n\n\n# Load spatial data\n\n\n\n\n# Check that all data loaded correctly\n\nQuestions to answer: - How many hospitals are in your dataset? - How many census tracts? - What coordinate reference system is each dataset in?\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\n\n\n\n\n# Join to tract boundaries\n\nQuestions to answer: - What year of ACS data are you using? - How many tracts have missing income data? - What is the median income across all PA census tracts?\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition?\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\n\n\n# Aggregate statistics by county\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#part-2-comprehensive-visualization",
    "href": "labs/lab_2/assignment2_template.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab_2/assignment2_template.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nQuestions to answer: - What dataset did you choose and why? - What is the data source and date? - How many features does it contain? - What CRS is it in? Did you need to transform it?\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nExamples: - “Do vulnerable tracts have adequate public transit access to hospitals?” - “Are EMS stations appropriately located near vulnerable populations?” - “Do areas with low vehicle access have worse hospital access?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\n[Write your findings here]"
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab_2/assignment2_template.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "labs/lab_2/assignment2_template.html#submission-requirements",
    "href": "labs/lab_2/assignment2_template.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\nSubmit the correct and working links of your assignment on Canvas"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables.\n[write the code below - first add a code chunk]\n\nage_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*age\"))\n\nage_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Age\"))\n\n# Median Age by Sex: B01002_001\n\n\n\n0.5 Advanced Search Techniques\nYour Task: Learn more sophisticated search methods.\n\n# Search for multiple terms at once\nhousing_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*(rent|value)\"))\n\nprint(\"Housing cost variables:\")\n\n[1] \"Housing cost variables:\"\n\nhead(housing_vars, 10)\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B07002PR_004 Estimate!!Median age --!!Total:!!Moved from d… Median… &lt;NA&gt;     \n 2 B07002_004   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 3 B07002_005   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 4 B07011PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n 5 B07011_004   Estimate!!Median income in the past 12 months… Median… tract    \n 6 B07011_005   Estimate!!Median income in the past 12 months… Median… tract    \n 7 B07402PR_004 Estimate!!Median age --!!Total living in area… Median… &lt;NA&gt;     \n 8 B07402_004   Estimate!!Median age --!!Total living in area… Median… county   \n 9 B07402_005   Estimate!!Median age --!!Total living in area… Median… county   \n10 B07411PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n\n# Search excluding certain terms\nincome_not_family &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\") & \n         !str_detect(label, \"family\"))\n\nprint(\"Income variables (not family income):\")\n\n[1] \"Income variables (not family income):\"\n\nhead(income_not_family)\n\n# A tibble: 6 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n6 B06011_001   Estimate!!Median income in the past 12 months … Median… tract    \n\n# Case-insensitive search using regex\neducation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, regex(\"bachelor\", ignore_case = TRUE)))\n\nprint(\"Education variables:\")\n\n[1] \"Education variables:\"\n\nhead(education_vars, 5)\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06009PR_005 Estimate!!Total:!!Bachelor's degree             Place … &lt;NA&gt;     \n2 B06009PR_011 Estimate!!Total:!!Born in Puerto Rico:!!Bachel… Place … &lt;NA&gt;     \n3 B06009PR_017 Estimate!!Total:!!Born in the United States:!!… Place … &lt;NA&gt;     \n4 B06009PR_023 Estimate!!Total:!!Native; born elsewhere:!!Bac… Place … &lt;NA&gt;     \n5 B06009PR_029 Estimate!!Total:!!Foreign born:!!Bachelor's de… Place … &lt;NA&gt;     \n\n\n\n\n0.6 Interactive Exploration\nYour Task: Use RStudio’s viewer for easier searching.\n\n# Open the full variable list in RStudio viewer\n# This opens a searchable data table\nView(acs_vars_2022)\n\n# Pro tip: You can also search specific table groups\n# B01 = Age and Sex\n# B19 = Income  \n# B25 = Housing\ntable_b19 &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(name, \"^B19\"))  # ^ means \"starts with\"\n\nprint(\"All B19 (Income) table variables:\")\n\n[1] \"All B19 (Income) table variables:\"\n\nhead(table_b19, 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n\n0.7 Verify Your Variable Choices\nYour Task: Test your variables by getting a small sample of data.\n\n# Test the variables you found\ntest_vars &lt;- c(\n  total_pop = \"B01003_001\",      # Total population\n  median_income = \"B19013_001\",  # Median household income\n  median_age = \"B01002_001\"      # Median age\n)\n\n# Get data for just one state to test\ntest_data &lt;- get_acs(\n  geography = \"state\",\n  variables = test_vars,\n  state = \"PA\",\n  year = 2022\n)\n\n# Check that you got what you expected\ntest_data\n\n# A tibble: 3 × 5\n  GEOID NAME         variable        estimate   moe\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 42    Pennsylvania median_age          40.8   0.1\n2 42    Pennsylvania total_pop     12989208    NA  \n3 42    Pennsylvania median_income    73170   347  \n\n\n\n\n0.8 Common Variable Patterns\nReference guide for future use:\n\n# Common patterns to remember:\ncommon_variables &lt;- tribble(\n  ~concept, ~typical_code, ~description,\n  \"Total Population\", \"B01003_001\", \"Total population\",\n  \"Median Age\", \"B01002_001\", \"Median age of population\", \n  \"Median HH Income\", \"B19013_001\", \"Median household income\",\n  \"White Population\", \"B03002_003\", \"White alone population\",\n  \"Black Population\", \"B03002_004\", \"Black/African American alone\",\n  \"Hispanic Population\", \"B03002_012\", \"Hispanic or Latino population\",\n  \"Bachelor's Degree\", \"B15003_022\", \"Bachelor's degree or higher\",\n  \"Median Rent\", \"B25058_001\", \"Median contract rent\",\n  \"Median Home Value\", \"B25077_001\", \"Median value owner-occupied\"\n)\n\nprint(\"Common Census Variables:\")\n\n[1] \"Common Census Variables:\"\n\ncommon_variables\n\n# A tibble: 9 × 3\n  concept             typical_code description                  \n  &lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;                        \n1 Total Population    B01003_001   Total population             \n2 Median Age          B01002_001   Median age of population     \n3 Median HH Income    B19013_001   Median household income      \n4 White Population    B03002_003   White alone population       \n5 Black Population    B03002_004   Black/African American alone \n6 Hispanic Population B03002_012   Hispanic or Latino population\n7 Bachelor's Degree   B15003_022   Bachelor's degree or higher  \n8 Median Rent         B25058_001   Median contract rent         \n9 Median Home Value   B25077_001   Median value owner-occupied  \n\n\nKey Tips for Variable Hunting:\n\nStart with concepts - search for the topic you want (income, age, housing)\nLook for “Median” vs “Mean” - median is usually more policy-relevant\nCheck the universe - some variables are for “households,” others for “population”\nTest with small data before running large queries\nBookmark useful variables for future projects (type them in your weekly notes!)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", \", state_choice)))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name                     median_incomeE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Bucks County, Pennsylvania              107826\n2 Chester County, Pennsylvania            118574\n3 Montgomery County, Pennsylvania         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n# histogram of total population\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 50, fill = \"#73956F\", colour = \"grey25\", alpha = 0.9) +\n  labs(\n    title = \"Distribution of Total Population\",\n    x = \"Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous()\n\n\n\n\n\n\n\n\n\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_boxplot(fill = \"#73956F\", , width = 0.1) +\n  labs(\n    title = \"Total Population Distribution with Outliers\",\n    x = \"Population\"\n  ) +\n  ylim(-0.25, 0.25) +\n  theme_minimal() +\n  scale_x_continuous()\n\n\n\n\n\n\n\n# Identify the outlier counties\npop_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(total_popE, 0.25, na.rm = TRUE),\n    Q3 = quantile(total_popE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = total_popE &lt; (Q1 - 1.5 * IQR) | total_popE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, total_popE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\npop_outliers\n\n# A tibble: 7 × 2\n  county_name                       total_popE\n  &lt;chr&gt;                                  &lt;dbl&gt;\n1 Allegheny County, Pennsylvania       1245310\n2 Bucks County, Pennsylvania            645163\n3 Chester County, Pennsylvania          536474\n4 Delaware County, Pennsylvania         575312\n5 Lancaster County, Pennsylvania        553202\n6 Montgomery County, Pennsylvania       856399\n7 Philadelphia County, Pennsylvania    1593208"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship? - Answer → Yes, it condenses the data and produces a much more visual trend\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\nggplot(data = county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(fill = \"red\", alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = T, color = \"grey95\") +\n  labs(title = \"Median Age vs. Median Income of Pennsylvania Counties\", \n       x = \"Median Income (k$)\",\n       y = \"Median Age (yrs)\") +\n  theme_dark() +\n  scale_x_continuous(labels = dollar)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\ncounty_reliability &lt;- county_reliability %&gt;% \n  mutate(median_ageM_pct = (median_ageM/median_ageE)*100)\n\nggplot(data = county_reliability) +\n  aes(x = total_popE, y = median_ageM_pct) +\n  geom_point(color = \"purple3\", size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 5, colour = \"lightblue3\", linetype = \"dashed\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = median_ageM_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile?\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\n\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "ClassMaterials_Copy/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-well-cover",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-well-cover",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Algorithmic Decision Making\n\nWhat are algorithms in public policy?\nWhen algorithmic decision making goes wrong\nCurrent policy responses\n\nPart 2: Active Learning\n\nSmall group scenarios: designing ethical algorithms\nDiscussion and reflection\n\nPart 3: Census Data Foundations\n\nUnderstanding census data for policy analysis\nGeography and data availability\n\nPart 4: Hands-On Census Data with R\n\nLive demonstration of key functions\nPractice exercises"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#opening-question",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#opening-question",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Opening Question",
    "text": "Opening Question\nDiscuss with your table (1 minutes):\nWhat is an algorithm?\nThink beyond just computer code - how do you make decisions in daily life?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-is-an-algorithm",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#what-is-an-algorithm",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "What Is An Algorithm?",
    "text": "What Is An Algorithm?\nDefinition: A set of rules or instructions for solving a problem or completing a task\nExamples:\n\nRecipe for cooking\nDirections to get somewhere\n\nDecision tree for hiring\nComputer program that processes data to make predictions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#algorithmic-decision-making-in-government",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#algorithmic-decision-making-in-government",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Algorithmic Decision Making in Government",
    "text": "Algorithmic Decision Making in Government\nSystems used to assist or replace human decision-makers\nBased on predictions from models that process historical data containing:\n\nInputs (“features”, “predictors”, “independent variables”, “x”)\nOutputs (“labels”, “outcome”, “dependent variable”, “y”)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#real-world-examples",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#real-world-examples",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Real-World Examples",
    "text": "Real-World Examples\n\n\nCriminal Justice Recidivism risk scores for bail and sentencing decisions\n\nHousing & Finance\nMortgage lending and tenant screening algorithms\n\nHealthcare Patient care prioritization and resource allocation"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#clarifying-key-terms",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#clarifying-key-terms",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Clarifying Key Terms",
    "text": "Clarifying Key Terms\nData Science → Computer science/engineering focus on algorithms and methods\nData Analytics → Application of data science methods to other disciplines\nMachine Learning → Algorithms for classification & prediction that learn from data\nAI → Algorithms that adjust and improve across iterations (neural networks, etc.)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#public-sector-context",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#public-sector-context",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Public Sector Context",
    "text": "Public Sector Context\nLong history of government data collection:\n\nCivic registration systems\n\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental”)\nFocus on prediction rather than explanation\nHarder to interpret and explain"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-government-uses-algorithms",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-government-uses-algorithms",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Why Government Uses Algorithms",
    "text": "Why Government Uses Algorithms\nGovernments have limited budgets and need to serve everyone\nAlgorithmic decision making is especially appealing because it promises:\n\nEfficiency - process more cases faster\nConsistency - same rules applied to everyone\n\nObjectivity - removes human bias\nCost savings - fewer staff needed\n\nBut does it deliver on these promises?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#remember-data-analytics-is-subjective",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#remember-data-analytics-is-subjective",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Remember: Data Analytics Is Subjective",
    "text": "Remember: Data Analytics Is Subjective\nEvery step involves human choices:\n\nData cleaning decisions\nData coding or classification\n\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you put in the model\n\nThese choices embed human values and biases"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#section",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#section",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Healthcare Algorithm Bias\nThe Problem:\nAlgorithm used to identify high-risk patients for additional care systematically discriminated against Black patients\nWhat Went Wrong:\n\nAlgorithm used healthcare costs as a proxy for need\nBlack patients typically incur lower costs due to systemic inequities in access\n\nResult: Black patients under-prioritized despite equivalent levels of illness\n\nScale: Used by hospitals and insurers for over 200 million people annually\nSource: Obermeyer et al. (2019), Science"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#criminal-justice-algorithm-bias",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#criminal-justice-algorithm-bias",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Criminal Justice Algorithm Bias",
    "text": "Criminal Justice Algorithm Bias\nCOMPAS Recidivism Prediction:\nThe Problem:\n\nAlgorithm 2x as likely to falsely flag Black defendants as high risk\nWhite defendants often rated low risk even when they do reoffend\n\nWhy This Happens:\n\nHistorical arrest data reflects biased policing patterns\nSocioeconomic proxies correlate with race\n“Objective” data contains subjective human decisions\n\nSource: ProPublica investigation"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#dutch-welfare-fraud-detection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#dutch-welfare-fraud-detection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Dutch Welfare Fraud Detection",
    "text": "Dutch Welfare Fraud Detection\nThe Problem:\n\n“Black box” system operated in secrecy\nImpossible for individuals to understand or challenge decisions\nDisproportionately targeted vulnerable populations\n\nCourt Ruling:\n\nBreached privacy rights under European Convention on Human Rights\nHighlighted unfair profiling and discrimination\nSystem eventually shut down"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#small-group-challenge-10-minutes-we-keep-a-tight-ship-around-here.",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#small-group-challenge-10-minutes-we-keep-a-tight-ship-around-here.",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Small Group Challenge (10 Minutes! We keep a tight ship around here.)",
    "text": "Small Group Challenge (10 Minutes! We keep a tight ship around here.)\nAt your table, pick one scenario and answer three prompts.\nPrompts (plain English, no tech):\n\nProxy: What would you use to stand in for what you want?\nBlind spot: What data gap or historical bias could skew results?\nHarm + Guardrail: Who could be harmed, and one simple safeguard?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#pick-one-scenario",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#pick-one-scenario",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Pick one scenario",
    "text": "Pick one scenario\n\nEmergency response prioritization (natural disasters)\n\nSchool enrollment assignment\n\nAutomated traffic enforcement (red-light cameras)\n\nHousing assistance allocation\n\nPredictive policing"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#example-so-you-see-the-level",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#example-so-you-see-the-level",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Example (so you see the level)",
    "text": "Example (so you see the level)\nScenario: Emergency response\n\nProxy: 911 call volume → stand-in for “need”\n\nBlind spot: Under-calling where trust/connectivity is low\n\nHarm + Guardrail: Wealthier areas over-prioritized → add a vulnerability boost (age/disability) and a minimum-service floor per zone"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#discuss-at-your-table-8-minutes",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#discuss-at-your-table-8-minutes",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Discuss at your table (8 minutes)",
    "text": "Discuss at your table (8 minutes)\nAnswer these out loud and on one device or notepad:\n\nProxy → “We’d use ____ as a stand-in for ____.”\nBlind spot → “This could miss/undercount ____ because ____.”\nHarm + Guardrail → “Group ____ could be hurt by ____. We’d add ____ (one safeguard).”\n\nChoose ONE guardrail type:\n\nPrioritize vulnerable groups\n\nCap disparities across areas (simple rule)\n\nHuman review + appeals for edge cases\n\nReplace a bad proxy (collect the right thing)\n\nPublish criteria & run a periodic bias check"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#lightning-shares-23-tables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#lightning-shares-23-tables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Lightning shares (2–3 tables)",
    "text": "Lightning shares (2–3 tables)\nIn ≤20 seconds, say:\n\nScenario, one proxy, one harm, one guardrail\n\nClass quick poll: Would that guardrail help?\n\n👍 Green light\n👎 Red light"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-census-data-matters",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#why-census-data-matters",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Why Census Data Matters",
    "text": "Why Census Data Matters\nCensus data is the foundation for:\n\nUnderstanding community demographics\nAllocating government resources\n\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nConnection: The same demographic data used in census goes into many of the algorithms we analyzed"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-vs.-american-community-survey",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-vs.-american-community-survey",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Census vs. American Community Survey",
    "text": "Census vs. American Community Survey\n\n\nDecennial Census (2020)\n\nEveryone counted every 10 years\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS)\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced the old “long form” in 2005\nA big source of data we’ll use this semester"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#acs-estimates-what-you-need-to-know",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#acs-estimates-what-you-need-to-know",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "ACS Estimates: What You Need to Know",
    "text": "ACS Estimates: What You Need to Know\n1-Year Estimates (areas &gt; 65,000 people)\n\nMost current data, smallest sample\n\n5-Year Estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nWhat you’ll use most often\n\nKey Point: All ACS data comes with margins of error - we’ll learn to work with uncertainty"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-geography-hierarchy",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-geography-hierarchy",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Census Geography Hierarchy",
    "text": "Census Geography Hierarchy\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at:\n\nCounty level - state and regional planning\nCensus tract level - neighborhood analysis\nBlock group level - very local analysis (tempting, but big MOEs)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-innovation-differential-privacy",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#census-innovation-differential-privacy",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "2020 Census Innovation: Differential Privacy",
    "text": "2020 Census Innovation: Differential Privacy\nThe Challenge: Modern computing can “re-identify” individuals from census data\nThe Solution: Add mathematical “noise” to protect privacy while preserving overall patterns\nThe Controversy: Some places now show populations living “underwater” or other impossible results\nWhy This Matters: Even “objective” data involves subjective choices about privacy vs. accuracy. Also, the errors."
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#accessing-census-data-in-r",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#accessing-census-data-in-r",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Accessing Census Data in R",
    "text": "Accessing Census Data in R\nTraditional approach: Download CSV files from Census website\nModern approach: Use R packages to access data directly\nBenefits of programmatic access:\n\nAlways get latest data\nReproducible workflows\n\nAutomatic geographic boundaries\nBuilt-in error handling\n\nWe’ll use the tidycensus package starting in Lab 1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-acs-data-structure",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-acs-data-structure",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Understanding ACS Data Structure",
    "text": "Understanding ACS Data Structure\nData organized in tables:\n\nB19013 - Median Household Income\nB25003 - Housing Tenure (Own/Rent)\n\nB15003 - Educational Attainment\nB08301 - Commuting to Work\n\nEach table has multiple variables:\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\nYou’ll learn to find the right variables for your research questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-margins-of-error",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-margins-of-error",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Working with Margins of Error",
    "text": "Working with Margins of Error\nEvery ACS estimate comes with uncertainty\nRule of thumb:\n\nLarge MOE relative to estimate = less reliable\nSmall MOE relative to estimate = more reliable\n\nIn your analysis:\n\nAlways report MOE alongside estimates\nBe cautious comparing estimates with overlapping error margins\nConsider using 5-year estimates for greater reliability"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#two-types-of-census-data",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#two-types-of-census-data",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Two Types of Census Data",
    "text": "Two Types of Census Data\n\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\n\nPUMS - Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#when-new-data-comes-out",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#when-new-data-comes-out",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "When New Data Comes Out",
    "text": "When New Data Comes Out\nACS 1-year estimates: Released in September (previous year’s data)\nACS 5-year estimates: Released in December\nDecennial Census: Released on rolling schedule over 2-3 years\nFor Lab 1: We’ll use 2018-2022 ACS 5-year estimates (latest available)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-sources-youll-use",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-sources-youll-use",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Data Sources You’ll Use",
    "text": "Data Sources You’ll Use\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org) - Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database - Track changes over time"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#live-demo-setup",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#live-demo-setup",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Live Demo Setup",
    "text": "Live Demo Setup\nLet’s see tidycensus in action with some basic examples:\n\nlibrary(tidycensus)\n\nWarning: package 'tidycensus' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n# Set API key (you'll get yours for Lab 1)\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n\nFollow along: We’ll work through examples together, then you’ll practice in Lab 1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#basic-get_acs-function",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#basic-get_acs-function",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Basic get_acs() Function",
    "text": "Basic get_acs() Function\nMost important function you’ll use:\n\n# Get state-level population data\nstate_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",  # Total population\n  year = 2022,\n  survey = \"acs5\"\n)\n\nGetting data from the 2018-2022 5-year ACS\n\nglimpse(state_pop)\n\nRows: 52\nColumns: 5\n$ GEOID    &lt;chr&gt; \"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"…\n$ NAME     &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Co…\n$ variable &lt;chr&gt; \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B01003_001\", \"B010…\n$ estimate &lt;dbl&gt; 5028092, 734821, 7172282, 3018669, 39356104, 5770790, 3611317…\n$ moe      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nKey parameters: geography, variables, year, survey"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-the-output",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#understanding-the-output",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Understanding the Output",
    "text": "Understanding the Output\nEvery ACS result includes:\n\nGEOID - Geographic identifier\nNAME - Human-readable location name\n\nvariable - Census variable code\nestimate - The actual value\nmoe - Margin of error\n\nThis is the foundation for all your analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-multiple-variables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#working-with-multiple-variables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Working with Multiple Variables",
    "text": "Working with Multiple Variables\n\n# Get income and population for Pennsylvania counties\npa_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"  # Makes analysis easier\n)\n\nGetting data from the 2018-2022 5-year ACS\n\nhead(pa_data)\n\n# A tibble: 6 × 6\n  GEOID NAME                 total_popE total_popM median_incomeE median_incomeM\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 42001 Adams County, Penns…     104604         NA          78975           3334\n2 42003 Allegheny County, P…    1245310         NA          72537            869\n3 42005 Armstrong County, P…      65538         NA          61011           2202\n4 42007 Beaver County, Penn…     167629         NA          67194           1531\n5 42009 Bedford County, Pen…      47613         NA          58337           2606\n6 42011 Berks County, Penns…     428483         NA          74617           1191\n\n\nNote: output = \"wide\" gives you one row per place, multiple columns for variables"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-cleaning-essentials",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#data-cleaning-essentials",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Data Cleaning Essentials",
    "text": "Data Cleaning Essentials\nClean up messy geographic names:\n\npa_clean &lt;- pa_data %&gt;%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    # Remove \"County\" word\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Compare before and after\nselect(pa_clean, NAME, county_name)\n\n# A tibble: 67 × 2\n   NAME                           county_name\n   &lt;chr&gt;                          &lt;chr&gt;      \n 1 Adams County, Pennsylvania     Adams      \n 2 Allegheny County, Pennsylvania Allegheny  \n 3 Armstrong County, Pennsylvania Armstrong  \n 4 Beaver County, Pennsylvania    Beaver     \n 5 Bedford County, Pennsylvania   Bedford    \n 6 Berks County, Pennsylvania     Berks      \n 7 Blair County, Pennsylvania     Blair      \n 8 Bradford County, Pennsylvania  Bradford   \n 9 Bucks County, Pennsylvania     Bucks      \n10 Butler County, Pennsylvania    Butler     \n# ℹ 57 more rows\n\n\nFunctions you’ll use: str_remove(), str_extract(), str_replace()"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#calculating-data-reliability",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#calculating-data-reliability",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Calculating Data Reliability",
    "text": "Calculating Data Reliability\nThis is crucial for policy work:\n\npa_reliability &lt;- pa_clean %&gt;%\n  mutate(\n    # Calculate MOE as percentage of estimate\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 2),\n    \n    # Create reliability categories\n    reliability = case_when(\n      moe_percentage &lt; 5 ~ \"High Confidence\",\n      moe_percentage &gt;= 5 & moe_percentage &lt;= 10 ~ \"Moderate\",\n      moe_percentage &gt; 10 ~ \"Low Confidence\"\n    )\n  )\n\ncount(pa_reliability, reliability)\n\n# A tibble: 2 × 2\n  reliability         n\n  &lt;chr&gt;           &lt;int&gt;\n1 High Confidence    57\n2 Moderate           10\n\n\nKey functions: case_when() for categories, MOE calculations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#finding-patterns-with-dplyr",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#finding-patterns-with-dplyr",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Finding Patterns with dplyr",
    "text": "Finding Patterns with dplyr\n\nFind counties with highest uncertainty\nSummarize by reliability category\n\n\n# Find counties with highest uncertainty\nhigh_uncertainty &lt;- pa_reliability %&gt;%\n  filter(moe_percentage &gt; 8) %&gt;%\n  arrange(desc(moe_percentage)) %&gt;%\n  select(county_name, median_incomeE, moe_percentage)\n\n# Summary by reliability category  \nreliability_summary &lt;- pa_reliability %&gt;%\n  group_by(reliability) %&gt;%\n  summarize(\n    counties = n(),\n    avg_income = round(mean(median_incomeE, na.rm = TRUE), 0)\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#professional-tables",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#professional-tables",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Professional Tables",
    "text": "Professional Tables\nMaking results presentation-ready:\n\n# Create formatted table\nkable(high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"MOE %\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nCounties with Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE %\n\n\n\n\nForest\n46,188\n9.99\n\n\nSullivan\n62,910\n9.25\n\n\n\n\n\nKey points:\n\nUse kable() for professional formatting\nAdd descriptive column names and captions\n\nFormat numbers appropriately"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#quick-practice",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#quick-practice",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Quick Practice",
    "text": "Quick Practice\nTry this with a neighbor (5 minutes):\nUsing the pa_reliability data we just created:\n\nFilter for counties with “High Confidence” data\nArrange by median income (highest first)\n\nSelect county name and median income\nSlice the top 3 counties\n\nWe’ll share answers before moving on"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#policy-connection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#policy-connection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Policy Connection",
    "text": "Policy Connection\nWhy this matters:\n\nAlgorithmic fairness: Unreliable data can bias automated decisions\nResource allocation: Know which areas need extra attention\nEquity analysis: Some communities may be systematically under-counted\nProfessional credibility: Always assess your data quality\n\nThis connects directly to our algorithmic bias discussion"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#from-algorithms-to-analysis",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#from-algorithms-to-analysis",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "From Algorithms to Analysis",
    "text": "From Algorithms to Analysis\nToday’s key connections:\nAlgorithmic Decision Making → Understanding why your analysis matters for real policy decisions\nData Subjectivity → Why we emphasize transparent, reproducible methods in this class\nCensus Data → The foundation for most urban planning and policy analysis\nR Skills → The tools to do this work professionally and ethically"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions-for-reflection",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions-for-reflection",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Questions for Reflection",
    "text": "Questions for Reflection\nAs you work with data this semester, ask:\n\nWhat assumptions am I making in my data choices?\nWho might be excluded from my analysis?\nHow could my findings be misused if taken out of context?\nWhat would I want policymakers to understand about my methods?\n\nThese questions will make you a more thoughtful analyst and better future policymaker"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#before-next-class",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#before-next-class",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Before Next Class",
    "text": "Before Next Class\n\nComplete Lab 0 if you haven’t finished\nPost your weekly notes - reflect on today’s discussion\nStart Lab 1 - census data exploration (begins today!)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions",
    "href": "ClassMaterials_Copy/week-02/lectures/week2_slides.html#questions",
    "title": "Algorithmic Decision Making & Census Data",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-1-course-foundation",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-1-course-foundation",
    "title": "Welcome to MUSA 5080",
    "section": "Part 1: Course Foundation",
    "text": "Part 1: Course Foundation\n\nCourse overview and philosophy\nProfessional tools we’ll use this semester\nAssessment approach and portfolio development"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-2-github-version-control",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-2-github-version-control",
    "title": "Welcome to MUSA 5080",
    "section": "Part 2: GitHub & Version Control",
    "text": "Part 2: GitHub & Version Control\n\nGit fundamentals for data science\nGitHub Classroom workflow\nCollaborative coding practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-3-reproducible-research-tools",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-3-reproducible-research-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Part 3: Reproducible Research Tools",
    "text": "Part 3: Reproducible Research Tools\n\nQuarto for professional documentation\nMarkdown basics for clear communication\nRStudio settings for reproducibility"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-4-r-project-workflow",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-4-r-project-workflow",
    "title": "Welcome to MUSA 5080",
    "section": "Part 4: R Project Workflow",
    "text": "Part 4: R Project Workflow\n\nProject organization best practices\nFile management and relative paths\nWeekly workflow you’ll follow all semester"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-5-data-analysis-with-tidyverse",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-5-data-analysis-with-tidyverse",
    "title": "Welcome to MUSA 5080",
    "section": "Part 5: Data Analysis with tidyverse",
    "text": "Part 5: Data Analysis with tidyverse\n\ndplyr fundamentals and function patterns\nPipes for readable code\ngroup_by() and summarize() for policy analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-6-hands-on-setup",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#part-6-hands-on-setup",
    "title": "Welcome to MUSA 5080",
    "section": "Part 6: Hands-On Setup",
    "text": "Part 6: Hands-On Setup\n\nPortfolio repository creation\nLive demonstration of complete workflow\nYour first analysis in professional format"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-this-course-is-about",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-this-course-is-about",
    "title": "Welcome to MUSA 5080",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nAdvanced spatial analysis for urban planning and public policy\nData science tools within policy context\nFocus on understanding concepts rather than just completing code\nProfessional portfolio development using modern tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#unlike-private-sector-data-science",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#unlike-private-sector-data-science",
    "title": "Welcome to MUSA 5080",
    "section": "Unlike Private Sector Data Science",
    "text": "Unlike Private Sector Data Science\n\nNot just about optimization\nPublic goods, governance, equity considerations\nTransparency and interpretability are crucial\nAlgorithmic bias has real consequences for communities"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#this-semesters-innovation",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#this-semesters-innovation",
    "title": "Welcome to MUSA 5080",
    "section": "This Semester’s Innovation",
    "text": "This Semester’s Innovation\nProblem: AI tools making it easy to complete code without understanding\nSolution:\n\n40% weekly in-class quizzes (test conceptual understanding)\nLow-stakes portfolio assignments (focus on learning, not grades)\nGitHub-based workflow (professional skills)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-tools",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Tools?",
    "text": "Why These Tools?\nGitHub: Industry standard for version control and collaboration\nQuarto: Modern approach to reproducible research and documentation\nR: Powerful for spatial analysis and policy-focused statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#professional-development",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#professional-development",
    "title": "Welcome to MUSA 5080",
    "section": "Professional Development",
    "text": "Professional Development\nThese aren’t just “class tools” - they’re career tools:\n\nPortfolio employers can see\nVersion control skills for any data job\nProfessional documentation practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-git",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-git",
    "title": "Welcome to MUSA 5080",
    "section": "What is Git?",
    "text": "What is Git?\nVersion control system that tracks changes in files\nThink of it as:\n\n“Track changes” for code projects\nTime machine for your work\nCollaboration tool for teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nCloud hosting for Git repositories\n\nBackup your work in the cloud\n\nShare projects with others\nDeploy websites (like our portfolios)\nCollaborate on code projects"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#key-github-concepts",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#key-github-concepts",
    "title": "Welcome to MUSA 5080",
    "section": "Key GitHub Concepts",
    "text": "Key GitHub Concepts\nRepository (repo): Folder containing your project files\nCommit: Snapshot of your work at a point in time\nPush: Send your changes to GitHub cloud\nPull: Get latest changes from GitHub cloud"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#github-in-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#github-in-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "GitHub in This Course",
    "text": "GitHub in This Course\nYour workflow each week:\n1. Edit files in RStudio\n2. Commit changes with descriptive message  \n3. Push to GitHub\n4. Your portfolio website updates automatically\nThis becomes second nature by mid-semester!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github-classroom",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-github-classroom",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nEducational tool that:\n\nCreates individual repositories for each student\nDistributes assignments automatically\n\nEnables efficient feedback and grading\nTeaches professional Git workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-it-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-it-works",
    "title": "Welcome to MUSA 5080",
    "section": "How It Works",
    "text": "How It Works\n\nDr. Delmelle creates assignment with starter code\nYou accept assignment via special link\nGitHub creates your personal repository\nYou complete work in your repository\nTAs provide feedback through GitHub tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#benefits-for-you",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#benefits-for-you",
    "title": "Welcome to MUSA 5080",
    "section": "Benefits for You",
    "text": "Benefits for You\n\nIndividual workspace that’s yours to customize\nProfessional portfolio you can show employers\nVersion control practice for future jobs\nDirect feedback from instructors on your code"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-is-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nPublishing system that combines:\n\nCode (R, Python, etc.)\nText (explanations, analysis)\nOutput (plots, tables, results)\n\nInto professional documents"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nReproducible research:\n\nCode and explanation in one place\nOthers can re-run your analysis\nProfessional presentation\n\nCareer relevance:\n\nIndustry standard for data science communication\nCreates websites, PDFs, presentations\nUsed at major tech companies and government agencies"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-vs.-r-markdown",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-vs.-r-markdown",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto vs. R Markdown",
    "text": "Quarto vs. R Markdown\nIf you know R Markdown:\n\nQuarto is the “next generation”\nBetter website creation\nWorks with multiple programming languages\nSame basic concept, improved features"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-document-structure",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#quarto-document-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto Document Structure",
    "text": "Quarto Document Structure\nYAML header:\n---\ntitle: \"My Analysis\" \nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nR code chunk:\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/car_sales_data.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#text-formatting",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#text-formatting",
    "title": "Welcome to MUSA 5080",
    "section": "Text Formatting",
    "text": "Text Formatting\n**Bold text**\n*Italic text*\n***Bold and italic***\n`code text`\n~~Strikethrough~~\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#headers",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#headers",
    "title": "Welcome to MUSA 5080",
    "section": "Headers",
    "text": "Headers\n# Main Header\n## Section Header  \n### Subsection Header\nUse headers to organize your analysis sections."
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#lists",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#lists",
    "title": "Welcome to MUSA 5080",
    "section": "Lists",
    "text": "Lists\n## Unordered List\n- Item 1\n- Item 2\n  - Sub-item A\n  - Sub-item B\n\n## Ordered List  \n1. First item\n2. Second item\n3. Third item"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#links-and-images",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#links-and-images",
    "title": "Welcome to MUSA 5080",
    "section": "Links and Images",
    "text": "Links and Images\n[Link text](https://example.com)\n[Link to another page](about.qmd)\n![Alt text](path/to/image.png)\nEssential for professional portfolios:\n\nLink to data sources\nReference course materials\n\nInclude relevant images/plots"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-r-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-r-for-policy-analysis",
    "title": "Welcome to MUSA 5080",
    "section": "Why R for Policy Analysis?",
    "text": "Why R for Policy Analysis?\n\nFree and open source\nExcellent for spatial data\nStrong statistical capabilities\nLarge community in urban planning/policy\nReproducible research workflows"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-projects-essential-habit",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-projects-essential-habit",
    "title": "Welcome to MUSA 5080",
    "section": "RStudio Projects: Essential Habit",
    "text": "RStudio Projects: Essential Habit\nAlways work within projects for:\n\nOrganized file structure - data, scripts, outputs in one place\nRelative file paths - \"data/cars.csv\" works for everyone\n\nVersion control integration - Git works seamlessly\nReproducible workflow - others can run your code\n\nProfessional standard - employers expect this!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-benefits-for-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-benefits-for-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "Project Benefits for This Course",
    "text": "Project Benefits for This Course\nYour GitHub portfolio IS a project:\n# This works reliably in projects:\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# This breaks when shared:\ncar_data &lt;- read_csv(\"/Users/yourname/Desktop/cars.csv\")\nWe’ll work in projects all semester - builds good habits!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#creating-your-project",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#creating-your-project",
    "title": "Welcome to MUSA 5080",
    "section": "Creating Your Project",
    "text": "Creating Your Project\nStep 1: Clone your GitHub repository\ngit clone https://github.com/username/musa5080-portfolio.git\ncd musa5080-portfolio\nStep 2: Open in RStudio\n\nOpen RStudio\nFile → Open Project\nNavigate to your cloned folder\nSelect the .Rproj file"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-file-structure",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#project-file-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Project File Structure",
    "text": "Project File Structure\nOrganized structure from day one:\nmusa5080-portfolio/\n├── .Rproj\n├── .gitignore\n├── data/\n│   ├── raw/\n│   └── processed/\n├── scripts/\n├── docs/\n├── outputs/\n│   ├── figures/\n│   └── tables/\n└── week01/\n    ├── index.qmd\n    └── data/"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-this-structure-matters",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-this-structure-matters",
    "title": "Welcome to MUSA 5080",
    "section": "Why This Structure Matters",
    "text": "Why This Structure Matters\nProfessional habit:\n\nAnyone can understand your project layout\nScripts know where to find data files\nEasy to maintain as projects grow\nIndustry standard for data science teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#file-naming-conventions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#file-naming-conventions",
    "title": "Welcome to MUSA 5080",
    "section": "File Naming Conventions",
    "text": "File Naming Conventions\nBe consistent and descriptive:\n# Good examples:\nweek01_exploratory_analysis.qmd\n2025-09-08_census_data_cleaning.R\nphiladelphia_housing_2020-2024.csv\n\n# Avoid these:\nanalysis.qmd\ntemp.R\ndata.csv\nnew_version_final.qmd"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#working-with-data-files",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#working-with-data-files",
    "title": "Welcome to MUSA 5080",
    "section": "Working with Data Files",
    "text": "Working with Data Files\nBest practices for this course:\n# Raw data (never edit these!)\nraw_census &lt;- read_csv(here(\"data\", \"raw\", \"acs_2022_philadelphia.csv\"))\n\n# Process and save cleaned versions\nclean_census &lt;- raw_census %&gt;%\n  clean_names() %&gt;%\n  filter(!is.na(median_income))\n\nwrite_csv(clean_census, here(\"data\", \"processed\", \"acs_2022_clean.csv\"))\n\n# Use processed data in analysis\nanalysis_data &lt;- read_csv(here(\"data\", \"processed\", \"acs_2022_clean.csv\"))"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-settings-for-reproducibility",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#rstudio-settings-for-reproducibility",
    "title": "Welcome to MUSA 5080",
    "section": "RStudio Settings for Reproducibility",
    "text": "RStudio Settings for Reproducibility\nCritical settings to change RIGHT NOW:\nTools → Global Options → General:\n\nUncheck “Restore most recently opened project at startup”\nUncheck “Restore previously opened source documents”\n\nTools → Global Options → Workspace:\n\nUncheck “Restore .RData into workspace at startup”\n\nSet “Save workspace to .RData on exit” to “Never”"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-settings-matter",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-these-settings-matter",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Settings Matter",
    "text": "Why These Settings Matter\nWithout these changes:\n\nOld objects stick around between sessions\nCode appears to work but fails for others\nHidden dependencies break reproducibility\nYour portfolio assignments might not run for TAs!\n\nWith these settings:\n\nFresh environment every time\nCode must be complete and self-contained\nTrue reproducibility\nProfessional habits from day one"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#managing-r-environment",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#managing-r-environment",
    "title": "Welcome to MUSA 5080",
    "section": "Managing R Environment",
    "text": "Managing R Environment\nKeep your environment clean:\n# Start each session fresh\nrm(list = ls())\n\n# Use projects instead of setwd()\n# NEVER use setwd() in your code!\n\n# Check your working directory\ngetwd()  # Should be your project root\n\n# Use here() for all file paths\nhere(\"data\", \"my_file.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#tibbles-vs-data-frames",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#tibbles-vs-data-frames",
    "title": "Welcome to MUSA 5080",
    "section": "Tibbles vs Data Frames",
    "text": "Tibbles vs Data Frames\nTidyverse uses “tibbles” - enhanced data frames:\n# Traditional data frame\nclass(data)\n# [1] \"data.frame\"\n\n# Convert to tibble  \ncar_data &lt;- as_tibble(data)\nclass(car_data)\n# [1] \"tbl_df\" \"tbl\" \"data.frame\""
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-tibbles-are-better",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-tibbles-are-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why Tibbles Are Better",
    "text": "Why Tibbles Are Better\nSmarter printing:\n\nShows first 10 rows by default\nDisplays column types\nFits nicely on screen\n\nWe’ll see the difference with our car data…"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#essential-dplyr-functions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#essential-dplyr-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Essential dplyr Functions",
    "text": "Essential dplyr Functions\nWe’ll use these constantly:\n\nselect() - choose columns\nfilter() - choose rows\n\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#dplyr-function-rules",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#dplyr-function-rules",
    "title": "Welcome to MUSA 5080",
    "section": "dplyr Function Rules",
    "text": "dplyr Function Rules\nAll dplyr functions follow the same pattern:\n\nFirst argument is always a data frame\nSubsequent arguments describe which columns to operate on (using variable names without quotes)\nOutput is always a new data frame\n\nThis consistency makes dplyr predictable and easy to learn!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#function-pattern-examples",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#function-pattern-examples",
    "title": "Welcome to MUSA 5080",
    "section": "Function Pattern Examples",
    "text": "Function Pattern Examples\n# Rule 1: Data frame first\nselect(car_data, Manufacturer, Price)\nfilter(car_data, Price &gt; 20000)\nmutate(car_data, price_k = Price / 1000)\n\n# Rule 2: Column names without quotes\nselect(car_data, Manufacturer, Model, Price)  # Not \"Manufacturer\"\nfilter(car_data, Year &gt;= 2020, Mileage &lt; 50000)\n\n# Rule 3: Always returns a new data frame\nnew_data &lt;- select(car_data, Manufacturer, Price)\n# car_data is unchanged, new_data contains selected columns"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#live-demo-basic-dplyr",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#live-demo-basic-dplyr",
    "title": "Welcome to MUSA 5080",
    "section": "Live Demo: Basic dplyr",
    "text": "Live Demo: Basic dplyr\n\nlibrary(tidyverse)\n\n# Load car sales data\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# Basic exploration\nglimpse(car_data)\nnames(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#data-manipulation-pipeline",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#data-manipulation-pipeline",
    "title": "Welcome to MUSA 5080",
    "section": "Data Manipulation Pipeline",
    "text": "Data Manipulation Pipeline\nPipes (%&gt;%) are the magic of dplyr:\n# The power of pipes - read as \"then\"\ncar_summary &lt;- data %&gt;%\n  filter(`Year of manufacture` &gt;= 2020) %&gt;%      # Recent models only\n  select(Manufacturer, Model, Price, Mileage) %&gt;% # Key variables\n  mutate(price_k = Price / 1000) %&gt;%             # Convert to thousands\n  filter(Mileage &lt; 50000) %&gt;%                    # Low mileage cars\n  group_by(Manufacturer) %&gt;%                     # Group by brand\n  summarize(                                     # Calculate statistics\n    avg_price = mean(price_k, na.rm = TRUE),\n    count = n()\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-pipes",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-pipes",
    "title": "Welcome to MUSA 5080",
    "section": "Understanding Pipes",
    "text": "Understanding Pipes\nWhat is %&gt;%?\n\nTakes the output from the left side\nFeeds it as the first argument to the function on the right side\nThink: “and then…”\n\nWithout pipes (nested functions):\n# Hard to read - inside out!\ncar_summary &lt;- summarize(\n  group_by(\n    filter(\n      mutate(\n        select(filter(data, `Year of manufacture` &gt;= 2020), \n               Manufacturer, Model, Price, Mileage),\n        price_k = Price / 1000),\n      Mileage &lt; 50000),\n    Manufacturer),\n  avg_price = mean(price_k, na.rm = TRUE),\n  count = n()\n)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#without-pipes-multiple-objects",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#without-pipes-multiple-objects",
    "title": "Welcome to MUSA 5080",
    "section": "Without Pipes (Multiple Objects)",
    "text": "Without Pipes (Multiple Objects)\nAlternative: create many intermediate objects\n# Clutters your environment\nrecent_cars &lt;- filter(data, `Year of manufacture` &gt;= 2020)\nkey_vars &lt;- select(recent_cars, Manufacturer, Model, Price, Mileage)\nprice_thousands &lt;- mutate(key_vars, price_k = Price / 1000)\nlow_mileage &lt;- filter(price_thousands, Mileage &lt; 50000)\ngrouped_cars &lt;- group_by(low_mileage, Manufacturer)\ncar_summary &lt;- summarize(grouped_cars, \n                        avg_price = mean(price_k, na.rm = TRUE),\n                        count = n())\nProblems: Lots of temporary objects, hard to follow the logic"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-pipes-are-better",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#why-pipes-are-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why Pipes Are Better",
    "text": "Why Pipes Are Better\nReadable: Follow the logical flow of analysis\nEfficient: No temporary objects cluttering environment\nDebuggable: Easy to run line-by-line\nProfessional: Industry standard for data science"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#reading-pipes-aloud",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#reading-pipes-aloud",
    "title": "Welcome to MUSA 5080",
    "section": "Reading Pipes Aloud",
    "text": "Reading Pipes Aloud\ncar_data %&gt;%\n  filter(Price &gt; 15000) %&gt;%\n  select(Manufacturer, Price) %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price))\nRead as:\n“Take car_data, then filter for cars over $15,000, then select manufacturer and price columns, then group by manufacturer, then calculate average price”"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-group_by-and-summarize",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#understanding-group_by-and-summarize",
    "title": "Welcome to MUSA 5080",
    "section": "Understanding group_by() and summarize()",
    "text": "Understanding group_by() and summarize()\nThese functions work as a team:\n\ngroup_by() - sets up grouping for subsequent operations\nsummarize() - collapses rows into summary statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-group_by-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-group_by-works",
    "title": "Welcome to MUSA 5080",
    "section": "How group_by() Works",
    "text": "How group_by() Works\n# group_by() doesn't change what you see...\ncar_data %&gt;% \n  group_by(Manufacturer)\n\n# ...but it sets up invisible grouping for next operations\n# Look for: \"Groups: Manufacturer [5]\" in the output\nKey insight: group_by() prepares the data, doesn’t transform it yet"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-summarize-works",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#how-summarize-works",
    "title": "Welcome to MUSA 5080",
    "section": "How summarize() Works",
    "text": "How summarize() Works\n# Without grouping - one row of results\ncar_data %&gt;%\n  summarize(\n    avg_price = mean(Price, na.rm = TRUE),\n    total_cars = n()\n  )\n# Result: 1 row with overall averages\n\n# With grouping - one row per group\ncar_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\n    avg_price = mean(Price, na.rm = TRUE),\n    total_cars = n()\n  )\n# Result: 5 rows (one per manufacturer)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#before-and-after-example",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#before-and-after-example",
    "title": "Welcome to MUSA 5080",
    "section": "Before and After Example",
    "text": "Before and After Example\nOriginal data (imagine this):\nManufacturer  Price   Mileage\nToyota       25000    30000\nToyota       28000    15000  \nHonda        22000    45000\nHonda        30000    20000\nFord         35000    10000\nAfter group_by(Manufacturer) %&gt;% summarize(…):\nManufacturer  avg_price  total_cars\nToyota       26500      2\nHonda        26000      2  \nFord         35000      1"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#common-summarize-functions",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#common-summarize-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Common summarize() Functions",
    "text": "Common summarize() Functions\nEssential summary functions:\ncar_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\n    count = n(),                          # Number of rows\n    avg_price = mean(Price, na.rm = TRUE), # Average\n    med_price = median(Price, na.rm = TRUE), # Median  \n    min_price = min(Price, na.rm = TRUE),   # Minimum\n    max_price = max(Price, na.rm = TRUE),   # Maximum\n    std_dev = sd(Price, na.rm = TRUE)       # Standard deviation\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#policy-analysis-applications",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#policy-analysis-applications",
    "title": "Welcome to MUSA 5080",
    "section": "Policy Analysis Applications",
    "text": "Policy Analysis Applications\nPerfect for policy questions like:\n\nAverage household income by neighborhood\nCrime rates by police district\n\nHousing prices by year\nTransportation usage by demographic group\nEducational outcomes by school district\n\nPattern: group_by(category) %&gt;% summarize(metric)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#weekly-pattern",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#weekly-pattern",
    "title": "Welcome to MUSA 5080",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nMonday Class: - New concepts and methods - Hands-on coding practice - Lab work with TA support\nDuring Week: - Complete portfolio assignments - Weekly notes and reflection - Office hours for help"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#assessment-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#assessment-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "Assessment Philosophy",
    "text": "Assessment Philosophy\nFocus on understanding, not perfect code:\n\nWeekly quizzes test concepts\nPortfolio assignments build skills\nLow stakes encourage experimentation\nProfessional development throughout"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-development",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-development",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Development",
    "text": "Portfolio Development\nYour GitHub portfolio will include:\n\nWeekly learning reflections\nCompleted lab analyses\nProfessional documentation\nWork you can show employers"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-setup-process",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#portfolio-setup-process",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Setup Process",
    "text": "Portfolio Setup Process\n\nAccept GitHub Classroom assignment\nClone repository to your computer\n\nCustomize with your information\nEnable GitHub Pages\nComplete first analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-well-accomplish",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#what-well-accomplish",
    "title": "Welcome to MUSA 5080",
    "section": "What We’ll Accomplish",
    "text": "What We’ll Accomplish\nBy end of today:\n\nWorking portfolio repository\nLive website with your work\nFirst R analysis in professional format\nFamiliarity with workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#support-available",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#support-available",
    "title": "Welcome to MUSA 5080",
    "section": "Support Available",
    "text": "Support Available\n\nDr. Delmelle and TAs circulating during hands-on time\nOffice hours starting this week\nGitHub Issues for technical questions\nCanvas discussion for course questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#ready-to-get-started",
    "href": "ClassMaterials_Copy/week-01-intro/musa5080_week1_slides.html#ready-to-get-started",
    "title": "Welcome to MUSA 5080",
    "section": "Ready to Get Started?",
    "text": "Ready to Get Started?\nNext: Portfolio setup with GitHub Classroom\nRemember: This is a learning process - ask for help when you need it!"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#who-we-are",
    "href": "assignments/midterm/midterm_presentation.html#who-we-are",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Who We Are",
    "text": "Who We Are"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#research-question-and-motivation",
    "href": "assignments/midterm/midterm_presentation.html#research-question-and-motivation",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Research Question and Motivation",
    "text": "Research Question and Motivation\n\n\nResearch Question\nIdentify which predictors (spatial, structural, and/or socio-economic) contribute to the most accurate model for the City’s Automated Valuation Model\n\nMotivation\nResidential property tax assessments have equitable and financial implications for homeowners as well as fiscal implications for the city"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#data-sources",
    "href": "assignments/midterm/midterm_presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nProperty Sales: (Philadelphia, 2023-2024) City of Philadelphia - OpenDataPhilly\nSocio-Economics: United States Census - American Community Survey\nSpatial Features: City of Philadelphia - OpenDataPhilly"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#spatial-distribution-of-sale-prices-in-philadelphia",
    "href": "assignments/midterm/midterm_presentation.html#spatial-distribution-of-sale-prices-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Spatial Distribution of Sale Prices in Philadelphia",
    "text": "Spatial Distribution of Sale Prices in Philadelphia\n\n\n\n\n\n\n\n\n\nHigher sale prices are concentrated in Central and Northwest Philadelphia"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#factors-impacting-sale-price",
    "href": "assignments/midterm/midterm_presentation.html#factors-impacting-sale-price",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Factors Impacting Sale Price",
    "text": "Factors Impacting Sale Price\nThere is a notable relationship between sale price and total livable area"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#model-comparison",
    "href": "assignments/midterm/midterm_presentation.html#model-comparison",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nModel Performance Metrics\n\n\nModel\nRMSE\nMAE\nR²\n\n\n\n\nModel 1\n230,140.41\n15,174.13\n0.287\n\n\nModel 2\n215,430.39\n90,217.97\n0.380\n\n\nModel 3\n211,021.39\n90,144.74\n0.405\n\n\nModel 4\n196,081.17\n71,582.45\n0.490\n\n\n\n\nModel 1 (Structural) and Model 2 (Structural + Census) had the worst performance.\nSpatial features (Model 3) and interaction terms (Model 4) boosted model performance."
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#top-predictors",
    "href": "assignments/midterm/midterm_presentation.html#top-predictors",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Top Predictors",
    "text": "Top Predictors\n\nIn a wealthy neighborhood (\\(\\beta\\) = 50,306.450, p &lt; 0.01)\nNumber of bedrooms (\\(\\beta\\) = 33,544.290, p &lt; 0.01)\nNumber of bathrooms (\\(\\beta\\) = 29,510.940, p &lt; 0.01)\n\nInterpretation: Higher home values in wealthier neighborhoods make sense, especially historically affluent areas with reputational appeal and historic housing stock. Higher numbers of bedrooms and bathrooms tend to indicate more total livable area, which aligns with our observation that total livable area impacts sale price."
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#model-performance",
    "href": "assignments/midterm/midterm_presentation.html#model-performance",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance",
    "text": "Model Performance"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#hardest-to-predict",
    "href": "assignments/midterm/midterm_presentation.html#hardest-to-predict",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Hardest to Predict",
    "text": "Hardest to Predict\n\nNortheast and West Philly are hardest to predict due to the low income neighborhood limitation of our model"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#recommendations",
    "href": "assignments/midterm/midterm_presentation.html#recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Recommendations",
    "text": "Recommendations\n\nPolicymakers should be aware that houses in lower-income neighborhoods will not be accurately predicted by this model.\nPhysical features of a structure such as the number of bedrooms/bathrooms serve as strong predictors of home sale price. Nearby attractive amenities such as proximity to transit stations as well as distance from crime also contribute."
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#limitations-next-steps",
    "href": "assignments/midterm/midterm_presentation.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\n\nLimitations\n\nPhiladelphia sale prices don’t have a linear relationship, particularly among lower-priced homes\nVariables used were aggregated and not weighted\n\nNext Steps\n\nUpdate the model to account for lower neighborhoods and tailor spatial features to add texture and depth to the model"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#questions",
    "href": "assignments/midterm/midterm_presentation.html#questions",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Questions?",
    "text": "Questions?\nThank you! from Sujan, Henry, Ryan, Kavana, Chloe, and Nina :)   Contact us at: inquiries@tnc.com"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#scenario",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#learning-objectives",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-instructions",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-retrieval",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\nvt_data1 &lt;- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim &lt;- vt_data1 %&gt;% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %&gt;% head(5)\n\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct &lt;- vt_data1_trim %&gt;%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct &lt; 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct &gt;= 5 & med_hh_inc_Mpct &lt;= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct &gt; 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  summarize(count = n())\n\nvt_data1_reliability\n\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  &lt;chr&gt;               &lt;int&gt;\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct &lt;- vt_data1_MOEpct %&gt;%\n  arrange(desc(med_hh_inc_Mpct)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, col.names = c(\"County\", \"Median Household Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"), format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian Household Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nLamoille\n69886\n5846\n25977\n8.37\nModerate Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\nFranklin\n73633\n4436\n50101\n6.02\nModerate Confidence\n\n\nWindham\n65473\n3331\n45857\n5.09\nModerate Confidence\n\n\n\n\n\n Data Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]   Counties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %&gt;% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nAddison\n85870\n2958\n37434\n3.44\nHigh Confidence\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\n\n\n\nComment on the output: [write something :)]   While Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\ndemo_vars &lt;- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties &lt;- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 &lt;- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep &lt;- vt_data2 %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts &lt;- vt_data2_sep %&gt;%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat &lt;- vt_data2_pcts %&gt;% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo &lt;- vt_data2_pcts %&gt;% \n  group_by(COUNTY) %&gt;% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTract Count\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pct\n\n\n\n\nAddison\n10\n91.4\n0.8869\n2.50\n\n\nEssex\n3\n94.0\n0.0335\n1.54\n\n\nGrand Isle\n2\n91.0\n1.0907\n2.00"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct &lt;- vt_data2_sep %&gt;%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability &lt;- vt_data2_MOEpct %&gt;% \n  mutate(race_all_conf = ifelse(race_white_Mpct &gt; 15 | race_black_Mpct &gt; 15 | race_hispLatinoMpct &gt; 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats &lt;- vt_data2_reliability %&gt;%\n  group_by(race_all_conf) %&gt;% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nConfidence Category\nAvg Tract Pop\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pop\n\n\n\n\n1\n3383\n91.7\n0.816\n2.31\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont’s low population outside of city centers also contributes to increased unreliability of estimates."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract’s sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table &lt;- vt_data1_MOEpct %&gt;%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %&gt;% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMOE Pct\nConfidence Rating\nAlgorithm Recommendation\n\n\n\n\nAddison\n85870\n3.44\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBennington\n68558\n4.23\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCaledonia\n62964\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChittenden\n89494\n2.55\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n55247\n6.66\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n73633\n6.02\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrand Isle\n86639\n12.38\nLow Confidence\nRequires manual review or additional data\n\n\nLamoille\n69886\n8.37\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOrange\n74534\n3.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n63981\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRutland\n62641\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n77278\n3.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWindham\n65473\n5.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWindsor\n69492\n4.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\n\nAddison\nBennington\nCaledonia\nChittenden\nOrange\nOrleans\nRutland\nWashington\nWindsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\n\nEssex\nFranklin\nLamoille\nWindham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n\nGrand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nWhat are the underlying contributing factors for the dominance of white residents in Vermont?\nAs a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-checklist",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html",
    "href": "assignments/assignment1/Assignment1_BlankClone.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#scenario",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#learning-objectives",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#submission-instructions",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#data-retrieval",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\nvt_data1 &lt;- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim &lt;- vt_data1 %&gt;% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %&gt;% head(5)\n\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#data-quality-assessment",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct &lt;- vt_data1_trim %&gt;%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct &lt; 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct &gt;= 5 & med_hh_inc_Mpct &lt;= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct &gt; 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  summarize(count = n())\n\nvt_data1_reliability\n\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  &lt;chr&gt;               &lt;int&gt;\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct &lt;- vt_data1_MOEpct %&gt;%\n  arrange(desc(med_hh_inc_Mpct)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, format = \"html\")\n\n\n\n\nNAME\nmed_hh_incE\nmed_hh_incM\ntot_popE\nmed_hh_inc_Mpct\nmed_hh_inc_conf\n\n\n\n\nGrand Isle\n86639\n10729\n7335\n12.383569\nLow Confidence\n\n\nLamoille\n69886\n5846\n25977\n8.365052\nModerate Confidence\n\n\nEssex\n55247\n3679\n5976\n6.659185\nModerate Confidence\n\n\nFranklin\n73633\n4436\n50101\n6.024473\nModerate Confidence\n\n\nWindham\n65473\n3331\n45857\n5.087593\nModerate Confidence\n\n\n\n\n\n Data Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]   Counties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#focus-area-selection",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %&gt;% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nAddison\n85870\n2958\n37434\n3.44\nHigh Confidence\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\n\n\n\nComment on the output: [write something :)]   While Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#tract-level-demographics",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\ndemo_vars &lt;- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties &lt;- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 &lt;- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep &lt;- vt_data2 %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts &lt;- vt_data2_sep %&gt;%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#demographic-analysis",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat &lt;- vt_data2_pcts %&gt;% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo &lt;- vt_data2_pcts %&gt;% \n  group_by(COUNTY) %&gt;% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTract Count\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pct\n\n\n\n\nAddison\n10\n91.4\n0.8869\n2.50\n\n\nEssex\n3\n94.0\n0.0335\n1.54\n\n\nGrand Isle\n2\n91.0\n1.0907\n2.00"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct &lt;- vt_data2_sep %&gt;%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability &lt;- vt_data2_MOEpct %&gt;% \n  mutate(race_all_conf = ifelse(race_white_Mpct &gt; 15 | race_black_Mpct &gt; 15 | race_hispLatinoMpct &gt; 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#pattern-analysis",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats &lt;- vt_data2_reliability %&gt;%\n  group_by(race_all_conf) %&gt;% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nConfidence Category\nAvg Tract Pop\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pop\n\n\n\n\n1\n3383\n91.7\n0.816\n2.31\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont’s low population outside of city centers also contributes to increased unreliability of estimates."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract’s sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#specific-recommendations",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table &lt;- vt_data1_MOEpct %&gt;%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %&gt;% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMOE Pct\nConfidence Rating\nAlgorithm Recommendation\n\n\n\n\nAddison\n85870\n3.44\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBennington\n68558\n4.23\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCaledonia\n62964\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChittenden\n89494\n2.55\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n55247\n6.66\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n73633\n6.02\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrand Isle\n86639\n12.38\nLow Confidence\nRequires manual review or additional data\n\n\nLamoille\n69886\n8.37\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOrange\n74534\n3.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n63981\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRutland\n62641\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n77278\n3.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWindham\n65473\n5.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWindsor\n69492\n4.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algoLarithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\n\nAddison\nBennington\nCaledonia\nChittenden\nOrange\nOrleans\nRutland\nWashington\nWindsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\n\nEssex\nFranklin\nLamoille\nWindham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n\nGrand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates."
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nWhat are the underlying contributing factors for the dominance of white residents in Vermont?\nAs a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?"
  },
  {
    "objectID": "assignments/assignment1/Assignment1_BlankClone.html#submission-checklist",
    "href": "assignments/assignment1/Assignment1_BlankClone.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(pacman)\np_load(tidyverse, tidycensus, tigris, sf, knitr)\n\n# Load spatial data\nfips_code_pa &lt;- 42\ndata_year &lt;- 2022\n\ncounties &lt;- counties(state = fips_code_pa,\n                     year = data_year,\n                     progress_bar = F)\ntracts &lt;- tracts(state = fips_code_pa,\n                 year = data_year,\n                 progress_bar = F)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = T)\n\ndata_list &lt;- mget(c(\"counties\", \"tracts\", \"hospitals\"))\n\n# Check that all data loaded correctly\npaste0(\"Hospital Count = \", nrow(hospitals))\n\n[1] \"Hospital Count = 223\"\n\npaste0(\"Census Tract Count = \", nrow(tracts))\n\n[1] \"Census Tract Count = 3446\"\n\ncrs_df &lt;- data.frame(Datum = sapply(data_list, function(x) {st_crs(x, parameters = T)$Name}),\n                     EPSG = sapply(data_list, function(x) {st_crs(x, parameters = T)$epsg}))\n\nkable(crs_df)\n\n\n\n\n\nDatum\nEPSG\n\n\n\n\ncounties\nNAD83\n4269\n\n\ntracts\nNAD83\n4269\n\n\nhospitals\nWGS 84\n4326\n\n\n\n\n\n\n\n\n\n\nvariable_names &lt;- tidycensus::load_variables(2022, \"acs5\")\n\n# Get demographic data from ACS\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\")\n\npop_over65_vars &lt;- c(\"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\",\n              \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\")\n\npopTot_medInc &lt;- get_acs(geography = \"tract\",\n                     variable = vars,\n                     year = data_year,\n                     state = fips_code_pa,\n                     output = \"wide\")\n\npop_over65 &lt;- get_acs(geography = \"tract\",\n                      variable = pop_over65_vars,\n                      year = data_year,\n                      state = fips_code_pa)\n\npop_over65_sum &lt;- pop_over65 %&gt;% \n  group_by(GEOID) %&gt;% \n  summarise(pop_elderlyE = sum(estimate),\n            pop_elderlyM = sum(moe))\n\ndemo_vars &lt;- left_join(popTot_medInc, pop_over65_sum, by = \"GEOID\")\n\n# Join to tract boundaries\ndemo_vars_sf &lt;- left_join(demo_vars, tracts %&gt;% select(GEOID), by = \"GEOID\") %&gt;% \n  st_as_sf()\n\n# Separate out Tract/County names\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n\n# Answers to questions\npaste0(\"ACS Year = \", data_year)\n\n[1] \"ACS Year = 2022\"\n\npaste0(\"Count of Tracts with Missing Income Data = \", sum(is.na(demo_vars_sf$med_hh_incE)))\n\n[1] \"Count of Tracts with Missing Income Data = 63\"\n\npaste0(\"Median Income across All PA Census Tracts = \", median(demo_vars_sf$med_hh_incE, na.rm = T))\n\n[1] \"Median Income across All PA Census Tracts = 70188\"\n\n\n\n\n\n\n\n# Filter for vulnerable tracts based on criteria\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(pop_elderly_pct = round((pop_elderlyE / pop_totE)*100, 2),\n         low_inc = as.integer(med_hh_incE &lt; 50000),\n         high_elderly = as.integer(pop_elderly_pct &gt; 16.8),\n         vulnerable = case_when(low_inc == 1 & high_elderly == 1 ~ 1,\n                                .default = 0))\n\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\nkable(head(vulnerable_tracts %&gt;% \n             select(TRACT, COUNTY, pop_totE, med_hh_incE, pop_elderlyE, pop_elderly_pct) %&gt;% \n             st_drop_geometry()),\n      col.names = c(\"Tract\", \"County\", \"Total Population\", \"Median HH Inc ($)\", \"Elderly Population\", \"Percent Elderly\"),\n      caption = \"Example tracts with low median household income and a high percentage of elderly residents:\")\n\n\nExample tracts with low median household income and a high percentage of elderly residents:\n\n\n\n\n\n\n\n\n\n\nTract\nCounty\nTotal Population\nMedian HH Inc ($)\nElderly Population\nPercent Elderly\n\n\n\n\n315.02\nAdams\n3908\n46109\n698\n17.86\n\n\n305.00\nAllegheny\n3044\n36932\n544\n17.87\n\n\n501.00\nAllegheny\n1561\n30036\n345\n22.10\n\n\n506.00\nAllegheny\n1960\n42951\n407\n20.77\n\n\n509.00\nAllegheny\n1380\n12740\n236\n17.10\n\n\n709.00\nAllegheny\n4196\n49421\n721\n17.18\n\n\n\n\n\nAccording to an article in Philadelphia Today, the household poverty threshold in 2023 was $30,000. While this is the official threshold - and likely higher than it was in 2022 - healthcare costs and a lack of consistent income post-retirement for elderly folks likely result in additional financial strain even if they have income levels slightly greater than $30,000. For this analysis, a low-income threshold of $50,000 was chosen instead to encompass these groups of residents.\nBetween 2010 and 2020, the people aged 65 and older in the United States exceeded 1 in 6 residents (16.8%). This percentage was used as the threshold for having an abnormally high elderly population.\n\n# count of tracts that meet vulnerability critetia\npaste0(\"Count of Vulnerable Tracts = \", nrow(vulnerable_tracts))\n\n[1] \"Count of Vulnerable Tracts = 258\"\n\n# percent of census tracts considered vulnerable\npaste0(\"Percentage of PA Tracts Considered Vulnerable = \", round((nrow(vulnerable_tracts)/nrow(demo_vars_sf))*100, 2), \"%\")\n\n[1] \"Percentage of PA Tracts Considered Vulnerable = 7.49%\"\n\n\n\n\n\n\nA projected coordinate system centered around southern Pennsylvania (EPSG 2272) was chosen for this analysis in order to accurately calculate distances to the nearest hospital, unlike the original geographic coordinate systems each layer was in previously which are in units of decimal degrees. This coordinate system is based on the NAD83 datum and has units of US Survey Feet.\n\n# Transform to appropriate projected CRS\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  st_transform(2272)\nhospitals &lt;- hospitals %&gt;% \n  st_transform(2272)\n\n# Calculate distance matrix for each tract centroid to the nearest hospital\nhospital_dist_matrix &lt;- demo_vars_sf %&gt;%\n  st_centroid() %&gt;% \n  st_distance(hospitals)\n\nhospital_dist_min &lt;- apply(hospital_dist_matrix, 1, min)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  mutate(hospital_dist = round((hospital_dist_min)/5280, 2))\n\n# Recreate vulnerable tracts df with hospital_dist variable\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\n# Average distance to hospitals\npaste0(\"Average Distance from Vulnerable Tracts to the Nearest Hospital = \",\n       round(mean(vulnerable_tracts$hospital_dist), 2), \n       \" mi\")\n\n[1] \"Average Distance from Vulnerable Tracts to the Nearest Hospital = 3.24 mi\"\n\n# Maximum distance to hospitals\npaste0(\"Maximum Distance from Tract Centroids to Nearest Hospital = \",\n       max(vulnerable_tracts$hospital_dist),\n       \" mi\")\n\n[1] \"Maximum Distance from Tract Centroids to Nearest Hospital = 18.67 mi\"\n\n# Count of vulnerable tracts more than 15 mi from the nearest hospital\npaste0(\"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = \",\n       sum(vulnerable_tracts$hospital_dist &gt; 15))\n\n[1] \"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = 7\"\n\n\n\n\n\n\n“Underserved” tracts are defined here as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\n# Number of underserved tracts\npaste0(\"Number of Underserved Tracts = \", sum(vulnerable_tracts$underserved))\n\n[1] \"Number of Underserved Tracts = 7\"\n\n# Percentage of vulnerable tracts that are underserved\npaste0(\"Percentage of Vulnerable Tracts that are Underserved = \",\n       round(sum(vulnerable_tracts$underserved)/nrow(vulnerable_tracts)*100, 2),\n       \"%\")\n\n[1] \"Percentage of Vulnerable Tracts that are Underserved = 2.71%\"\n\n\nThe percentage of vulnerable tracts that are underserved is rather low in Pennsylvania, which is surprising. Pennsylvania has many rural communities with sparsely distributed resources, including hospitals. The distribution of underserved census tracts in Pennsylvania do appear to be in rural, central PA counties (see the table below for the counties associated with these seven tracts); however, it is puzzling how few of them there are.\n\n# Table of the counties associated with the seven vulnerable tracts that are underserved in PA\nkable(vulnerable_tracts %&gt;% \n        filter(underserved == 1) %&gt;%\n        select(TRACT, COUNTY) %&gt;%\n        st_drop_geometry(),\n      col.names = c(\"Tract\", \"County\"),\n      caption = \"Underserved Vulnerable Tracts in PA\")\n\n\nUnderserved Vulnerable Tracts in PA\n\n\nTract\nCounty\n\n\n\n\n9601.00\nCameron\n\n\n3306.00\nClearfield\n\n\n3316.00\nClearfield\n\n\n5301.00\nForest\n\n\n5302.00\nForest\n\n\n702.01\nJuniata\n\n\n3003.11\nMonroe\n\n\n\n\n\n\n\n\n\n\n# Spatial join tracts to counties\n# Spatial joins were not used because spatial inconsistencies between the layers were producing duplicate instances of tracts across multiple counties despite everything being in the same crs. Tabular joins were more effective.\n\ncounties &lt;- counties %&gt;% \n  st_transform(2272)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\ndemo_vars &lt;- demo_vars_sf %&gt;% \n  st_drop_geometry()\n\ndemo_vars_cty &lt;- left_join(counties %&gt;% select(GEOID, NAME),\n                         demo_vars, by = c(\"NAME\" = \"COUNTY\"))\n\n# Aggregate statistics by county\ndemo_vars_cty_stats &lt;- demo_vars_cty %&gt;% \n  group_by(NAME) %&gt;% \n  summarise(pop_tot_agg = sum(pop_totE, na.rm = T),\n            med_hh_inc_agg = median(med_hh_incE, na.rm = T), \n            pop_elderly_agg = sum(pop_elderlyE, na.rm = T),\n            low_inc_cnt = sum(low_inc, na.rm = T),\n            vulnerable_cnt = sum(vulnerable, na.rm = T),\n            underserved_cnt = sum(underserved, na.rm = T),\n            hospital_dist_avg = round(mean(hospital_dist, na.rm = T), 2),\n            pop_vulnerable = sum(pop_elderlyE*vulnerable)) %&gt;% \n  mutate(pop_elderly_pct_agg = round((pop_elderly_agg/pop_tot_agg)*100, 2),\n         underserved_pct = round((underserved_cnt/vulnerable_cnt)*100, 2))\n\nAfter conducting this analysis, there are only 5 counties that have underserved census tracts: Cameron, Clearfield, Forest, Juniata, and Monroe.\n\nunderserved_pct_top &lt;- demo_vars_cty_stats %&gt;%\n  select(NAME, underserved_pct) %&gt;% \n  arrange(-underserved_pct) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(underserved_pct_top,\n      col.names = c(\"County\", \"% of Vulnerable Tracts Underserved\"))\n\n\n\n\nCounty\n% of Vulnerable Tracts Underserved\n\n\n\n\nCameron\n100.00\n\n\nForest\n100.00\n\n\nMonroe\n100.00\n\n\nClearfield\n66.67\n\n\nJuniata\n50.00\n\n\n\n\n\nThere are also only five counties that have an average distance to the nearest hospital greater than 15 miles: Cameron, Forest, Juniata, Pike, and Sullivan. However, due to the nature of aggregating tract-level data to the county level, Pike and Sullivan have no vulnerable tracts and thus have no vulnerable population (which was calculated by isolating tracts considered vulnerable and summing their elderly population). If considering living far from hospitals to include any distance greater than 10 miles instead of 15, the top five counties with the greatest vulnerable population are:\n\nvulnerable_pop_top &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;%\n  arrange(-pop_vulnerable) %&gt;% \n  select(NAME, pop_vulnerable) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(vulnerable_pop_top, col.names = c(\"County\", \"Vulnerable Population\"))\n\n\n\n\nCounty\nVulnerable Population\n\n\n\n\nClearfield\n2083\n\n\nForest\n1593\n\n\nJuniata\n1332\n\n\nBradford\n751\n\n\nCameron\n428\n\n\n\n\n\nThere is a pattern that counties with underserved elderly populations typically lie in the more rural areas of central and northeastern Pennsylvania, far from major city centers like Philadelphia and Pittsburgh that have high concentrations of concentrated hospitals and medical services.\n\ndemo_vars_cty_stats &lt;- demo_vars_cty_stats %&gt;% \n  mutate(underserved_cnt_factor = as.factor(underserved_cnt))\nplot(demo_vars_cty_stats[,\"underserved_cnt_factor\"],\n     pal = c(\"grey\", \"green4\", \"steelblue\"),\n     main = \"Count of Underserved Census Tracts\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop10_priority &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;% \n  arrange(-pop_vulnerable) %&gt;% \n  head(10) %&gt;% \n  select(NAME, pop_tot_agg, med_hh_inc_agg, pop_elderly_pct_agg, pop_vulnerable, hospital_dist_avg) %&gt;% \n  st_drop_geometry()\n\n# Create and format priority counties table\nkable(top10_priority,\n      col.names = c(\"County\", \"Total Population\", \"Median HH Income ($)\", \"% Elderly Population\", \"Vulnerable Population\", \"Mean Distance to Nearest Hospital (mi)\"), format.args = list(big.mark = \",\"),\n      caption = \"Top Ten Counties to Target for Healthcare Investment\")\n\n\nTop Ten Counties to Target for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population\nMedian HH Income ($)\n% Elderly Population\nVulnerable Population\nMean Distance to Nearest Hospital (mi)\n\n\n\n\nClearfield\n79,707\n54,033.5\n21.07\n2,083\n13.13\n\n\nForest\n6,959\n45,728.0\n22.89\n1,593\n18.40\n\n\nJuniata\n23,535\n64,173.0\n20.56\n1,332\n15.01\n\n\nBradford\n60,159\n58,866.0\n21.74\n751\n10.18\n\n\nCameron\n4,536\n45,279.5\n27.98\n428\n19.07\n\n\nMonroe\n168,128\n81,127.0\n18.27\n314\n10.05\n\n\nPerry\n45,941\n74,562.5\n19.37\n0\n13.26\n\n\nPike\n58,996\n79,479.5\n22.99\n0\n19.22\n\n\nPotter\n16,390\n54,904.0\n24.87\n0\n11.02\n\n\nSnyder\n39,797\n66,042.0\n19.83\n0\n14.47"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(pacman)\np_load(tidyverse, tidycensus, tigris, sf, knitr)\n\n# Load spatial data\nfips_code_pa &lt;- 42\ndata_year &lt;- 2022\n\ncounties &lt;- counties(state = fips_code_pa,\n                     year = data_year,\n                     progress_bar = F)\ntracts &lt;- tracts(state = fips_code_pa,\n                 year = data_year,\n                 progress_bar = F)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = T)\n\ndata_list &lt;- mget(c(\"counties\", \"tracts\", \"hospitals\"))\n\n# Check that all data loaded correctly\npaste0(\"Hospital Count = \", nrow(hospitals))\n\n[1] \"Hospital Count = 223\"\n\npaste0(\"Census Tract Count = \", nrow(tracts))\n\n[1] \"Census Tract Count = 3446\"\n\ncrs_df &lt;- data.frame(Datum = sapply(data_list, function(x) {st_crs(x, parameters = T)$Name}),\n                     EPSG = sapply(data_list, function(x) {st_crs(x, parameters = T)$epsg}))\n\nkable(crs_df)\n\n\n\n\n\nDatum\nEPSG\n\n\n\n\ncounties\nNAD83\n4269\n\n\ntracts\nNAD83\n4269\n\n\nhospitals\nWGS 84\n4326\n\n\n\n\n\n\n\n\n\n\nvariable_names &lt;- tidycensus::load_variables(2022, \"acs5\")\n\n# Get demographic data from ACS\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\")\n\npop_over65_vars &lt;- c(\"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\",\n              \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\")\n\npopTot_medInc &lt;- get_acs(geography = \"tract\",\n                     variable = vars,\n                     year = data_year,\n                     state = fips_code_pa,\n                     output = \"wide\")\n\npop_over65 &lt;- get_acs(geography = \"tract\",\n                      variable = pop_over65_vars,\n                      year = data_year,\n                      state = fips_code_pa)\n\npop_over65_sum &lt;- pop_over65 %&gt;% \n  group_by(GEOID) %&gt;% \n  summarise(pop_elderlyE = sum(estimate),\n            pop_elderlyM = sum(moe))\n\ndemo_vars &lt;- left_join(popTot_medInc, pop_over65_sum, by = \"GEOID\")\n\n# Join to tract boundaries\ndemo_vars_sf &lt;- left_join(demo_vars, tracts %&gt;% select(GEOID), by = \"GEOID\") %&gt;% \n  st_as_sf()\n\n# Separate out Tract/County names\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n\n# Answers to questions\npaste0(\"ACS Year = \", data_year)\n\n[1] \"ACS Year = 2022\"\n\npaste0(\"Count of Tracts with Missing Income Data = \", sum(is.na(demo_vars_sf$med_hh_incE)))\n\n[1] \"Count of Tracts with Missing Income Data = 63\"\n\npaste0(\"Median Income across All PA Census Tracts = \", median(demo_vars_sf$med_hh_incE, na.rm = T))\n\n[1] \"Median Income across All PA Census Tracts = 70188\"\n\n\n\n\n\n\n\n# Filter for vulnerable tracts based on criteria\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(pop_elderly_pct = round((pop_elderlyE / pop_totE)*100, 2),\n         low_inc = as.integer(med_hh_incE &lt; 50000),\n         high_elderly = as.integer(pop_elderly_pct &gt; 16.8),\n         vulnerable = case_when(low_inc == 1 & high_elderly == 1 ~ 1,\n                                .default = 0))\n\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\nkable(head(vulnerable_tracts %&gt;% \n             select(TRACT, COUNTY, pop_totE, med_hh_incE, pop_elderlyE, pop_elderly_pct) %&gt;% \n             st_drop_geometry()),\n      col.names = c(\"Tract\", \"County\", \"Total Population\", \"Median HH Inc ($)\", \"Elderly Population\", \"Percent Elderly\"),\n      caption = \"Example tracts with low median household income and a high percentage of elderly residents:\")\n\n\nExample tracts with low median household income and a high percentage of elderly residents:\n\n\n\n\n\n\n\n\n\n\nTract\nCounty\nTotal Population\nMedian HH Inc ($)\nElderly Population\nPercent Elderly\n\n\n\n\n315.02\nAdams\n3908\n46109\n698\n17.86\n\n\n305.00\nAllegheny\n3044\n36932\n544\n17.87\n\n\n501.00\nAllegheny\n1561\n30036\n345\n22.10\n\n\n506.00\nAllegheny\n1960\n42951\n407\n20.77\n\n\n509.00\nAllegheny\n1380\n12740\n236\n17.10\n\n\n709.00\nAllegheny\n4196\n49421\n721\n17.18\n\n\n\n\n\nAccording to an article in Philadelphia Today, the household poverty threshold in 2023 was $30,000. While this is the official threshold - and likely higher than it was in 2022 - healthcare costs and a lack of consistent income post-retirement for elderly folks likely result in additional financial strain even if they have income levels slightly greater than $30,000. For this analysis, a low-income threshold of $50,000 was chosen instead to encompass these groups of residents.\nBetween 2010 and 2020, the people aged 65 and older in the United States exceeded 1 in 6 residents (16.8%). This percentage was used as the threshold for having an abnormally high elderly population.\n\n# count of tracts that meet vulnerability critetia\npaste0(\"Count of Vulnerable Tracts = \", nrow(vulnerable_tracts))\n\n[1] \"Count of Vulnerable Tracts = 258\"\n\n# percent of census tracts considered vulnerable\npaste0(\"Percentage of PA Tracts Considered Vulnerable = \", round((nrow(vulnerable_tracts)/nrow(demo_vars_sf))*100, 2), \"%\")\n\n[1] \"Percentage of PA Tracts Considered Vulnerable = 7.49%\"\n\n\n\n\n\n\nA projected coordinate system centered around southern Pennsylvania (EPSG 2272) was chosen for this analysis in order to accurately calculate distances to the nearest hospital, unlike the original geographic coordinate systems each layer was in previously which are in units of decimal degrees. This coordinate system is based on the NAD83 datum and has units of US Survey Feet.\n\n# Transform to appropriate projected CRS\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  st_transform(2272)\nhospitals &lt;- hospitals %&gt;% \n  st_transform(2272)\n\n# Calculate distance matrix for each tract centroid to the nearest hospital\nhospital_dist_matrix &lt;- demo_vars_sf %&gt;%\n  st_centroid() %&gt;% \n  st_distance(hospitals)\n\nhospital_dist_min &lt;- apply(hospital_dist_matrix, 1, min)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  mutate(hospital_dist = round((hospital_dist_min)/5280, 2))\n\n# Recreate vulnerable tracts df with hospital_dist variable\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\n# Average distance to hospitals\npaste0(\"Average Distance from Vulnerable Tracts to the Nearest Hospital = \",\n       round(mean(vulnerable_tracts$hospital_dist), 2), \n       \" mi\")\n\n[1] \"Average Distance from Vulnerable Tracts to the Nearest Hospital = 3.24 mi\"\n\n# Maximum distance to hospitals\npaste0(\"Maximum Distance from Tract Centroids to Nearest Hospital = \",\n       max(vulnerable_tracts$hospital_dist),\n       \" mi\")\n\n[1] \"Maximum Distance from Tract Centroids to Nearest Hospital = 18.67 mi\"\n\n# Count of vulnerable tracts more than 15 mi from the nearest hospital\npaste0(\"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = \",\n       sum(vulnerable_tracts$hospital_dist &gt; 15))\n\n[1] \"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = 7\"\n\n\n\n\n\n\n“Underserved” tracts are defined here as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\n# Number of underserved tracts\npaste0(\"Number of Underserved Tracts = \", sum(vulnerable_tracts$underserved))\n\n[1] \"Number of Underserved Tracts = 7\"\n\n# Percentage of vulnerable tracts that are underserved\npaste0(\"Percentage of Vulnerable Tracts that are Underserved = \",\n       round(sum(vulnerable_tracts$underserved)/nrow(vulnerable_tracts)*100, 2),\n       \"%\")\n\n[1] \"Percentage of Vulnerable Tracts that are Underserved = 2.71%\"\n\n\nThe percentage of vulnerable tracts that are underserved is rather low in Pennsylvania, which is surprising. Pennsylvania has many rural communities with sparsely distributed resources, including hospitals. The distribution of underserved census tracts in Pennsylvania do appear to be in rural, central PA counties (see the table below for the counties associated with these seven tracts); however, it is puzzling how few of them there are.\n\n# Table of the counties associated with the seven vulnerable tracts that are underserved in PA\nkable(vulnerable_tracts %&gt;% \n        filter(underserved == 1) %&gt;%\n        select(TRACT, COUNTY) %&gt;%\n        st_drop_geometry(),\n      col.names = c(\"Tract\", \"County\"),\n      caption = \"Underserved Vulnerable Tracts in PA\")\n\n\nUnderserved Vulnerable Tracts in PA\n\n\nTract\nCounty\n\n\n\n\n9601.00\nCameron\n\n\n3306.00\nClearfield\n\n\n3316.00\nClearfield\n\n\n5301.00\nForest\n\n\n5302.00\nForest\n\n\n702.01\nJuniata\n\n\n3003.11\nMonroe\n\n\n\n\n\n\n\n\n\n\n# Spatial join tracts to counties\n# Spatial joins were not used because spatial inconsistencies between the layers were producing duplicate instances of tracts across multiple counties despite everything being in the same crs. Tabular joins were more effective.\n\ncounties &lt;- counties %&gt;% \n  st_transform(2272)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\ndemo_vars &lt;- demo_vars_sf %&gt;% \n  st_drop_geometry()\n\ndemo_vars_cty &lt;- left_join(counties %&gt;% select(GEOID, NAME),\n                         demo_vars, by = c(\"NAME\" = \"COUNTY\"))\n\n# Aggregate statistics by county\ndemo_vars_cty_stats &lt;- demo_vars_cty %&gt;% \n  group_by(NAME) %&gt;% \n  summarise(pop_tot_agg = sum(pop_totE, na.rm = T),\n            med_hh_inc_agg = median(med_hh_incE, na.rm = T), \n            pop_elderly_agg = sum(pop_elderlyE, na.rm = T),\n            low_inc_cnt = sum(low_inc, na.rm = T),\n            vulnerable_cnt = sum(vulnerable, na.rm = T),\n            underserved_cnt = sum(underserved, na.rm = T),\n            hospital_dist_avg = round(mean(hospital_dist, na.rm = T), 2),\n            pop_vulnerable = sum(pop_elderlyE*vulnerable)) %&gt;% \n  mutate(pop_elderly_pct_agg = round((pop_elderly_agg/pop_tot_agg)*100, 2),\n         underserved_pct = round((underserved_cnt/vulnerable_cnt)*100, 2))\n\nAfter conducting this analysis, there are only 5 counties that have underserved census tracts: Cameron, Clearfield, Forest, Juniata, and Monroe.\n\nunderserved_pct_top &lt;- demo_vars_cty_stats %&gt;%\n  select(NAME, underserved_pct) %&gt;% \n  arrange(-underserved_pct) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(underserved_pct_top,\n      col.names = c(\"County\", \"% of Vulnerable Tracts Underserved\"))\n\n\n\n\nCounty\n% of Vulnerable Tracts Underserved\n\n\n\n\nCameron\n100.00\n\n\nForest\n100.00\n\n\nMonroe\n100.00\n\n\nClearfield\n66.67\n\n\nJuniata\n50.00\n\n\n\n\n\nThere are also only five counties that have an average distance to the nearest hospital greater than 15 miles: Cameron, Forest, Juniata, Pike, and Sullivan. However, due to the nature of aggregating tract-level data to the county level, Pike and Sullivan have no vulnerable tracts and thus have no vulnerable population (which was calculated by isolating tracts considered vulnerable and summing their elderly population). If considering living far from hospitals to include any distance greater than 10 miles instead of 15, the top five counties with the greatest vulnerable population are:\n\nvulnerable_pop_top &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;%\n  arrange(-pop_vulnerable) %&gt;% \n  select(NAME, pop_vulnerable) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(vulnerable_pop_top, col.names = c(\"County\", \"Vulnerable Population\"))\n\n\n\n\nCounty\nVulnerable Population\n\n\n\n\nClearfield\n2083\n\n\nForest\n1593\n\n\nJuniata\n1332\n\n\nBradford\n751\n\n\nCameron\n428\n\n\n\n\n\nThere is a pattern that counties with underserved elderly populations typically lie in the more rural areas of central and northeastern Pennsylvania, far from major city centers like Philadelphia and Pittsburgh that have high concentrations of concentrated hospitals and medical services.\n\ndemo_vars_cty_stats &lt;- demo_vars_cty_stats %&gt;% \n  mutate(underserved_cnt_factor = as.factor(underserved_cnt))\nplot(demo_vars_cty_stats[,\"underserved_cnt_factor\"],\n     pal = c(\"grey\", \"green4\", \"steelblue\"),\n     main = \"Count of Underserved Census Tracts\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop10_priority &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;% \n  arrange(-pop_vulnerable) %&gt;% \n  head(10) %&gt;% \n  select(NAME, pop_tot_agg, med_hh_inc_agg, pop_elderly_pct_agg, pop_vulnerable, hospital_dist_avg) %&gt;% \n  st_drop_geometry()\n\n# Create and format priority counties table\nkable(top10_priority,\n      col.names = c(\"County\", \"Total Population\", \"Median HH Income ($)\", \"% Elderly Population\", \"Vulnerable Population\", \"Mean Distance to Nearest Hospital (mi)\"), format.args = list(big.mark = \",\"),\n      caption = \"Top Ten Counties to Target for Healthcare Investment\")\n\n\nTop Ten Counties to Target for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population\nMedian HH Income ($)\n% Elderly Population\nVulnerable Population\nMean Distance to Nearest Hospital (mi)\n\n\n\n\nClearfield\n79,707\n54,033.5\n21.07\n2,083\n13.13\n\n\nForest\n6,959\n45,728.0\n22.89\n1,593\n18.40\n\n\nJuniata\n23,535\n64,173.0\n20.56\n1,332\n15.01\n\n\nBradford\n60,159\n58,866.0\n21.74\n751\n10.18\n\n\nCameron\n4,536\n45,279.5\n27.98\n428\n19.07\n\n\nMonroe\n168,128\n81,127.0\n18.27\n314\n10.05\n\n\nPerry\n45,941\n74,562.5\n19.37\n0\n13.26\n\n\nPike\n58,996\n79,479.5\n22.99\n0\n19.22\n\n\nPotter\n16,390\n54,904.0\n24.87\n0\n11.02\n\n\nSnyder\n39,797\n66,042.0\n19.83\n0\n14.47"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\n\nggplot() +\n  geom_sf(\n    data = demo_vars_cty_stats,\n    aes(fill = underserved_pct),\n    color = \"transparent\"\n    ) +\n  scale_fill_binned(\n    name = \"Underserved Pct (%)\",\n    breaks = seq(0, 100, by = 20),\n    low = \"#e5f5e0\",\n    high = \"#006d2c\",\n    na.value = \"grey75\"\n    ) +\n  geom_sf(data = hospitals, aes(color = \"Hospitals\"), size = 0.75) +\n  scale_color_manual(name = \"Hospital Locations\",\n                     values = c(\"Hospitals\" = \"black\")\n                     ) +\n  theme_void() +\n  labs(title = \"Percent of Census Tracts Classified as Underserved Within a County\",\n       subtitle = \"Relative to Hospital Locations\")\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\n# Create detailed tract-level map\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            mutate(fill_category = case_when(\n              underserved == 1 ~ \"Underserved\",\n              vulnerable == 1 ~ \"Vulnerable\",\n              vulnerable == 0 ~ \"Not Vulnerable\",\n              .default = \"No Data\"\n            )),\n          aes(fill = factor(fill_category, levels = c(\"Underserved\",\n                                                      \"Vulnerable\",\n                                                      \"Not Vulnerable\")\n                            )\n              ),\n          color = \"transparent\") +\n  scale_fill_manual(name = \"Underserved | Vulnerability\",\n                    values = c(\"Underserved\" = \"red3\",\n                               \"Vulnerable\" = \"gold2\",\n                               \"Not Vulnerable\" = \"grey75\",\n                               \"No Data\" = \"grey65\")\n                    ) +\n  geom_sf(data = counties,\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = hospitals,\n          aes(color = \"Hospitals\"),\n          size = 0.75) +\n   scale_color_manual(name = \"Hospital Locations\",\n                     values = c(\"Hospitals\" = \"black\")\n                     ) +\n  theme_void() +\n  labs(title = \"Spatial Comparison of Underserved and Vulnerable Census Tracts\",\n       subtitle = \"Relative to County Boundaries and Hospitals\")\n\n\n\n\n\n\n\n\n\n\n\nChart 1: Distribution Analysis\n\n# Create distribution visualization\nggplot() +\n  geom_histogram(data = demo_vars_sf %&gt;% filter(vulnerable == 1),\n                 aes(hospital_dist),\n                 bins = 50,\n                 na.rm = T, fill = \"steelblue\",\n                 colour = \"grey30\") +\n  labs(title = \"Histogram Distribution of Distances to the Nearest Hospital\",\n       subtitle = \"From Census Tract Centroids\",\n       x = \"Distance to the Nearest Hospital (mi)\",\n       y = \"Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe distribution is right-skewed with a majority of census tract centroids lying within 4-5 miles of the closest hospital. The maximum distance from a hospital is slightly less than 20 miles, and there are a considerable number of outliers between 5 and 20 miles."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-3-additional-data-analysis",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-3-additional-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Additional Data Analysis",
    "text": "Part 3: Additional Data Analysis\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nAnalysis\n\n# Load additional dataset(s)\npop_phl &lt;- demo_vars_sf %&gt;%\n  filter(COUNTY == \"Philadelphia\") %&gt;% \n  select(GEOID, TRACT, COUNTY, STATE, pop_totE, pop_totM)\n\nfirehouses &lt;- st_read(\"./data/Fire_Dept_Facilities.geojson\", quiet = T) %&gt;% \n  st_transform(st_crs(pop_phl))\n\npaste0(\"Number of Fire Stations in PHL = \", nrow(firehouses))\n\n[1] \"Number of Fire Stations in PHL = 69\"\n\n\nThe dataset loaded above was sourced from OpenDataPhilly’s Fire Department Facilities dataset, created in 2014. Despite the age of the dataset, it’s assumed that fire stations are relatively permanent fixtures in communities and likely have persisted into the year of the population data (2022). Through this assumption, this spatial analysis will seek to illustrate how well/poorly fire stations in Philadelphia are adapted to the current population patterns within the city.\nThe firehouse dataset was originally in a geographic WGS84 coordinate system (EPSG 4326), which necessitated transforming it to a projected coordinate system. The CRS used in the previous analysis (EPSG 2272) was intended for use in Southern Pennsylvania, and so the CRS from the population dataset was called instead of using the EPSG code to ensure they’d be the same.\n\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            filter(COUNTY == \"Philadelphia\") %&gt;%\n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey75\") +\n  geom_sf(data = demo_vars_cty %&gt;% \n            filter(NAME == \"Philadelphia\") %&gt;% \n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = firehouses,\n          aes(color = \"Firehouses\"),\n          size = 3) +\n  scale_color_manual(name = \"\",\n                     values = c(\"Firehouses\"= \"red3\")) +\n  theme_void() +\n  labs(title = \"Firehouse Locations in Philadelphia\",\n       subtitle = \"Relative to Census Tracts\")\n\n\n\n\n\n\n\n\nNote: one fire station is located outside the southern boundary of Philadelphia County. This is likely a fire station dedicated to serving the Philadelphia International Airport, since most of the airport lies within Delaware County. This airport will be omitted from this analysis partially due to its location, but also because it likely primarily (or almost exclusively) serves the airport, and from past experience analyzing this area the population density is far lower due a high concentration of both airport and industrial land uses.\n\n# remove firehouses outside the boundary of Philadelphia County\nfirehouses &lt;- st_filter(firehouses, demo_vars_cty %&gt;% filter(NAME == \"Philadelphia\"))\n\n\nPose a research question\nDoes the distribution of fire stations in Philadelphia as registered in 2014 (assumed to be the same, if not similar, to that of today) adequately cover the population of the city as of 2022? Which fire stations serve the lowest population density and where are they located in the city?\n\nConduct spatial analysis\n\n# Create a two mile buffer zone around every fire station in Philadelphia County\nfirehouses_2mi &lt;- st_buffer(firehouses, dist = (2*5280))\n\n# Calculate areas of census tracts to perform an areal interpolation\npop_phl &lt;- pop_phl %&gt;% \n  mutate(area_tract = as.numeric(st_area(.)))\n\n# perform an intersection to get every partial overlap of each buffer with the surrounding census tracts\n# calculate the areas of these intersections and get their area, then divide by the area of the corresponding census tract to get an area ratio\n# multiply this ratio to the total population of the census tract to get a partial population count for each intersection\n# NOTE: assumes even population distribution across every census tract\nfirehouses_int &lt;- st_intersection(firehouses_2mi, pop_phl) %&gt;% \n  mutate(area_int = as.numeric(st_area(.)),\n         area_frac = area_int/area_tract,\n         pop_int = pop_totE * area_frac)\n\n# sum population fractions to each buffer to get the total serviced population\nfirehouses_sum &lt;- firehouses_int %&gt;% \n  group_by(objectid) %&gt;% \n  summarise(pop_served = sum(pop_int))\n\n# join results back to the point dataset to map\nfirehouses &lt;- left_join(firehouses, firehouses_sum %&gt;% st_drop_geometry(), by = \"objectid\")\n\nfirehouses_stats &lt;- firehouses %&gt;%\n  st_drop_geometry %&gt;%\n  summarise(Minimum = min(pop_served, na.rm = T),\n            Mean = mean(pop_served, na.rm = T),\n            Median = median(pop_served, na.rm = T),\n            Maximum = max(pop_served, na.rm = T),\n            Total = sum(pop_served, na.rm = T)) %&gt;% \n  kable(digits = 2,\n        format.args = list(big.mark = \",\"),\n        caption = \"Summary Statistics of Population Served by Firehouses in Philadelphia\")\n\nfirehouses_stats\n\n\nSummary Statistics of Population Served by Firehouses in Philadelphia\n\n\nMinimum\nMean\nMedian\nMaximum\nTotal\n\n\n\n\n26,670.02\n161,730.4\n176,151.6\n276,796.7\n10,997,666\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            filter(COUNTY == \"Philadelphia\") %&gt;%\n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey75\") +\n  geom_sf(data = demo_vars_cty %&gt;% \n            filter(NAME == \"Philadelphia\") %&gt;% \n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = firehouses %&gt;% mutate(pop_10k = pop_served/10000),\n          aes(fill = pop_10k),\n          shape = 21,\n          color = \"grey25\",\n          size = 3) +\n  scale_fill_binned(\n    name = \"Population Served (10k)\",\n    breaks = seq(0, 28, by = 7),\n    limits = c(0, 28),\n    low = \"#EEE5E9\",\n    high = \"red3\",\n    na.value = \"grey75\"\n    ) +\n  theme_void() +\n  labs(title = \"Total Population Served by Fire Stations in Philadelphia\",\n       subtitle = \"Station Locations: 2014, Population Data: 2022\")\n\n\n\n\n\n\n\n\nInterpretation of Findings:\nAccording to the ACS5, the population in Philadelphia County is approximately 1,593,000 people. If every fire station served the median number of people served (176,000 people) and Philadelphia’s population were evenly distributed throughout the city, there would theoretically only need to be 10 fire stations to cover the entirety of the city’s population. This isn’t the whole picture, however, as with all emergency services redundancy and spatial proximity to residents is also critical towards ensuring that all everying is served adequately and rapidly in the event of a fire.\nFire stations in Philadelphia are spread throughout the city, with greater concentrations in Center City and North Philadelphia. The greatest populations served are also in these areas. As fire stations get closer to the border of the county the population served decreases. This is likely due to edge effects where populations in surrounding counties (Montgomery, Delaware, etc.) are not being counted despite these stations still likely assisting them. On the eastern border of the county, this is due to the Delaware River and not due to the data restrictions set at the beginning of this analysis.\nVisually, as the Navy Yard continues to see development in the coming decades and as population density in the far Northeast continues to increase, it might be a good policy decision in the future to encourage the creation of additional fire stations in these areas in order to relieve burden from surrounding stations and potentially improve response times. Further analysis could investigate average response times and/or quantify the burden currently on each station to see where additional stations/resources are needed."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#comments-about-incorporation-of-feedback",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#comments-about-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Comments about incorporation of feedback:",
    "text": "Comments about incorporation of feedback:\nAdditional care was taken to clean up this document, removing instruction text and changing the wording of certain headings. All tables and charts that are produced in the report are intended to have a purpose/serve the narrative of the report. Tweaks were made to the classification criteria at certain points (i.e. changing the threshold of hospital distance from 15 miles to 10 miles) in order to achieve a more insightful result."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#submission-requirements",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/midterm/model_script.html",
    "href": "assignments/midterm/model_script.html",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "",
    "text": "This technical appendix documents the full workflow used to engineer and visualize spatial features for predicting residential housing prices in Philadelphia.\nCode\noptions(scipen = 999)\n\n# Packages\nif(!require(pacman)){install.packages(\"pacman\"); library(pacman, quietly = T)}\n\n\nLoading required package: pacman\n\n\nWarning: package 'pacman' was built under R version 4.4.3\n\n\nCode\np_load(knitr, sf, tidyverse, tidycensus, tigris, here, dplyr, FNN, ggplot2, scales, patchwork, caret, Hmisc, stargazer)\n\n# Files\nsf_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE)\nnhoods &lt;- st_read(\"./data/philadelphia-neighborhoods.geojson\", quiet = TRUE)"
  },
  {
    "objectID": "assignments/midterm/model_script.html#data-preparation",
    "href": "assignments/midterm/model_script.html#data-preparation",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.1 Data Preparation",
    "text": "1.1 Data Preparation\nWe apply several filters to the property data to for quality and relevance. First, we restrict our analysis to residential properties sold between 2023 and 2024, excluding any other property categories. Second, we remove properties with sale prices below $10, as these are abnormal prices for residential properties.\nTo work with Github file size limits, the data is further trimmed of irrelevant columns.\n\n\nCode\n# Restrict to residential only\nresidential_categories &lt;- c(\n  \"APARTMENTS &gt; 4 UNITS\",\n  \"MULTI FAMILY\",\n  \"SINGLE FAMILY\",\n  \"MIXED USE\"\n)\nresidential_data &lt;- sf_data %&gt;%\n  filter(category_code_description %in% residential_categories,\n         year(sale_date) %in% c(2023, 2024),\n         mailing_city_state == \"PHILADELPHIA PA\",\n         sale_price &gt; 10\n         )\n\ntable(residential_data$category_code_description)\n\n# Making sure the file saved to the repo is the trimmed data (to stay below GitHub data limits)\nst_write(residential_data, \"./data/OPA_data.geojson\", driver = \"GeoJSON\", delete_dsn = TRUE, quiet = TRUE)\nfile.exists(\"./data/OPA_data.geojson\")\nOPA_raw &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE) %&gt;% \n  st_transform(2272)\n\n# OPA_data -&gt; cutting mostly NA columns or irrelevant columns for this model.\nOPA_raw &lt;- OPA_raw %&gt;%\n  select(-c(\n  cross_reference, date_exterior_condition, exempt_land, fireplaces, fuel, garage_type, house_extension, mailing_address_2, mailing_care_of, market_value_date, number_of_rooms, other_building, owner_2, separate_utilities, sewer, site_type, street_direction, suffix, unfinished, unit, utility\n  ))\n\nnames(OPA_raw)\n\n\nThe property sales data was gathered from the OPA properties public data set from the City of Philadelphia. This data set was 32 columns and 583,825 observations. This file was too large for our shared GitHub work space so it was reduced by filtering for residential properties, years 2023 and 2024, location within Philadelphia, and sale price over 10 since some were NA, 1, or 10. This was just enough to get the most basic and general data to work with that ran with GitHub size limits. This reduced the size to 22121 observations. The original geojson file was overwritten and named OPA_data.\nProperties selected for residential included apartments &gt;4 units, single family, multi-family, and mixed use. Mixed use was left in as there are still residential unit to account yet add more complex property types to our total data set when comparing sale price and other aspects such as total area to other observations. These properties should also be cross referenced with zoning codes for future research.\nWe left mixed use in during this process to give us the most general data set representation. There was also limited data cleaning other than omitting columns that were mostly NA. This gave our model the most general data set to work with despite lower future RMSE values. Future research would be needed to most accurately assess the choices of losing data and a more generalized Philadelphia housing market verses very clean data and more specific Philadelphia housing market that may omit certain aspects of the housing market like data in lower income areas or multi use residential aspects. This could have also been conducted in grouping NA values and sparse categories. More complexity could be accounted for in future work.\nThis was our start simple and add complexity approach. Our original to final OPA data set went from 583,825 to 22,121 observations and from 32 to 68 variables."
  },
  {
    "objectID": "assignments/midterm/model_script.html#exploratory-data-analysis",
    "href": "assignments/midterm/model_script.html#exploratory-data-analysis",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.2 Exploratory Data Analysis",
    "text": "1.2 Exploratory Data Analysis\nBelow are selected property variables—Total Livable Area, Bedrooms, Bathrooms, and Age—in relation to Sale Price. Properties with excessive square footage (&gt;10,000 sqft), missing bedroom or bathroom data, over 12 bathrooms with low sale prices, or implausible construction years were removed to reduce skew and data errors. This additional filtering was kept for the rest of the analysis in this report.\n\n\nCode\n# filter out outliers from the dataset\nOPA_data &lt;- OPA_raw %&gt;%\n  filter(\n    total_livable_area &lt;= 10000,\n    year_built &gt; 1800,\n    !is.na(number_of_bathrooms),\n    !is.na(number_of_bedrooms),\n    number_of_bathrooms &lt; 12,\n  ) %&gt;%\n  mutate(\n    year_built = as.numeric(year_built),\n    building_age = 2025 - year_built\n  )\n\np1 &lt;- ggplot(OPA_data, aes(x = total_livable_area, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  scale_x_continuous(labels = comma_format()) +\n  labs(\n    title = \"Sale Price vs. Total Livable Area\",\n    x = \"Total Livable Area (sq ft)\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np2 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bedrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bedrooms\",\n    x = \"Number of Bedrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np3 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bathrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bathrooms\",\n    x = \"Number of Bathrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np4 &lt;- ggplot(OPA_data, aes(x = building_age, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Age\",\n    x = \"Age\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\n# Combine plots\n(p1 | p2) / (p3 | p4)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "assignments/midterm/model_script.html#feature-engineering",
    "href": "assignments/midterm/model_script.html#feature-engineering",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.3 Feature Engineering",
    "text": "1.3 Feature Engineering\n\n\nCode\nOPA_data &lt;- OPA_data %&gt;%\n  mutate(\n    # convert to numeric before interactions\n    total_livable_area = as.numeric(total_livable_area),\n    census_tract = as.numeric(as.character(census_tract)),\n    year_built = as.numeric(year_built),\n    total_area = as.numeric(total_area),\n    market_value = as.numeric(market_value),\n    number_of_bedrooms = as.numeric(number_of_bedrooms),\n\n    # building code and total area\n    int_type_tarea = as.numeric(as.factor(building_code_description)) * total_area,\n\n    # market and livable area\n    int_value_larea = market_value * total_livable_area,\n\n    # market and total area\n    int_value_tarea = market_value * total_area,\n\n    # livable area and exterior condition\n    int_larea_econd = total_livable_area * as.numeric(as.factor(exterior_condition)),\n\n    # livable area and interior condition\n    int_larea_icond = total_livable_area * as.numeric(as.factor(interior_condition)),\n\n    # livable area and bedrooms\n    int_larea_beds = total_livable_area * number_of_bedrooms\n  )\n\n\n\n\nCode\npa_crs &lt;- 2272  \nneighbor_points &lt;- st_transform(OPA_data, pa_crs)\n\nnrow(nhoods)\n\nst_crs(neighbor_points)\nnhoods &lt;- st_transform(nhoods, 2272)\n\n#joining houses to neighborhoods\nneighbor_points &lt;- neighbor_points %&gt;%\n  st_join(., nhoods, join = st_intersects)\n\n# one property doesn't lie in any neighborhood\nneighbor_points &lt;- neighbor_points[!is.na(neighbor_points$NAME),]\n\n#results \nneighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  count(NAME) %&gt;%\n  arrange(desc(n))\n\n\n\n\nCode\n#spatial joins\nprice_by_nhood &lt;- neighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(NAME) %&gt;%\n  dplyr::summarize(\n    median_price = median(sale_price, na.rm = TRUE),\n    n_sales = n()\n  )\n\nnhoods_prices &lt;- nhoods %&gt;%\n  left_join(., price_by_nhood, by = \"NAME\")\n\n#setting median house price classes\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    price_class = cut(median_price,\n                      breaks = c(0, 400000, 600000, 800000, 1000000, Inf),\n                      labels = c(\"Under $400k\", \"$400k-$600k\", \"$600k-$800k\", \n                                 \"$800k-$1M\", \"Over $1M\"),\n                      include.lowest = TRUE)\n  )\n\n#mapping\nggplot() +\n  geom_sf(data = nhoods_prices, aes(fill = price_class), \n          color = \"white\", size = 0.5) +\n  scale_fill_brewer(\n    name = \"Median Price\",\n    palette = \"YlOrRd\",\n    na.value = \"grey90\",\n    direction = 1\n  ) +\n  labs(\n    title = \"Median Home Prices by Philadelphia Neighborhood\",\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  head(10)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(n_sales)) %&gt;%\n  head(5)\n\n\n\n\nCode\n# Define wealthy as &gt;=$420,000 which is 1.5x city median of 279,900\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(median_price &gt;= 420000, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\nnhoods_prices %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n  wealthy_neighborhood   n\n1          Not Wealthy 123\n2              Wealthy  26\n3                 &lt;NA&gt;  10\n\n\nCode\nneighbor_points &lt;- neighbor_points %&gt;%\n  left_join(.,\n            nhoods_prices %&gt;%\n              st_drop_geometry %&gt;%\n              select(NAME, wealthy_neighborhood),\n            by = \"NAME\")\n\n# Still add neighbor points to OPA data\n\n\nHouseholds were denoted as wealthy if their median household price was over $420,000, which is 1.5x city median of 279,900. This term will be used in an interaction in Model 4 to account for theoretical differences in wealthy neighborhoods, such as inflated costs for additional home amenities such as bedrooms, bathrooms, or livable floor area."
  },
  {
    "objectID": "assignments/midterm/model_script.html#data-preparation-1",
    "href": "assignments/midterm/model_script.html#data-preparation-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "2.1 Data Preparation",
    "text": "2.1 Data Preparation\n\n\nCode\n# link variables and aliases\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\",\n          \"med_age\" = \"B01002_001\")\n\n# the FIPS code for the state of PA is 42\nfips_pa &lt;- 42\n\n\nVariables pulled from the census include total population, median household income, and median age.\n\n\nCode\n# retrieve data from the ACS for 2023\ndemo_vars_pa &lt;- get_acs(geography = \"tract\",\n                        variable = vars,\n                        year = 2023,\n                        state = fips_pa,\n                        output = \"wide\",\n                        geometry = T,\n                        progress_bar = F) %&gt;% \n  st_transform(2272)\n\n# separate NAME column into its constituent parts\ndemo_vars_pa &lt;- demo_vars_pa %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n# filter out Philadelphia tracts\ndemo_vars_phl &lt;- demo_vars_pa %&gt;%\n  filter(COUNTY == \"Philadelphia\")\n\n\n\n\nCode\n# plot cenusus variables to compare\nplot(demo_vars_phl[,\"pop_totE\"],\n     main = \"Total Population\",\n     breaks = seq(0, 10500, 500),\n     nbreaks = 21)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_hh_incE\"],\n     main = \"Median Household Income\",\n     breaks = seq(0, 200000, 10000),\n     nbreaks = 20)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_ageE\"],\n     main = \"Median Age\",\n     breaks = seq(0, 75, 5),\n     nbreaks = 15)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# get NA counts per column\nna_counts &lt;- sapply(demo_vars_phl, function(x) {sum(is.na(x))})\nkable(t(as.data.frame(na_counts)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nTRACT\nCOUNTY\nSTATE\npop_totE\npop_totM\nmed_hh_incE\nmed_hh_incM\nmed_ageE\nmed_ageM\ngeometry\n\n\n\n\nna_counts\n0\n0\n0\n0\n0\n0\n27\n27\n17\n17\n0\n\n\n\n\n\nCode\n# filter out all rows that have at least one column with an na value\nna_index &lt;- !complete.cases(demo_vars_phl %&gt;% st_drop_geometry())\ndemo_vars_phl_na &lt;- demo_vars_phl[na_index,]\nkable(head(demo_vars_phl_na, 5) %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101980902\n9809.02\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101980800\n9808.00\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n\n\n\n27 and 17 census tracts have a value of NA for median household income and median age, respectively. For the 17 census tracts where there is no reported population, the median household income and median age will be set to 0. The remaining 10 census tracts that have population but no reported median household income will be omitted from the dataset.\n\n\nCode\n# create a dataset with NAs replaced with zero where applicable\ndemo_vars_phl_rep &lt;- demo_vars_phl %&gt;% \n  mutate(med_hh_incE = case_when(pop_totE == 0 & is.na(med_hh_incE) ~ 0,\n                                 .default = med_hh_incE),\n         med_ageE = case_when(pop_totE == 0 & is.na(med_ageE) ~ 0,\n                                 .default = med_ageE))\n\n# final cleaned dataset without the 10 census tracts that have population values but have NA values for Median Household Income\ndemo_vars_phl_clean &lt;- demo_vars_phl_rep[complete.cases(demo_vars_phl_rep %&gt;%\n                                                          select(-ends_with(\"M\")) %&gt;%\n                                                          st_drop_geometry()),]\n\n# table with the omitted census tracts\ndemo_vars_phl_omit &lt;- demo_vars_phl_rep[!complete.cases(demo_vars_phl_rep %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry()),]\nkable(demo_vars_phl_omit %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F, caption = \"Census Tracts Omitted from Analysis due to Data Unavailability\")\n\n\n\nCensus Tracts Omitted from Analysis due to Data Unavailability\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n42101014800\n148.00\nPhiladelphia\nPennsylvania\n892\nNA\n40.9\n\n\n42101027700\n277.00\nPhiladelphia\nPennsylvania\n5489\nNA\n36.9\n\n\n42101030100\n301.00\nPhiladelphia\nPennsylvania\n6446\nNA\n37.2\n\n\n42101989100\n9891.00\nPhiladelphia\nPennsylvania\n1240\nNA\n29.7\n\n\n42101989300\n9893.00\nPhiladelphia\nPennsylvania\n160\nNA\n30.5\n\n\n42101020500\n205.00\nPhiladelphia\nPennsylvania\n3383\nNA\n33.3\n\n\n42101980200\n9802.00\nPhiladelphia\nPennsylvania\n396\nNA\n74.9\n\n\n\n\n\n\n\nCode\n# join census variables to the OPA data\nOPA_data &lt;- st_join(OPA_data, demo_vars_phl_clean %&gt;% select(pop_totE, med_hh_incE, med_ageE))\n\n# isolate NA rows and plot where they are\ncensusNAs &lt;- OPA_data[is.na(OPA_data$med_hh_incE),]\n\ncensus_plt1 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry) +\n  geom_sf(data = censusNAs, size = 0.15) +\n  theme_void() +\n  labs(title = \"Properties without Census Data\")\ncensus_plt2 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry, fill = \"black\", color = \"transparent\") +\n  theme_void() +\n  labs(title = \"Census Tracts with Data (Black)\")\n\n(census_plt1 | census_plt2)\n\n\n\n\n\n\n\n\n\nOf the 22121 properties in the dataset after cleaning and omitting outliers, 248 - approximately 1.1% of the dataset - have no associated census data due to the lack of a Median Household Income value for those census tracts. Comparing plots of property locations without census data and that of census tracts which have data confirms this spatial relationship."
  },
  {
    "objectID": "assignments/midterm/model_script.html#data-preparation-2",
    "href": "assignments/midterm/model_script.html#data-preparation-2",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.1 Data Preparation",
    "text": "3.1 Data Preparation\nThis stage prepares and validates the OpenDataPhilly spatial datasets used to engineer neighborhood-level variables for the housing model.\nSteps Performed\n\nTransformed all spatial datasets to EPSG 2272 (NAD83 / PA South ftUS) for consistent distance measurements.\nRemoved invalid geometries, dropped Z/M values, and converted all housing geometries to points.\nImported and projected OpenDataPhilly amenities:\n\nTransit Stops\nHospitals\nParks & Recreation Sites\nSchools Parcels (centroids created from polygon features)\nCrime Incidents (2023 and 2024 combined)\n\n\n\n\nCode\n#CRS & radii\npa_crs &lt;- 2272    # NAD83 / PA South (ftUS)\nmi_to_ft   &lt;- 5280\nr_park_ft   &lt;- 0.25 * mi_to_ft\nr_crime_ft  &lt;- 0.50 * mi_to_ft\nr_school_ft &lt;- 0.50 * mi_to_ft\n\n# turn off spherical geometry (makes buffer/join ops faster)\nsf::sf_use_s2(FALSE)\n\n## CONVERT SALES DATA TO POINTS ##\nOPA_points &lt;- st_transform(OPA_data, pa_crs)\n\n#Drop Z/M if present\nst_geometry(OPA_points) &lt;- st_zm(st_geometry(OPA_points), drop = TRUE, what = \"ZM\")\n\n#Make geometries valid\nst_geometry(OPA_points) &lt;- st_make_valid(st_geometry(OPA_points))\n\n#Ensure POINT geometry (works for points/lines/polygons/collections)\nst_geometry(OPA_points) &lt;- st_point_on_surface(st_geometry(OPA_points))\n\n#Add sale ID\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(sale_id = dplyr::row_number())\n\n#read & project layers\ntransit   &lt;- st_read(\"./data/Transit_Stops_(Spring_2025)/Transit_Stops_(Spring_2025).shp\", quiet = TRUE) |&gt; st_transform(pa_crs)\nhospitals &lt;- st_read(\"./data/Hospitals.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\nparksrec  &lt;- st_read(\"./data/PPR_Program_Sites.geojson\", quiet = TRUE)|&gt; st_transform(pa_crs)\nschools_polygons   &lt;- st_read(\"./data/Schools_Parcels.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\ncrime_2023     &lt;- st_read(\"./data/crime_incidents_2023/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\ncrime_2024     &lt;- st_read(\"./data/crime_incidents_2024/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\n\n#combine 2023 & 2024 crime datasets\ncrime &lt;- rbind(crime_2023, crime_2024)\n\n#create centroids for schools dataset\nschools &lt;- if (any(st_geometry_type(schools_polygons) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_centroid(schools_polygons, )\n} else {\n  schools_polygons\n}\n\n#crop transit data to philadelphia\nphilly_boundary &lt;- st_union(nhoods)\n\ntransit &lt;- st_filter(transit, philly_boundary, .predicate = st_within)"
  },
  {
    "objectID": "assignments/midterm/model_script.html#exploratory-data-analysis-1",
    "href": "assignments/midterm/model_script.html#exploratory-data-analysis-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.2 Exploratory Data Analysis",
    "text": "3.2 Exploratory Data Analysis\nExploratory plots and maps examine the raw accessibility patterns across Philadelphia before feature engineering.\n\n\nCode\n# Transit stops (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = transit, size = 0.3, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Transit Stops (SEPTA Spring 2025)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Hospitals (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = hospitals, size = 0.6, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Hospitals\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Parks & Recreation Program Sites (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = parksrec, size = 0.4, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Parks & Recreation Sites\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Schools (centroids of polygons) — raw\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = schools, size = 0.4, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Schools (Centroids)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Crime points are huge; sampling for speed\nset.seed(5080)\ncrime_quick &lt;- if (nrow(crime) &gt; 30000) dplyr::slice_sample(crime, n = 30000) else crime\n\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = crime_quick, size = 0.1, alpha = 0.25) +\n  labs(title = \"Raw Layer Check: Crime Incidents (sampled if large)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n3.2.1 Interpretation\n\nTransit Stops: Dense corridors radiate from Center City, showing strong transit coverage.\nHospitals: Sparse but geographically balanced.\nParks & Recreation: uneven distribution,\nSchools: evenly distributed across most neighborhoods\nCrime: Visibly concentrated, confirming the need for log-transformed counts"
  },
  {
    "objectID": "assignments/midterm/model_script.html#feature-engineering-1",
    "href": "assignments/midterm/model_script.html#feature-engineering-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.3 Feature Engineering",
    "text": "3.3 Feature Engineering\nSpatial features were derived using two complementary approaches: k-Nearest Neighbor (kNN) and buffer-based counts, depending on whether accessibility was best captured as proximity or exposure.\n\n\nCode\n#| message: false\n#| warning: false\n\n#clean sales data\nsales_xy &lt;- st_coordinates(OPA_points)\nok_sales  &lt;- complete.cases(sales_xy)\nOPA_points &lt;- OPA_points[ok_sales, ]    # keep only rows with valid XY\nsales_xy  &lt;- st_coordinates(OPA_points) # refresh coordinates\n\ntransit_xy &lt;- st_coordinates(transit)\nhosp_xy    &lt;- st_coordinates(hospitals)\n\n# feature 1 - distance to nearest transit stop (ft)\nknn_tr &lt;- FNN::get.knnx(\n  data  = st_coordinates(transit),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_transit_ft = as.numeric(knn_tr$nn.dist[,1]))\n\n# feature 2 - distance to nearest hospital (ft)\nknn_hp &lt;- FNN::get.knnx(\n  data  = st_coordinates(hospitals),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_hospital_ft = as.numeric(knn_hp$nn.dist[,1]))\n\n\n\n\nCode\n# feature 3 - parks/rec sites within 0.25 mi (count)\nrel_parks &lt;- sf::st_is_within_distance(OPA_points, parksrec, dist = r_park_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(parks_cnt_0p25mi = lengths(rel_parks))\n\n# feature 4 - crime count within 0.5 mi (per square mile)\ncrime_buffer &lt;- st_buffer(OPA_points, dist = r_crime_ft)\n\nrel_crime &lt;- st_intersects(crime_buffer, crime, sparse = TRUE)\n\n# count number of crimes\ncrime_cnt &lt;- lengths(rel_crime)\n\nrm(rel_crime)\n\nOPA_points &lt;- OPA_points |&gt;\n  mutate(\n    crime_cnt_0p5mi     = crime_cnt,\n    log1p_crime_cnt_0p5 = log1p(crime_cnt_0p5mi)\n  )\n\n# feature 5 - schools within 0.5 mi (using centroids)\nrel_sch &lt;- sf::st_is_within_distance(OPA_points, schools, dist = r_school_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(schools_cnt_0p5mi = lengths(rel_sch))\n\n\n\n3.3.1 Summary Table and Justification\n\n\n\n\n\n\n\n\n\nFeature\nMethod\nParameter\nTheoretical Rationale\n\n\n\n\nDistance to Nearest Transit Stop\nkNN (k = 1)\n–\nCaptures ease of access to public transport; nearest stop approximates walkability and job access.\n\n\nDistance to Nearest Hospital\nkNN (k = 1)\n–\nReflects accessibility to health care and emergency services; proximity adds perceived security for households.\n\n\nParks & Rec Sites within 0.25 mi\nBuffer Count\nr = 0.25 mi\nMeasures exposure to green space and recreational facilities within a 5-minute walk; positive amenity effect on property value.\n\n\nCrime Incidents within 0.5 mi\nBuffer Count\nr = 0.5 mi\nRepresents local safety environment; higher crime counts reduce housing desirability.\n\n\nSchools within 0.5 mi\nBuffer Count\nr = 0.5 mi\nReflects educational access and family appeal; clustering of schools often raises residential demand.\n\n\nPopulation\nCensus\n–\nRepresents the present residential demand within a census tract\n\n\nMedian Household Income\nCensus\n–\nIndicative of the ability of present residents of a census tract to afford housing\n\n\nMedian Age\nCensus\n–\nMeasure of the dominant age group in a census tract (i.e. high student or elderly population)\n\n\n\n\n\n3.3.2 Feature Validation and Visualization\n\n\nCode\n## Transit Accessibility\nggplot(OPA_points, aes(x = dist_nearest_transit_ft)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Transit Stop\",\n       x = \"Feet to Nearest Stop\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_transit_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"plasma\", labels = comma) +\n  labs(title = \"Transit Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Hospital Proximity\nggplot(OPA_points, aes(x = dist_nearest_hospital_ft)) +\n  geom_histogram(fill = \"darkorange\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Hospital\",\n       x = \"Feet to Nearest Hospital\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_hospital_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"magma\", labels = comma) +\n  labs(title = \"Hospital Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Parks & Recreation\nggplot(OPA_points, aes(x = parks_cnt_0p25mi)) +\n  geom_histogram(fill = \"seagreen\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Parks & Rec Sites Within 0.25 mi\",\n       x = \"Count within 0.25 mi\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = parks_cnt_0p25mi), size = 0.6) +\n  scale_color_viridis_c(option = \"viridis\") +\n  labs(title = \"Proximity to Parks & Recreation (0.25 mi Buffer)\",\n       color = \"Parks Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Crime Counts\nggplot(OPA_points, aes(x = crime_cnt_0p5mi)) +\n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Crime Incidents Within 0.5 mi\",\n       x = \"Crime Count (2023–2024)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = log1p_crime_cnt_0p5), size = 0.6) +\n  scale_color_viridis_c(option = \"inferno\") +\n  labs(title = \"Crime Exposure (log-transformed within 0.5 mi)\",\n       color = \"log(1+Crime Count)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Schools Accessibility\nggplot(OPA_points, aes(x = schools_cnt_0p5mi)) +\n  geom_histogram(fill = \"purple\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Schools Within 0.5 mi\",\n       x = \"School Count (0.5 mi Buffer)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = schools_cnt_0p5mi), size = 0.6) +\n  scale_color_viridis_c(option = \"cividis\") +\n  labs(title = \"School Accessibility (0.5 mi Buffer)\",\n       color = \"Schools Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nTransit proximity: Most parcels are within 500 ft of a stop, confirming strong transit coverage across Philadelphia.\nHospital proximity: Right-skewed distribution, consistent with limited facility count.\nParks access: Sparse exposure (mostly 0–1 within 0.25 mi), highlighting recreational inequities.\nCrime exposure: Wide variation, clustered along high-density corridors; log-transformed to stabilize scale.\nSchool proximity: Uniform urban coverage with typical parcels having 4-7 schools within 0.5 mi.\n\n\n\nCode\nsp_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = T)\n\nstr(sp_data$sale_price)\n\n\n int [1:23611] 389000 20000 309000 185000 399000 50000 70000 50000 30000 230000 ...\n\n\nCode\nsp_data_filtered &lt;- sp_data %&gt;%\n  mutate(sale_price = as.numeric(sale_price)) %&gt;%\n  filter(sale_price &gt; 1000)\n\nggplot(sp_data_filtered, aes(x = sale_price)) + \n  geom_histogram(\n    binwidth = 20000, \n    fill = \"grey\",\n    color = \"black\"\n  ) +\n  labs(\n    title = \"Histogram of Sale Prices in Philadelphia\", \n    x = \"Sale Price\",\n    y = \"Count of Homes\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(0, 2000000), ylim = c(0, 3000))\n\n\n\n\n\n\n\n\n\nCode\nsummary(sp_data_filtered$sale_price)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    1500   170000   269000   348626   395000 30000000 \n\n\nCode\nsp_data_filtered &lt;- sp_data_filtered %&gt;%\n  mutate(\n    sale_price = as.numeric(sale_price),\n    sale_price_capped = pmin(sale_price, quantile(sale_price, 0.99, na.rm = TRUE))\n  )\n\nggplot(sp_data_filtered) +\n  geom_sf(aes(color = sale_price_capped), size = 0.6, alpha = 0.7) +\n  scale_color_viridis_c(labels = label_dollar(), name = \"Sale Price (USD)\") +\n  labs(title = \"Philadelphia Sale Prices\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# function to check for statistically significant correlations between independent variables\nsig_corr &lt;- function(dat, dep_var) {\n  # remove the independent variable from the dataset\n  dat_corr &lt;- dat %&gt;% select(-all_of(dep_var))\n  \n  # run a correlation matrix for the independent vars\n  correlation_matrix &lt;- rcorr(as.matrix(dat_corr))\n  values &lt;- correlation_matrix$r\n  vifs &lt;- apply(values, 1, function(x){\n    return(round(1/(1-abs(x)), 2))\n  })\n  \n  values_df &lt;- values %&gt;% as.data.frame()\n  vifs_df &lt;- vifs %&gt;% as.data.frame()\n  \n  # convert correlation coefficients and p-values to long format\n  corCoeff_df &lt;- correlation_matrix$r %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corVIF_df &lt;- vifs %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corPval_df &lt;- correlation_matrix$P %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  # merge long format data\n  corMerge &lt;- list(\n    corCoeff_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\") %&gt;% as.data.frame(),\n    corVIF_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"vif_factor\") %&gt;% as.data.frame(),\n    corPval_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"p_value\") %&gt;% as.data.frame()) %&gt;%\n    reduce(left_join, by = c(\"var1\", \"var2\"))\n  \n  # filter to isolate unique pairs, then rows with correlation &gt; 0.5 and p &lt; 0.05\n  corUnfiltered &lt;- corMerge %&gt;% \n    filter(var1 != var2) %&gt;% \n    rowwise() %&gt;% \n    filter(var1 &lt; var2) %&gt;% \n    ungroup() %&gt;% \n    as.data.frame()\n  \n  corFiltered &lt;- corUnfiltered %&gt;% \n    filter(abs(vif_factor) &gt; 3 & p_value &lt; 0.05) %&gt;% \n    arrange(desc(abs(correlation)))\n  \n  # save the raw correlation values and the filtered variable pairs\n  final &lt;- set_names(list(values_df, vifs_df, corUnfiltered, corFiltered),\n                     c(\"R2\", \"VIF\", \"AllCor\", \"SelCor\"))\n  \n  return(final)\n}\n\n# create a dataset with just modeling variables\nOPA_modelvars &lt;- OPA_points %&gt;% select(sale_price, total_livable_area, building_age, number_of_bedrooms, number_of_bathrooms,\n                                       pop_totE, med_hh_incE, med_ageE,\n                                       dist_nearest_transit_ft, dist_nearest_hospital_ft, parks_cnt_0p25mi, log1p_crime_cnt_0p5, schools_cnt_0p5mi,\n                                       )\n\n# calculate VIFs and determine potentially troublesome correlations between variables\nvif_check &lt;- sig_corr(OPA_modelvars %&gt;% st_drop_geometry(), dep_var = \"sale_price\")\n\nkable(vif_check[[\"VIF\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_livable_area\nbuilding_age\nnumber_of_bedrooms\nnumber_of_bathrooms\npop_totE\nmed_hh_incE\nmed_ageE\ndist_nearest_transit_ft\ndist_nearest_hospital_ft\nparks_cnt_0p25mi\nlog1p_crime_cnt_0p5\nschools_cnt_0p5mi\n\n\n\n\ntotal_livable_area\nInf\n1.22\n1.70\n1.99\n1.14\n1.24\n1.07\n1.07\n1.01\n1.03\n1.22\n1.03\n\n\nbuilding_age\n1.22\nInf\n1.01\n1.32\n1.04\n1.23\n1.18\n1.21\n1.12\n1.05\n1.43\n1.24\n\n\nnumber_of_bedrooms\n1.70\n1.01\nInf\n2.26\n1.02\n1.09\n1.05\n1.05\n1.03\n1.01\n1.07\n1.02\n\n\nnumber_of_bathrooms\n1.99\n1.32\n2.26\nInf\n1.12\n1.25\n1.03\n1.02\n1.04\n1.01\n1.09\n1.01\n\n\npop_totE\n1.14\n1.04\n1.02\n1.12\nInf\n1.33\n1.16\n1.09\n1.18\n1.00\n1.01\n1.08\n\n\nmed_hh_incE\n1.24\n1.23\n1.09\n1.25\n1.33\nInf\n1.13\n1.01\n1.13\n1.04\n1.31\n1.03\n\n\nmed_ageE\n1.07\n1.18\n1.05\n1.03\n1.16\n1.13\nInf\n1.15\n1.27\n1.15\n1.70\n1.25\n\n\ndist_nearest_transit_ft\n1.07\n1.21\n1.05\n1.02\n1.09\n1.01\n1.15\nInf\n1.25\n1.11\n1.72\n1.44\n\n\ndist_nearest_hospital_ft\n1.01\n1.12\n1.03\n1.04\n1.18\n1.13\n1.27\n1.25\nInf\n1.10\n1.85\n1.66\n\n\nparks_cnt_0p25mi\n1.03\n1.05\n1.01\n1.01\n1.00\n1.04\n1.15\n1.11\n1.10\nInf\n1.25\n1.23\n\n\nlog1p_crime_cnt_0p5\n1.22\n1.43\n1.07\n1.09\n1.01\n1.31\n1.70\n1.72\n1.85\n1.25\nInf\n2.46\n\n\nschools_cnt_0p5mi\n1.03\n1.24\n1.02\n1.01\n1.08\n1.03\n1.25\n1.44\n1.66\n1.23\n2.46\nInf\n\n\n\n\n\nNone of the variables tested have a significant VIF score that is above 3, indicating that there is little concern of multicollinearity in the models moving forward."
  },
  {
    "objectID": "assignments/midterm/model_script.html#model-1-structural-terms",
    "href": "assignments/midterm/model_script.html#model-1-structural-terms",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.1 Model 1: Structural Terms",
    "text": "4.1 Model 1: Structural Terms\nOur first model uses structural property characteristics to build a multiple linear regression, regressing sale price on total livable area, number of bedrooms, number of bathrooms, and building age.\n\n\nCode\nmodel1_data &lt;- na.omit(OPA_points)\n\nmodel1 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n\n  data = model1_data\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age, data = model1_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1063434   -92640   -20598    58880  7759292 \n\nCoefficients:\n                      Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)         -11217.250  12625.853  -0.888               0.374    \ntotal_livable_area     221.516      5.126  43.217 &lt;0.0000000000000002 ***\nnumber_of_bedrooms  -34668.316   3239.669 -10.701 &lt;0.0000000000000002 ***\nnumber_of_bathrooms  70054.014   3967.307  17.658 &lt;0.0000000000000002 ***\nbuilding_age           144.447    104.814   1.378               0.168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 233700 on 10324 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2759 \nF-statistic: 984.7 on 4 and 10324 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/model_script.html#model-2-incorporation-of-census-data",
    "href": "assignments/midterm/model_script.html#model-2-incorporation-of-census-data",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.2 Model 2: Incorporation of Census Data",
    "text": "4.2 Model 2: Incorporation of Census Data\nOur second model builds on the structural property characteristics regression by incorporating census tract–level variables, including population, median household income, and median age.\n\n\nCode\nmodel2_data &lt;- na.omit(OPA_points)\n\nmodel2 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n\n    pop_totE +\n    med_hh_incE +\n    med_ageE,            \n    \n  data = model2_data\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + pop_totE + med_hh_incE + \n    med_ageE, data = model2_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-811594  -68780  -14242   37407 7882319 \n\nCoefficients:\n                        Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)         -146801.0740   22092.0331  -6.645      0.0000000000319 ***\ntotal_livable_area      182.7335       4.9502  36.914 &lt; 0.0000000000000002 ***\nnumber_of_bedrooms   -17516.7805    3085.6817  -5.677      0.0000000140948 ***\nnumber_of_bathrooms   57541.2262    3746.3467  15.359 &lt; 0.0000000000000002 ***\nbuilding_age            101.4828     101.8382   0.997                0.319    \npop_totE                 -7.5113       1.3699  -5.483      0.0000000427396 ***\nmed_hh_incE               2.5838       0.0747  34.587 &lt; 0.0000000000000002 ***\nmed_ageE                496.0580     357.9537   1.386                0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 219700 on 10321 degrees of freedom\nMultiple R-squared:  0.3601,    Adjusted R-squared:  0.3596 \nF-statistic: 829.6 on 7 and 10321 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/model_script.html#model-3-incorporation-of-spatial-features",
    "href": "assignments/midterm/model_script.html#model-3-incorporation-of-spatial-features",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.3 Model 3: Incorporation of Spatial Features",
    "text": "4.3 Model 3: Incorporation of Spatial Features\n\n\nCode\nmodel3_data &lt;- na.omit(OPA_points)\n\nmodel3 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n    \n  data = model3_data\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5, data = model3_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-918678  -69433  -13317   39370 7885217 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)              -289203.14616   43841.08957  -6.597\ntotal_livable_area           163.80852       5.66558  28.913\nnumber_of_bedrooms        -14331.54545    3085.57464  -4.645\nnumber_of_bathrooms        56708.75377    3736.10271  15.179\nbuilding_age                -222.32246     113.51444  -1.959\ntotal_area                     5.78704       0.77110   7.505\npop_totE                      -6.85323       1.38220  -4.958\nmed_hh_incE                    2.68603       0.08168  32.884\nmed_ageE                    1082.79304     372.86446   2.904\ndist_nearest_transit_ft        1.13603       7.45510   0.152\ndist_nearest_hospital_ft      -4.89926       0.77986  -6.282\nparks_cnt_0p25mi            3092.03921    3616.67427   0.855\nlog1p_crime_cnt_0p5        21979.86267    4391.65064   5.005\n                                     Pr(&gt;|t|)    \n(Intercept)                0.0000000000441242 ***\ntotal_livable_area       &lt; 0.0000000000000002 ***\nnumber_of_bedrooms         0.0000034479488026 ***\nnumber_of_bathrooms      &lt; 0.0000000000000002 ***\nbuilding_age                          0.05019 .  \ntotal_area                 0.0000000000000666 ***\npop_totE                   0.0000007228149059 ***\nmed_hh_incE              &lt; 0.0000000000000002 ***\nmed_ageE                              0.00369 ** \ndist_nearest_transit_ft               0.87889    \ndist_nearest_hospital_ft   0.0000000003472200 ***\nparks_cnt_0p25mi                      0.39260    \nlog1p_crime_cnt_0p5        0.0000005680776718 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 218400 on 10316 degrees of freedom\nMultiple R-squared:  0.3679,    Adjusted R-squared:  0.3672 \nF-statistic: 500.4 on 12 and 10316 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/model_script.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "href": "assignments/midterm/model_script.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.4 Model 4: Incorporation of Interactions and Fixed Effects",
    "text": "4.4 Model 4: Incorporation of Interactions and Fixed Effects\n\n\nCode\n# join data separately here to avoid conflicts with earlier code blocks\nOPA_points_copy &lt;- left_join(OPA_points,\n                             neighbor_points %&gt;%\n                               select(parcel_number, wealthy_neighborhood) %&gt;%\n                               st_drop_geometry(),\n                             by = \"parcel_number\")\n\n\n\n\nCode\nmodel4_data &lt;- na.omit(OPA_points_copy)\n\nmodel4 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n    \n                     \n  data = model4_data\n)\n\nsummary(model4)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5 + number_of_bathrooms * \n    wealthy_neighborhood + int_type_tarea + int_value_larea + \n    int_value_tarea + int_larea_econd + int_larea_icond + int_larea_beds, \n    data = model4_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1346085   -55380   -11966    28218  7763690 \n\nCoefficients:\n                                                         Estimate\n(Intercept)                                     124512.2529592717\ntotal_livable_area                                 157.6273524102\nnumber_of_bedrooms                               31208.7841915060\nnumber_of_bathrooms                              24997.9383903477\nbuilding_age                                      -369.4190652424\ntotal_area                                           9.5617113327\npop_totE                                            -3.8679981835\nmed_hh_incE                                          1.0393743579\nmed_ageE                                           435.9742538792\ndist_nearest_transit_ft                              1.1607100282\ndist_nearest_hospital_ft                            -3.4937314721\nparks_cnt_0p25mi                                  -471.3368815190\nlog1p_crime_cnt_0p5                              -9758.6877799857\nwealthy_neighborhoodWealthy                      39818.8812951697\nint_type_tarea                                      -0.0163412165\nint_value_larea                                      0.0001564457\nint_value_tarea                                     -0.0000081686\nint_larea_econd                                    -10.3285787884\nint_larea_icond                                    -12.2381816682\nint_larea_beds                                     -14.9092798920\nnumber_of_bathrooms:wealthy_neighborhoodWealthy  58001.7136402215\n                                                       Std. Error t value\n(Intercept)                                      45454.5022193101   2.739\ntotal_livable_area                                  14.7964439643  10.653\nnumber_of_bedrooms                                4850.3805607397   6.434\nnumber_of_bathrooms                               3766.3281923183   6.637\nbuilding_age                                       106.2530116469  -3.477\ntotal_area                                           1.4114779417   6.774\npop_totE                                             1.2822166727  -3.017\nmed_hh_incE                                          0.0911237754  11.406\nmed_ageE                                           345.6926256018   1.261\ndist_nearest_transit_ft                              6.8915594246   0.168\ndist_nearest_hospital_ft                             0.7205731337  -4.849\nparks_cnt_0p25mi                                  3343.6898579606  -0.141\nlog1p_crime_cnt_0p5                               4322.8360013350  -2.257\nwealthy_neighborhoodWealthy                      14956.5415753626   2.662\nint_type_tarea                                       0.0101374871  -1.612\nint_value_larea                                      0.0000057824  27.056\nint_value_tarea                                      0.0000007365 -11.091\nint_larea_econd                                      2.6046336489  -3.965\nint_larea_icond                                      2.0327578830  -6.020\nint_larea_beds                                       2.0097460026  -7.418\nnumber_of_bathrooms:wealthy_neighborhoodWealthy   7671.1953640371   7.561\n                                                            Pr(&gt;|t|)    \n(Intercept)                                                  0.00617 ** \ntotal_livable_area                              &lt; 0.0000000000000002 ***\nnumber_of_bedrooms                                0.0000000001295545 ***\nnumber_of_bathrooms                               0.0000000000335729 ***\nbuilding_age                                                 0.00051 ***\ntotal_area                                        0.0000000000131872 ***\npop_totE                                                     0.00256 ** \nmed_hh_incE                                     &lt; 0.0000000000000002 ***\nmed_ageE                                                     0.20728    \ndist_nearest_transit_ft                                      0.86625    \ndist_nearest_hospital_ft                          0.0000012618714105 ***\nparks_cnt_0p25mi                                             0.88790    \nlog1p_crime_cnt_0p5                                          0.02400 *  \nwealthy_neighborhoodWealthy                                  0.00777 ** \nint_type_tarea                                               0.10700    \nint_value_larea                                 &lt; 0.0000000000000002 ***\nint_value_tarea                                 &lt; 0.0000000000000002 ***\nint_larea_econd                                   0.0000737487629116 ***\nint_larea_icond                                   0.0000000017982736 ***\nint_larea_beds                                    0.0000000000001278 ***\nnumber_of_bathrooms:wealthy_neighborhoodWealthy   0.0000000000000434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201400 on 10308 degrees of freedom\nMultiple R-squared:  0.4631,    Adjusted R-squared:  0.462 \nF-statistic: 444.5 on 20 and 10308 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\n#there is only a premium on wealth neighborhood for total area, total livable area, and number of bathrooms that are significant. There is also a significant value for int_value_larea just from interacting the OPA data itsself which just assesses market value and size scalability."
  },
  {
    "objectID": "assignments/midterm/model_script.html#comparison-of-model-performance",
    "href": "assignments/midterm/model_script.html#comparison-of-model-performance",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.5 Comparison of Model Performance",
    "text": "4.5 Comparison of Model Performance\nWe can evaluate performance by conducting a 10-fold cross-validation of the 4 models, and comparing their RMSE, MAE, and \\(R^2\\).\n\n\nCode\n# Define 10-fold CV\ntrain_control &lt;- trainControl(\n  method = \"cv\",\n  number = 10,\n  savePredictions = \"final\"\n)\n\n\n\n\nCode\n# Model 1: Structural Features Only\nmodel1_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel1_cv\n\n\nLinear Regression \n\n10329 samples\n    5 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9296, 9297, 9296, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  230140.4  0.2865262  115174.1\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 2: Structural + Census\nmodel2_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    pop_totE +\n    med_hh_incE +\n    med_ageE,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel2_cv\n\n\nLinear Regression \n\n10329 samples\n    8 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9296, 9296, 9297, 9297, 9297, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  215430.3  0.3799429  90217.97\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 3: Structural + Census + Spatial\nmodel3_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel3_cv\n\n\nLinear Regression \n\n10329 samples\n   13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9295, 9295, 9296, 9298, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  211021.2  0.4053308  90144.36\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 4: Structural + Census + Spatial + Interaction\nmodel4_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n  \n  data = na.omit(OPA_points_copy),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel4_cv\n\n\nLinear Regression \n\n10329 samples\n   20 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9297, 9296, 9296, 9296, 9296, 9297, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  194649.2  0.4958671  71748.91\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\ncv_results &lt;- data.frame(\n  Model = c(\"Model 1\", \n            \"Model 2\", \n            \"Model 3\", \n            \"Model 4\"),\n  RMSE = c(\n        model1_cv$results$RMSE,\n        model2_cv$results$RMSE,\n        model3_cv$results$RMSE,\n        model4_cv$results$RMSE\n      ),\n  \n  log_RMSE = c(\n        log(model1_cv$results$RMSE),\n        log(model2_cv$results$RMSE),\n        log(model3_cv$results$RMSE),\n        log(model4_cv$results$RMSE)\n      ),\n  \n    MAE = c(\n        model1_cv$results$MAE,\n        model2_cv$results$MAE,\n        model3_cv$results$MAE,\n        model4_cv$results$MAE\n      ),\n  R_squared = c(\n        model1_cv$results$Rsquared,\n        model2_cv$results$Rsquared,\n        model3_cv$results$Rsquared,\n        model4_cv$results$Rsquared\n      )\n)\n\nprint(cv_results)\n\n\n    Model     RMSE log_RMSE       MAE R_squared\n1 Model 1 230140.4 12.34644 115174.13 0.2865262\n2 Model 2 215430.3 12.28039  90217.97 0.3799429\n3 Model 3 211021.2 12.25971  90144.36 0.4053308\n4 Model 4 194649.2 12.17895  71748.91 0.4958671\n\n\n\n\nCode\n# create model coefficient table in stargazer\nmodels_list &lt;- list(model1, model2, model3, model4)\nmodels_summary_table &lt;- stargazer(models_list, type = \"text\", style = \"default\")\n\n\n\n=============================================================================================================================================================\n                                                                                             Dependent variable:                                             \n                                                -------------------------------------------------------------------------------------------------------------\n                                                                                                 sale_price                                                  \n                                                           (1)                        (2)                         (3)                         (4)            \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\ntotal_livable_area                                      221.516***                 182.734***                 163.809***                  157.627***         \n                                                         (5.126)                    (4.950)                     (5.666)                    (14.796)          \n                                                                                                                                                             \nnumber_of_bedrooms                                    -34,668.320***             -17,516.780***             -14,331.550***               31,208.780***       \n                                                       (3,239.669)                (3,085.682)                 (3,085.575)                 (4,850.381)        \n                                                                                                                                                             \nnumber_of_bathrooms                                   70,054.010***              57,541.230***               56,708.750***               24,997.940***       \n                                                       (3,967.307)                (3,746.347)                 (3,736.103)                 (3,766.328)        \n                                                                                                                                                             \nbuilding_age                                             144.447                    101.483                    -222.322*                  -369.419***        \n                                                        (104.814)                  (101.838)                   (113.514)                   (106.253)         \n                                                                                                                                                             \ntotal_area                                                                                                     5.787***                    9.562***          \n                                                                                                                (0.771)                     (1.411)          \n                                                                                                                                                             \npop_totE                                                                           -7.511***                   -6.853***                   -3.868***         \n                                                                                    (1.370)                     (1.382)                     (1.282)          \n                                                                                                                                                             \nmed_hh_incE                                                                         2.584***                   2.686***                    1.039***          \n                                                                                    (0.075)                     (0.082)                     (0.091)          \n                                                                                                                                                             \nmed_ageE                                                                            496.058                  1,082.793***                   435.974          \n                                                                                   (357.954)                   (372.864)                   (345.693)         \n                                                                                                                                                             \ndist_nearest_transit_ft                                                                                          1.136                       1.161           \n                                                                                                                (7.455)                     (6.892)          \n                                                                                                                                                             \ndist_nearest_hospital_ft                                                                                       -4.899***                   -3.494***         \n                                                                                                                (0.780)                     (0.721)          \n                                                                                                                                                             \nparks_cnt_0p25mi                                                                                               3,092.039                   -471.337          \n                                                                                                              (3,616.674)                 (3,343.690)        \n                                                                                                                                                             \nlog1p_crime_cnt_0p5                                                                                          21,979.860***               -9,758.688**        \n                                                                                                              (4,391.651)                 (4,322.836)        \n                                                                                                                                                             \nwealthy_neighborhoodWealthy                                                                                                              39,818.880***       \n                                                                                                                                         (14,956.540)        \n                                                                                                                                                             \nint_type_tarea                                                                                                                              -0.016           \n                                                                                                                                            (0.010)          \n                                                                                                                                                             \nint_value_larea                                                                                                                            0.0002***         \n                                                                                                                                           (0.00001)         \n                                                                                                                                                             \nint_value_tarea                                                                                                                           -0.00001***        \n                                                                                                                                           (0.00000)         \n                                                                                                                                                             \nint_larea_econd                                                                                                                           -10.329***         \n                                                                                                                                            (2.605)          \n                                                                                                                                                             \nint_larea_icond                                                                                                                           -12.238***         \n                                                                                                                                            (2.033)          \n                                                                                                                                                             \nint_larea_beds                                                                                                                            -14.909***         \n                                                                                                                                            (2.010)          \n                                                                                                                                                             \nnumber_of_bathrooms:wealthy_neighborhoodWealthy                                                                                          58,001.710***       \n                                                                                                                                          (7,671.195)        \n                                                                                                                                                             \nConstant                                               -11,217.250              -146,801.100***             -289,203.100***             124,512.300***       \n                                                       (12,625.850)               (22,092.030)               (43,841.090)                (45,454.500)        \n                                                                                                                                                             \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nObservations                                              10,329                     10,329                     10,329                      10,329           \nR2                                                        0.276                      0.360                       0.368                       0.463           \nAdjusted R2                                               0.276                      0.360                       0.367                       0.462           \nResidual Std. Error                              233,656.600 (df = 10324)   219,730.600 (df = 10321)   218,431.100 (df = 10316)    201,398.000 (df = 10308)  \nF Statistic                                     984.706*** (df = 4; 10324) 829.572*** (df = 7; 10321) 500.372*** (df = 12; 10316) 444.490*** (df = 20; 10308)\n=============================================================================================================================================================\nNote:                                                                                                                             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nCode\n# plot predicted vs actual value plots\nggplot(model1_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 1 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model2_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 2 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model3_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 3 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model4_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 4 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-this-course-is-about",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-this-course-is-about",
    "title": "Welcome to MUSA 5080",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nAdvanced spatial analysis for urban planning and public policy\nData science tools within policy context\nFocus on understanding concepts rather than just completing code\nProfessional portfolio development using modern tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#unlike-private-sector-data-science",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#unlike-private-sector-data-science",
    "title": "Welcome to MUSA 5080",
    "section": "Unlike Private Sector Data Science",
    "text": "Unlike Private Sector Data Science\n\nNot just about optimization\nPublic goods, governance, equity considerations\nTransparency and interpretability are crucial\nAlgorithmic bias has real consequences for communities"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-semesters-innovation",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-semesters-innovation",
    "title": "Welcome to MUSA 5080",
    "section": "This Semester’s Innovation",
    "text": "This Semester’s Innovation\nProblem: AI tools making it easy to complete code without understanding\nSolution:\n\n40% weekly in-class quizzes (test conceptual understanding)\nLow-stakes portfolio assignments (focus on learning, not grades)\nGitHub-based workflow (professional skills)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-these-tools",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-these-tools",
    "title": "Welcome to MUSA 5080",
    "section": "Why These Tools?",
    "text": "Why These Tools?\nGitHub: Industry standard for version control and collaboration\nQuarto: Modern approach to reproducible research and documentation\nR: Powerful for spatial analysis and policy-focused statistics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#professional-development",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#professional-development",
    "title": "Welcome to MUSA 5080",
    "section": "Professional Development",
    "text": "Professional Development\nThese aren’t just “class tools” - they’re career tools:\n\nPortfolio employers can see\nVersion control skills for any data job\nProfessional documentation practices"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-git",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-git",
    "title": "Welcome to MUSA 5080",
    "section": "What is Git?",
    "text": "What is Git?\nVersion control system that tracks changes in files\nThink of it as:\n\n“Track changes” for code projects\nTime machine for your work\nCollaboration tool for teams"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nCloud hosting for Git repositories\n\nBackup your work in the cloud\n\nShare projects with others\nDeploy websites (like our portfolios)\nCollaborate on code projects"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#key-github-concepts",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#key-github-concepts",
    "title": "Welcome to MUSA 5080",
    "section": "Key GitHub Concepts",
    "text": "Key GitHub Concepts\nRepository (repo): Folder containing your project files\nCommit: Snapshot of your work at a point in time\nPush: Send your changes to GitHub cloud\nPull: Get latest changes from GitHub cloud"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#github-in-this-course",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#github-in-this-course",
    "title": "Welcome to MUSA 5080",
    "section": "GitHub in This Course",
    "text": "GitHub in This Course\nYour workflow each week:\n\nEdit files in RStudio\nCommit changes with descriptive message\n\nPush to GitHub\nYour portfolio website updates automatically"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-will-become-second-nature-soon",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#this-will-become-second-nature-soon",
    "title": "Welcome to MUSA 5080",
    "section": "This will become second nature soon!",
    "text": "This will become second nature soon!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github-classroom",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-github-classroom",
    "title": "Welcome to MUSA 5080",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nEducational tool that:\n\nCreates individual repositories for each student\nDistributes assignments automatically\n\nEnables efficient feedback and grading\nTeaches professional Git workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#how-it-works",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#how-it-works",
    "title": "Welcome to MUSA 5080",
    "section": "How It Works",
    "text": "How It Works\n\nDr. Delmelle creates assignment with starter code\nYou accept assignment via special link\nGitHub creates your personal repository\nYou complete work in your repository\nTAs provide feedback through GitHub tools"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#benefits-for-you",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#benefits-for-you",
    "title": "Welcome to MUSA 5080",
    "section": "Benefits for You",
    "text": "Benefits for You\n\nIndividual workspace that’s yours to customize\nProfessional portfolio you can show employers\nVersion control practice for future jobs\nDirect feedback from instructors on your code"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-is-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nPublishing system that combines:\n\nCode (R, Python, etc.)\nText (explanations, analysis)\nOutput (plots, tables, results)\n\nInto professional documents"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-quarto",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-quarto",
    "title": "Welcome to MUSA 5080",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nReproducible research:\n\nCode and explanation in one place\nOthers can re-run your analysis\nProfessional presentation\n\nCareer relevance:\n\nIndustry standard for data science communication\nCreates websites, PDFs, presentations\nUsed at major tech companies and government agencies"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-vs.-r-markdown",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-vs.-r-markdown",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto vs. R Markdown",
    "text": "Quarto vs. R Markdown\nIf you know R Markdown:\n\nQuarto is the “next generation”\nBetter website creation\nWorks with multiple programming languages\nSame basic concept, improved features"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-document-structure",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#quarto-document-structure",
    "title": "Welcome to MUSA 5080",
    "section": "Quarto Document Structure",
    "text": "Quarto Document Structure\nYAML header:\n---\ntitle: \"My Analysis\" \nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nR code chunk:\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/car_sales_data.csv\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#text-formatting",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#text-formatting",
    "title": "Welcome to MUSA 5080",
    "section": "Text Formatting",
    "text": "Text Formatting\n**Bold text**\n*Italic text*\n***Bold and italic***\n`code text`\n~~Strikethrough~~\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#headers",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#headers",
    "title": "Welcome to MUSA 5080",
    "section": "Headers",
    "text": "Headers\n# Main Header\n## Section Header  \n### Subsection Header\nUse headers to organize your analysis sections."
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#lists",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#lists",
    "title": "Welcome to MUSA 5080",
    "section": "Lists",
    "text": "Lists\n## Unordered List\n- Item 1\n- Item 2\n  - Sub-item A\n  - Sub-item B\n\n## Ordered List  \n1. First item\n2. Second item\n3. Third item"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#links-and-images",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#links-and-images",
    "title": "Welcome to MUSA 5080",
    "section": "Links and Images",
    "text": "Links and Images\n[Link text](https://example.com)\n[Link to another page](about.qmd)\n![Alt text](path/to/image.png)\nEssential for professional portfolios:\n\nLink to data sources\nReference course materials\n\nInclude relevant images/plots"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-r-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-r-for-policy-analysis",
    "title": "Welcome to MUSA 5080",
    "section": "Why R for Policy Analysis?",
    "text": "Why R for Policy Analysis?\n\nFree and open source\nExcellent for spatial data\nStrong statistical capabilities\nLarge community in urban planning/policy\nReproducible research workflows"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#rstudio-projects-essential-habit",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#rstudio-projects-essential-habit",
    "title": "Welcome to MUSA 5080",
    "section": "Rstudio Projects: Essential Habit",
    "text": "Rstudio Projects: Essential Habit\nAlways work within projects\n\norganized file structure - data, scripts, outputs in one place\nRelative file paths - `“data/cars.csv”’"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tidyverse-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tidyverse-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "tidyverse Philosophy",
    "text": "tidyverse Philosophy\nCollection of packages designed for data science:\n\nConsistent syntax across functions\nReadable code that tells a story\nEfficient workflows for common tasks"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tibbles-vs.-data-frames",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#tibbles-vs.-data-frames",
    "title": "Welcome to MUSA 5080",
    "section": "Tibbles vs. Data Frames",
    "text": "Tibbles vs. Data Frames\nTidyverse uses “tibbles” - enhanced data frames.\n#Traditional Data Frame\nclass(data)\n# Convert to tibble\ncar_data &lt;- as_tibble(data)\nclass(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-are-tibbles-better",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#why-are-tibbles-better",
    "title": "Welcome to MUSA 5080",
    "section": "Why are Tibbles Better?",
    "text": "Why are Tibbles Better?\nSmarter Printing:\n\nShows first 10 rows by default\nDisplays column names\nfits nicely on a screen"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#essential-dplyr-functions",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#essential-dplyr-functions",
    "title": "Welcome to MUSA 5080",
    "section": "Essential dplyr Functions",
    "text": "Essential dplyr Functions\nWe’ll use these constantly:\n\nselect() - choose columns\nfilter() - choose rows\n\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#live-demo-basic-dplyr",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#live-demo-basic-dplyr",
    "title": "Welcome to MUSA 5080",
    "section": "Live Demo: Basic dplyr",
    "text": "Live Demo: Basic dplyr\nlibrary(tidyverse)\n\n# Load car sales data\ncar_data &lt;- read_csv(\"data/car_sales_data.csv\")\n\n# Basic exploration\nglimpse(car_data)\nnames(car_data)"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#data-manipulation-pipeline",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#data-manipulation-pipeline",
    "title": "Welcome to MUSA 5080",
    "section": "Data Manipulation Pipeline",
    "text": "Data Manipulation Pipeline\n\n# The power of pipes - read as \"then\"\ncar_summary &lt;- data %&gt;%\n  filter(`Year of manufacture` &gt;= 2020) %&gt;%      # Recent models only\n  select(Manufacturer, Model, Price, Mileage) %&gt;% # Key variables\n  mutate(price_k = Price / 1000) %&gt;%             # Convert to thousands\n  filter(Mileage &lt; 50000) %&gt;%                    # Low mileage cars\n  group_by(Manufacturer) %&gt;%                     # Group by brand\n  summarize(                                     # Calculate statistics\n    avg_price = mean(price_k, na.rm = TRUE),\n    count = n()\n  )"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#policy-applications",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#policy-applications",
    "title": "Welcome to MUSA 5080",
    "section": "Policy Applications",
    "text": "Policy Applications\nThis semester we’ll use these skills for:\n\nCensus data analysis\nNeighborhood change studies\nPredictive modeling for resource allocation\nHousing market analysis\nTransportation equity assessment"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#weekly-pattern",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#weekly-pattern",
    "title": "Welcome to MUSA 5080",
    "section": "Weekly Pattern",
    "text": "Weekly Pattern\nMonday Class: - New concepts and methods - Hands-on coding practice - Lab work with TA support\nDuring Week: - Complete portfolio assignments - Weekly notes and reflection - Office hours for help"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#assessment-philosophy",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#assessment-philosophy",
    "title": "Welcome to MUSA 5080",
    "section": "Assessment Philosophy",
    "text": "Assessment Philosophy\nFocus on understanding, not perfect code:\n\nWeekly quizzes test concepts\nPortfolio assignments build skills\nLow stakes encourage experimentation\nProfessional development throughout"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-development",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-development",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Development",
    "text": "Portfolio Development\nYour GitHub portfolio will include:\n\nWeekly learning reflections\nCompleted lab analyses\nProfessional documentation\nWork you can show employers"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-setup-process",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#portfolio-setup-process",
    "title": "Welcome to MUSA 5080",
    "section": "Portfolio Setup Process",
    "text": "Portfolio Setup Process\n\nAccept GitHub Classroom assignment\nClone repository to your computer\n\nCustomize with your information\nEnable GitHub Pages\nComplete first analysis"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-well-accomplish",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#what-well-accomplish",
    "title": "Welcome to MUSA 5080",
    "section": "What We’ll Accomplish",
    "text": "What We’ll Accomplish\nBy end of today: - Working portfolio repository - Live website with your work - First R analysis in professional format - Familiarity with workflow"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#support-available",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#support-available",
    "title": "Welcome to MUSA 5080",
    "section": "Support Available",
    "text": "Support Available\n\nDr. Delmelle and TAs circulating during hands-on time\nOffice hours starting this week\nGitHub Issues for technical questions\nCanvas discussion for course questions"
  },
  {
    "objectID": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#ready-to-get-started",
    "href": "ClassMaterials_Copy/week-01-intro/week1_lecture_slides.html#ready-to-get-started",
    "title": "Welcome to MUSA 5080",
    "section": "Ready to Get Started?",
    "text": "Ready to Get Started?\nNext: Portfolio setup with GitHub Classroom\nRemember: This is a learning process - ask for help when you need it!"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#what-well-cover",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#opening-question",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-policy-implications",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nlibrary(tidycensus)\n\nWarning: package 'tidycensus' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n# Get some census data for demonstration\ndemo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(county_name = str_remove(NAME, \", Pennsylvania\"))\n\nGetting data from the 2018-2022 5-year ACS\n\n# Basic scatter plot\nggplot(demo_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point()"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes\n\nggplot(demo_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Income vs Population in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau ACS\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_x_continuous(labels = scales::comma)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#the-eda-mindset",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#understanding-distributions",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\n# Histogram of median income\nggplot(demo_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Median Income\",\n       x = \"Median Household Income\", \n       y = \"Number of Counties\")\n\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#boxplots",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!\n\n# Boxplot to see outliers and quartiles  \nggplot(demo_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Income Distribution with Outliers\",\n       y = \"Median Household Income\")"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\n# Visualize margins of error - ESSENTIAL for ethical practice\ndemo_data %&gt;%\n  mutate(moe_pct = median_incomeM / median_incomeE * 100) %&gt;%\n  ggplot() +\n  aes(x = total_popE, y = moe_pct) +\n  geom_point() +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Data Reliability by Population Size\",\n       x = \"Population\", \n       y = \"Margin of Error (%)\",\n       caption = \"Red line = 10% threshold for reliability\")\n\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#why-join-data",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n# Get income data\nincome_data &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"PA\", \n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, NAME, median_income = B19013_001E, income_moe = B19013_001M)\n\nGetting data from the 2018-2022 5-year ACS\n\n# Get education data  \neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B15003_022\",  # Bachelor's degree or higher\n  state = \"PA\",\n  year = 2022, \n  output = \"wide\"\n) %&gt;%\n  select(GEOID, college_pop = B15003_022E, college_moe = B15003_022M)\n\nGetting data from the 2018-2022 5-year ACS\n\n# Join them together\ncombined_data &lt;- income_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\nhead(combined_data)\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n# Check dimensions\ncat(\"Income data rows:\", nrow(income_data), \"\\n\")\n\nIncome data rows: 67 \n\ncat(\"Education data rows:\", nrow(education_data), \"\\n\") \n\nEducation data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\n# Check for missing values after join\ncombined_data %&gt;%\n  summarize(\n    missing_income = sum(is.na(median_income)),\n    missing_education = sum(is.na(college_pop))\n  )\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n# IMPORTANT: Assess reliability of joined data\ncombined_data %&gt;%\n  mutate(\n    income_cv = (income_moe / median_income) * 100,\n    college_cv = (college_moe / college_pop) * 100\n  ) %&gt;%\n  select(NAME, income_cv, college_cv) %&gt;%\n  head()\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#skills-youll-practice",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "ClassMaterials_Copy/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "ClassMaterials_Copy/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)\n\n\n\n\n\nEmail: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "labs/lab0/lab0_template.html",
    "href": "labs/lab0/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7 - Variable types: Character (chr), Double (dbl) - Problematic names: The columns that have spaces in their names require quotes around them while scripting"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\ncar_df %&gt;% head(10)\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: In Markdown, the identifier icon in the top left changes from “A tibble: 50,000 x 7” to “Description: df [50,000 x 7]” for tibbles and dfs, respectively. When rendering the website, printing a data frame prints all rows and does not allow for scrolling across columns. Instead, it prints extra columns in another row."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#selecting-columns",
    "href": "labs/lab0/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nselect(.data = car_data, Model, Mileage)\n\n# A tibble: 50,000 × 2\n   Model      Mileage\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Fiesta      127300\n 2 718 Cayman   57850\n 3 Mondeo       39190\n 4 RAV4        210814\n 5 Polo        127869\n 6 Focus        33603\n 7 Mondeo       86686\n 8 Prius        30663\n 9 Polo         73470\n10 Focus       262514\n# ℹ 49,990 more rows\n\n# Select Manufacturer, Price, and Fuel type\ncar_data %&gt;% select(Manufacturer, Price, `Fuel type`)\n\n# A tibble: 50,000 × 3\n   Manufacturer Price `Fuel type`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n 1 Ford          3074 Petrol     \n 2 Porsche      49704 Petrol     \n 3 Ford         24072 Diesel     \n 4 Toyota        1705 Hybrid     \n 5 VW            4101 Petrol     \n 6 Ford         29204 Petrol     \n 7 Ford         14350 Diesel     \n 8 Toyota       30297 Hybrid     \n 9 VW            9977 Petrol     \n10 Ford          1049 Diesel     \n# ℹ 49,990 more rows\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data %&gt;% select(-`Engine size`)\n\n# A tibble: 50,000 × 6\n   Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta     Petrol                       2002  127300  3074\n 2 Porsche      718 Cayman Petrol                       2016   57850 49704\n 3 Ford         Mondeo     Diesel                       2014   39190 24072\n 4 Toyota       RAV4       Hybrid                       1988  210814  1705\n 5 VW           Polo       Petrol                       2006  127869  4101\n 6 Ford         Focus      Petrol                       2018   33603 29204\n 7 Ford         Mondeo     Diesel                       2010   86686 14350\n 8 Toyota       Prius      Hybrid                       2015   30663 30297\n 9 VW           Polo       Petrol                       2012   73470  9977\n10 Ford         Focus      Diesel                       1992  262514  1049\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#renaming-columns",
    "href": "labs/lab0/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;% \n  rename(year = `Year of manufacture`)\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: year no longer has spaces"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;%\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\ncar_data %&gt;% select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorize-cars",
    "href": "labs/lab0/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is luxury (use case_when)\ncar_data &lt;- car_data %&gt;% \n  mutate(price_category = case_when(Price &lt; 15000 ~ \"budget\",\n                                     Price &gt;= 15000 & Price &lt; 30000 ~ \"midrange\",\n                                     .default = \"luxury\"))\n\n# Check your categories select the new column and show it\ncar_data %&gt;% select(Price, price_category)\n\n# A tibble: 50,000 × 2\n   Price price_category\n   &lt;dbl&gt; &lt;chr&gt;         \n 1  3074 budget        \n 2 49704 luxury        \n 3 24072 midrange      \n 4  1705 budget        \n 5  4101 budget        \n 6 29204 midrange      \n 7 14350 budget        \n 8 30297 luxury        \n 9  9977 budget        \n10  1049 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-filtering",
    "href": "labs/lab0/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;% filter(Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;% filter(Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter(price_category == \"luxury\" & Mileage &lt; 30000)\n\n# A tibble: 3,257 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,247 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Ford OR Porsche\ncar_data %&gt;% filter(Manufacturer %in% c(\"Ford\", \"Porsche\"))\n\n# A tibble: 17,568 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta               1   Petrol       2002  127300  3074    23\n 2 Porsche      718 Cayman           4   Petrol       2016   57850 49704     9\n 3 Ford         Mondeo               1.6 Diesel       2014   39190 24072    11\n 4 Ford         Focus                1.4 Petrol       2018   33603 29204     7\n 5 Ford         Mondeo               1.8 Diesel       2010   86686 14350    15\n 6 Ford         Focus                2   Diesel       1992  262514  1049    33\n 7 Ford         Mondeo               1.6 Diesel       1996   77584  5667    29\n 8 Porsche      911                  2.6 Petrol       2009   66273 41963    16\n 9 Porsche      911                  3.5 Petrol       2005  151556 19747    20\n10 Ford         Focus                1   Hybrid       2010   85131 12472    15\n# ℹ 17,558 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\ncar_data %&gt;% filter(Price &gt; 20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter(`Fuel type` == \"Diesel\" & age &lt; 10)\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2,040 vehicles"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-summaries",
    "href": "labs/lab0/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel &lt;- car_data %&gt;% \n  group_by(`Fuel type`) %&gt;% \n  summarize(avg_mileage = mean(Mileage, na.rm = T))\n\navg_mileage_by_fuel\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncount_by_manufacturer &lt;- car_data %&gt;% \n  group_by(Manufacturer) %&gt;% \n  summarize(count = n())\n\ncount_by_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\nprice_cat_freq_table &lt;- car_data %&gt;%\n  group_by(price_category) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(freq = count / sum(count))\n\nprice_cat_freq_table\n\n# A tibble: 3 × 3\n  price_category count  freq\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 budget         34040 0.681\n2 luxury          6179 0.124\n3 midrange        9781 0.196"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html",
    "href": "labs/midterm/Midterm_2025.html",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Due Date: October 27, 2025\nIn-Class Presentations: October 27, 2025 (5 minutes per team)\nWeight: 15% of final grade\nTeam: You’ll work with your table-mates as a team. Feel free to delegate. Everyone should upload their final products onto their own portfolio websites. Be sure to acknowledge your team-mates.\nSubmission Format:\n\nPresentation Slides (.qmd → revealjs, ~10-15 slides) - Main deliverable (see my weekly lecture notes for inspiration!)\nTechnical Appendix (.qmd → HTML document) - Supporting details"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#overview",
    "href": "labs/midterm/Midterm_2025.html#overview",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Due Date: October 27, 2025\nIn-Class Presentations: October 27, 2025 (5 minutes per team)\nWeight: 15% of final grade\nTeam: You’ll work with your table-mates as a team. Feel free to delegate. Everyone should upload their final products onto their own portfolio websites. Be sure to acknowledge your team-mates.\nSubmission Format:\n\nPresentation Slides (.qmd → revealjs, ~10-15 slides) - Main deliverable (see my weekly lecture notes for inspiration!)\nTechnical Appendix (.qmd → HTML document) - Supporting details"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#the-challenge",
    "href": "labs/midterm/Midterm_2025.html#the-challenge",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "The Challenge",
    "text": "The Challenge\nYou are consultancy (please name your consultancy) competing to win the bid to work for a project for the Philadelphia Office of Property Assessment. The city wants to improve its Automated Valuation Model (AVM) for property tax assessments. Your task is to build a predictive model for residential sale prices and present your findings to city officials in a 5-minute briefing.\nDeliverables:\n\nPresentation slides (10-15 slides MAX) - Your main findings for stakeholders\nTechnical appendix (HTML document) - All code, diagnostics, and detailed analysis\n5-minute in-class presentation - Deliver your slides on October 27th\n\nYour goal: Predict 2023-2024 home sale prices accurately while communicating findings clearly to a policy audience."
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#two-part-submission",
    "href": "labs/midterm/Midterm_2025.html#two-part-submission",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Two-Part Submission",
    "text": "Two-Part Submission\n\nPart A: Presentation Slides (Primary Deliverable)\nFormat: Quarto revealjs presentation\nExample YAML:\n---\ntitle: \"Philadelphia Housing Price Prediction\"\nsubtitle: \"Improving Property Tax Assessments\"\nauthor: \"Your Name\"\nformat: \n  revealjs:\n    theme: simple\n    slide-number: true\n    smaller: true\n---\nContent: ~10-15 (could be less!!) slides covering: 1. Research question & motivation (1-2 slides) 2. Data overview (1 slide) 3. Key visualizations (2-3 slides) 4. Model comparison results (1-2 slides) 5. Main findings (2-3 slides) 6. Policy recommendations (1-2 slides)\nAudience: City officials who don’t know R or care about the nitty gritty of stats. They just want the best estimates possible.\nNo code in these slides - just polished visualizations and key takeaways\n\n\n\nPart B: Technical Appendix (Supporting Documentation)\nFormat: Quarto HTML document\nExample YAML:\n---\ntitle: \"Philadelphia Housing Model - Technical Appendix\"\nauthor: \"Your Name\"\nformat: \n  html:\n    code-fold: show\n    toc: true\n    toc-location: left\n    theme: cosmo\n---\nContent: All the technical details: - Complete data cleaning code - All EDA visualizations - Feature engineering code - Full model outputs - Diagnostic plots - Detailed interpretations\nAudience: Data scientists and technical reviewers\nAll code visible - this is where you show your work"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#data-sources",
    "href": "labs/midterm/Midterm_2025.html#data-sources",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary Dataset: Philadelphia Property Sales\nSource: Philadelphia Property Sales\nThis dataset contains actual property sales with:\n\nSale price\nSale date\nProperty characteristics (bedrooms, bathrooms, sq ft, etc.)\nProperty location (address, coordinates)\n\nYou will need to:\n\nDownload the data\nClean it (missing values, outliers, data errors)\nFilter to 2023-2024 residential sales only\n\n\n\nSecondary Datasets (You Choose!)\nRequired: Browse the OpenPhily Data portal and use Census Data to incorporate spatial features into your model.\nYour task: Think like an urban planner. What location factors matter for housing prices in Philadelphia?"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#assignment-structure",
    "href": "labs/midterm/Midterm_2025.html#assignment-structure",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Assignment Structure",
    "text": "Assignment Structure\nYour work should follow this workflow, with results split between presentation and appendix:\n\nPhase 1: Data Preparation (Technical Appendix)\nLoad and clean Philadelphia sales data:\n\nFilter to residential properties, 2023-2024 sales\nRemove obvious errors\nHandle missing values\nDocument all cleaning decisions\n\nLoad secondary data:\n\nCensus data (tidycensus):\nSpatial amenities (OpenDataPhilly)\nJoin to sales data appropriately\nMake sure you have the correct CRS!\n\nDeliverable (Appendix only):\n\nComplete data cleaning code\nSummary tables showing before/after dimensions\nNarrative explaining decisions\n\n\n\n\nPhase 2: Exploratory Data Analysis\nCreate at least 5 professional visualizations:\n\nDistribution of sale prices (histogram)\nGeographic distribution (map)\nPrice vs. structural features (scatter plots)\nPrice vs. spatial features (scatter plots)\nOne creative visualization\n\nFor presentation slides: Select your best 2-3 visualizations that tell a compelling story\nFor appendix: Include all visualizations with detailed interpretations\nExample presentation slide:\n## Where Are Expensive Homes in Philadelphia?\n\n[Beautiful map showing price patterns]\n\n**Key Findings:**\n- Center City and University City command premium prices\n- River wards show emerging appreciation\n- Northeast Philadelphia remains most affordable\n\n\n\nPhase 3: Feature Engineering (Technical Appendix)\nCreate spatial features: (these are examples below, but how you construct your model is up to your team)\n\nBuffer-based features:\n\nParks within 500ft, 1000ft\nTransit stops within 400ft\nSchools, crime, etc.\n\nk-Nearest Neighbor features:\n\nAverage distance to k nearest parks, transit, etc.\n\nCensus variables:\n\nJoin median income, education, poverty, etc.\n\nInteraction terms:\n\nTheoretically motivated combinations\n\n\nDeliverable (Appendix only):\n\nAll feature engineering code\nSummary table of features created\nBrief justification for each feature\n\n\n\n\nPhase 4: Model Building\nBuild models progressively: (for example)\n\nStructural features only\n\nCensus variables\n\n\nSpatial features\n\n\nInteractions and fixed effects\n\n\nFor presentation slides: Show one comparison table (RMSE, R² for 4 different models you constructed in your process)\nFor appendix:\n\nComplete model code\nFull stargazer/modelsummary output\nCoefficient interpretations\n\nExample presentation slide:\n## Model Performance Improves with Each Layer\n\n| Model | CV RMSE (log) | R² |\n|-------|---------------|-----|\n| Structural Only | 0.42 | 0.61 |\n| + Census | 0.38 | 0.69 |\n| + Spatial | 0.31 | 0.78 |\n| + Interactions/FE | 0.26 | 0.84 |\n\n**Bottom line:** Neighborhood effects matter most!\n\n\n\nPhase 5: Model Validation\nUse 10-fold cross-validation: - Compare all 4 models - Report RMSE, MAE, R² for each - Create predicted vs. actual plot\nFor presentation slides: Final CV results table (shown above) + one compelling visual\nFor appendix:\n\nComplete CV code\nDetailed results\nPredicted vs. actual scatter plot\nDiscussion of which features matter most\n\n\n\n\nPhase 6: Model Diagnostics (Technical Appendix Only)\nCheck assumptions for best model:\n\nResidual plot (linearity, homoscedasticity)\nQ-Q plot (normality)\nCook’s distance (influential observations)\n\nDeliverable (Appendix only):\n\nAll 3 diagnostic plots\nInterpretation of each\nHow you addressed violations (if any)\n\nNote: Don’t include diagnostic plots in presentation - too technical!\n\n\n\nPhase 7: Conclusions & Recommendations\nAnswer these questions:\n\nWhat is your final model’s accuracy?\nWhich features matter most for Philadelphia prices?\nWhich neighborhoods are hardest to predict?\nEquity concerns?\nLimitations?\n\nFor presentation slides: 1-2 slides with clear, concise answers (bullet points)\nFor appendix: 2-3 paragraphs with detailed discussion\nExample presentation slide:\n## Key Findings & Recommendations\n\n**Model Accuracy:** RMSE = 0.26 (log scale) ≈ 26% typical error\n\n**Top Predictors:**\n- Neighborhood fixed effects (largest impact)\n- Square footage (β = 0.0003, p &lt; 0.001)\n- Distance to transit (β = -0.05, p &lt; 0.001)\n\n**Recommendations:**\n✓ Current AVM undervalues transit-accessible properties  \n✓ Model struggles in rapidly gentrifying neighborhoods"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#submission-requirements",
    "href": "labs/midterm/Midterm_2025.html#submission-requirements",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Submission Requirements",
    "text": "Submission Requirements\n\nWhat to Submit (by 9:59 AM, October 27, 2025)\nUpload to Canvas - A link to your portfolio that Contains\n\nPresentation Slides\n\nLastName_FirstName_Presentation.html (rendered slides)\nLastName_FirstName_Presentation.qmd (source file)\nMust use format: revealjs\n10-15 slides maximum\nNo code visible in slides\n\nTechnical Appendix\n\nLastName_FirstName_Appendix.html (rendered document)\nLastName_FirstName_Appendix.qmd (source file)\nMust use format: html\nAll code visible and commented\nComplete analysis documented\n\nData files OR clear download instructions in appendix\n\nFor Teams: Use LastName1_LastName2_Presentation.html\n\n\nIn-Class Presentation (October 27, 2025)\nFormat: 5 minutes per team\nWhat to present:\n\nWalk through your presentation slides. Choose your team’s spoke’s person or take turns You’ll all stand up there and try to look calm, confident, & collected.\nHit the highlights - research question, key viz, model results, recommendations\nSpeak to a policy audience (your classmates are pretending to be city officials)\nBe ready for 1-2 questions\nYou are trying to win the bid! Convince the audience of your agency’s work.\n\nWhat NOT to do:\n\nDon’t read slides verbatim\nDon’t show code\nDon’t go into technical details\nDon’t go over 5 minutes (I’ll cut you off!)"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#grading-rubric-scaled-to-15-of-course-grade",
    "href": "labs/midterm/Midterm_2025.html#grading-rubric-scaled-to-15-of-course-grade",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Grading Rubric (Scaled to 15% of course grade)",
    "text": "Grading Rubric (Scaled to 15% of course grade)\n\nPresentation Slides\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nResearch Question\n2\nClear motivation, Set the stage\n\n\nData Overview\n2\nConcise description of sources, sample size\n\n\nVisualizations\n3\n2-3 polished, publication-quality visualizations; clear takeaways\n\n\nModel Comparison\n3\nClean results table; clear winner; interprets improvement\n\n\nKey Findings\n3\nTop predictors identified; coefficients interpreted correctly\n\n\nPresentation Quality\n3\nProfessional design, no typos, flows logically, appropriate for audience\n\n\n\nKey: Slides should tell a compelling story without technical jargon. Imagine presenting to the Deputy Mayor.\n\n\n\nIn-Class Presentation\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nContent\n2\nCovers key points efficiently, answers questions thoughtfully\n\n\nTime Management\n2\nFinishes within 5 minutes without rushing\n\n\n\n\n\n\nTechnical Appendix\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nData Cleaning\n3\nComplete code, proper filtering, missing value handling, documentation\n\n\nEDA\n3\n5+ visualizations, each with interpretation\n\n\nFeature Engineering\n3\nBuffers , kNN , census , interactions , all properly created\n\n\nModel Building\n3\n4 progressive models, proper specification, handles sparse categories\n\n\nCross-Validation\n3\nProper 10-fold CV, results table, code runs without errors\n\n\nDiagnostics\n3\nResidual plots included, interpreted, violations addressed\n\n\nCode Quality\n3\nClean, commented, reproducible, follows best practices, no errors\n\n\n\nKey: This is where technical reviewers verify your work. All code must run without errors & be reproducible!!."
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#example-presentation-structure",
    "href": "labs/midterm/Midterm_2025.html#example-presentation-structure",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Example Presentation Structure",
    "text": "Example Presentation Structure\nSlide 1: Title - Your team name and teammates - Project title - Date\nSlide 2: The Problem\nSlide 3: Data Sources - Property sales (n = X,XXX, 2023-2024) - Census ACS (income, education, poverty) - OpenDataPhilly (parks, transit, crime)\nSlide 4: Where Are Expensive Homes? - [Map visualization] - Key pattern observed\nSlide 5: What Drives Prices? - [Best scatter plot or faceted visualization] - Key relationship identified\nSlide 6: Model Comparison - [Results table] - “Each layer improves prediction”\nSlide 7: Top Predictors - Neighborhood (biggest impact) - Square footage (β = X) - Transit access (β = Y)\nSlide 8: Model Performance - Final RMSE: 0.26 (log scale) - Translation: ~26% typical error - Beats baseline by 40%\nSlide 9: Hardest to Predict - [Visualization of residuals by neighborhood]\nSlide 10: Recommendations\nSlide 11: Limitations & Next Steps\nSlide 12: Questions? - Thank you - [Contact info]"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#tips-for-success",
    "href": "labs/midterm/Midterm_2025.html#tips-for-success",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart Early\n\nData cleaning always takes longer than expected\nOpenDataPhilly can be slow to download\nLeave time for troubleshooting AND RENDERING!!!\n\n\n\nCheck Your Work\n\nOrganize your file directory from the beginning.\nRun your entire .qmd file from scratch before submitting\nMake sure all visualizations display\nCheck that your narrative flows logically"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#frequently-asked-questions",
    "href": "labs/midterm/Midterm_2025.html#frequently-asked-questions",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Frequently Asked Questions",
    "text": "Frequently Asked Questions\nQ: Do I have to create both slides AND an appendix?\nA: Yes! Slides are your main deliverable (present findings). Appendix proves you did the work correctly.\nQ: Can code appear in my presentation slides?\nA: NO! Slides are for city officials. All code goes in the technical appendix.\nQ: How many slides should I have?\nA: 10-15 maximum. Quality over quantity. Each slide should have a clear purpose.\nQ: Can I use a different city?\nA: No, everyone uses Philadelphia for comparability.\nQ: How do I make my .qmd render as revealjs slides?\nA: Use format: revealjs in your YAML (see template above). Test it early!\nQ: My presentation is 8 minutes. Is that okay?\nA: NO! You must cut it to 5 minutes. Practice and trim ruthlessly.\nQ: Should I include all 5 EDA visualizations in my slides?\nA: No! Slides should have your best 2-3 visualizations. Put all 5 in the appendix.\nQ: My RMSE is 0.35 in log scale. Is that good?\nA: Depends on your data, but 0.25-0.45 is typical for hedonic models. Compare to your baseline or put your data back into dollars!\nQ: Should I remove all outliers?\nA: No! Only remove obvious errors. Use log transformation to handle legitimate outliers.\nQ: What if my code works on my computer but not when I knit?\nA: Start fresh, restart R, knit in a clean session. Check for hard-coded paths.\nQ: Can I use ChatGPT/Claude to write my analysis?\nA: You may use AI for debugging code, but NOT for writing your analysis.\nQ: How formal should my presentation be?\nA: Professional but not stuffy. Like you’re briefing a city council member who’s smart but doesn’t know statistics.\nQ: What happens if I go over 5 minutes?\nA: I’ll politely cut you off and you’ll lose points. Practice with a timer!"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#example-workflow",
    "href": "labs/midterm/Midterm_2025.html#example-workflow",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Example Workflow",
    "text": "Example Workflow\nWeek 1: - Download data - Initial cleaning - Basic EDA\nWeek 2: - Feature engineering - Build 4 models - Run cross-validation - Diagnostics - Write conclusions - Proofread and submit"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#academic-integrity",
    "href": "labs/midterm/Midterm_2025.html#academic-integrity",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nYou may discuss concepts with classmates\nYou may NOT share code or slides\nAll work must be your own (or your teams)\nCite any external resources used\nPlease acknowledge how you used AI in your work and which AI you used (For example, Claude helped me today in coming up with a draft of this assignment, but I edited it thoroughly!)"
  },
  {
    "objectID": "labs/midterm/Midterm_2025.html#final-checklist-before-submitting",
    "href": "labs/midterm/Midterm_2025.html#final-checklist-before-submitting",
    "title": "Midterm Challenge: Philadelphia Housing Price Prediction",
    "section": "Final Checklist Before Submitting",
    "text": "Final Checklist Before Submitting\n\nPresentation Slides\n\nRenders correctly as revealjs slides\n10-15 slides maximum\nNo code visible anywhere\n2-3 polished visualizations\nClear model comparison table\nNo typos or formatting errors\nProfessional theme applied\nTested presentation timing (&lt;5 minutes)\n\n\n\nTechnical Appendix\n\nRenders correctly as HTML document\nAll code visible and commented\nData cleaning fully documented\n5+ EDA visualizations included\nAll features properly created\n4 models with full output\n10-fold CV code runs without errors\nDiagnostic plots included\nSparse categories handled correctly\nTested that it knits without errors\n\n\n\nBoth Files\n\nProper file naming convention\nBoth .qmd source files included\nData sources clearly documented\nNo hard-coded file paths (use relative paths! - this should be reproducible)\nUploaded to Canvas on time"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "href": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#public-sector-context",
    "href": "weekly-notes/week-02-notes.html#public-sector-context",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Public Sector Context",
    "text": "Public Sector Context\n\nHow Governments Use Data\n\nLong history of government data collection and a massive amount of public data has become available over the past couple decades\nA desire for algorithmic efficiency often results in some actions that are not typically considered desireable (i.e. mass firings to save costs)\n\nData Analytics Subjectivity\n\nEvery step involves human choices that are inherently subjective\n\nEx. Removal/ommission, recoding, how results are interpreted\n\nHealthcare Algorithm Bias Example\n\nAn algorithm to identify high-risk patients systematically excluded Black patients\nThe model used healthcare costs as a proxy for need, but Black patients typically incurred lower costs due to “systemic inequities in access” to healthcare\n\ni.e. Black patients were not treated as often or as extensively, resulting in less healthcare costs overall\n\nTherefore, patients who were more likely to seek/afford treatment were prioritized over those who could not\n\nCOMPAS Recidivism Preciction Example\n\nBiased policing patterns producing more Black arrests resulted in an algorithm 2x as likely to falsely flag Black defendants as high risk\n\nDutch Welfare Fraud Detection Example\n\nDisproportionately targeted vulnerable populations, violated EU privacy laws\n\n\nActivity in class\n\nTopic: Graduation Probability\nProxy: GPA\nBlind Spot:\nHarm: High-functioning students might encounter a major health event/family-related issue that prevents graduation. Students doing poorly in classes might have a strong support network and simply graduate with a lower GPA.\nGuardrail: Potential survey of source of support to judge student health and support network."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#census-data-foundations",
    "href": "weekly-notes/week-02-notes.html#census-data-foundations",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Census Data Foundations",
    "text": "Census Data Foundations\n\nCensus mandated in the constitution by James Madison to ensure accurate representation in government\nDecennial Census vs. American Community Survey\n\nDecennial Census → Every 10 years, 9 basic questions (incl. age, race, sex, housing), it’s a constitutional requirement, determines political representation\nACS → 3% of households sampled every year, detailed questions (income, education, employment, housing costs), replaced the old “long form” in 2005\n\n1-year estimates (areas &gt; 65,000 people) have the most current data, smallest sample size\n5-year estimates (all areas) have more reliable data since it is aggregated across multiple years and has a larger sample\nThe ACS has a Margin of Error (MOE) that is important to keep an eye on\n\nLarge MOE’s relative to estimate indicate less reliable values\n\n\n\nCensus Geographic Hierarchy\n\nTract &gt; Block Group &gt; Block\nAt smaller geographies (i.e. blocks) mathematical noise is added to protect privacy\n\n“Even objective data involves subjective choices about privacy vs. accuracy”\n\nCensus boundaries change over time, and historical census data doesn’t always match up with modern boundaries\n\nNHGIS provides historical census data with consistent boundaries over time, good for longitudinal studies"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "href": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Tidycensus Tips",
    "text": "Tidycensus Tips\n\nget_acs\n\nCreating a list of c(var_name = “var_code”) automatically renames the columns in the resulting df"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nI still have lingering technical quirks with Quarto\n\nAreas needing more practice\n\nQuarto"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nWhile algorithms are incredibly useful for streamlining policy-making by creating a standard set of rules and criteria for various forms of decision-making, it’s important to keep in mind that the data used to build these models/algorithms and the ways in which they can be deployed can be inherently biased and lead to discriminatory results."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nDiving more deeply into the terminology of the world of algorithms, as well as learning some more census-related tips that’ll be helpful in the future\n\nHow I’ll apply this knowledge\n\nImproving how I approach the process of model-building and looking more deeply into where sources of potential bias might lie."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "href": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "href": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\nThe Earth is round, maps are flat → we can’t preserve area, distance, and angles simultaneously\nAll projections cause some distortions in the true shape of the Earth\nYou cannot use a projection system that uses lat/lon to calculate area\nThe shape of the Earth is a geoid, an imperfect sphere due to terrain\n\nStep 1: Approximate Earth’s shape with an ellipsoid with a uniform surface\nStep 2: Tie the ellipsoid to the real Earth (create a datum)\nStep 3: Overlay your lat/lon grid\n\nNorth American Datum 1927 (NAD27) → centered on Meades Ranch, Kansas\nNAD83 → earth centered\nWGS84 → also earth centered, but uses a different ellipsoid approximation\n*All of these are Geographic (geodetic) coordinate systems that utilize lat/lon\n\nProjecting a 3D Shape onto a 2D shape\n\nCylindrical Projection → wrapping a rectangular sheet around the ellipsoid with its cylindrical axis parallel to the poles (i.e. “Mercator Projection”)\n\nLine of Tangency refers to where a continuous line on the 3D shape aligns entirely with the x axis of the sheet\n\nTransverse Cylindrical → wrapping the cylinder around the ellipsoid, but now the cylindrical axis is parallel to the equatorial plane\nConic projections → wrapping the sheet around in a cone shape with the line of tangency running through the locations you want to study (reduces distortions)\n\nSADD Acronym → things that can be distorted when projecting\n\nShape, Area, Distance, Direction\n\nProjected Coordinate Systems\n\nUTM (Universal Transverse Mercator) → developed by the military by dividing a Mercator Projection into 6 degree segments (60 total)\n\nTypically denoted by a False Northing and a False Easting\n\nState Plane → each state has its own projection method, highly localized (on a global scale) to minimize distortions\n\nTypically conic\nPA has two, one for northern PA and one for southern PA (both in feet/meters)\n\n\n\nFor quiz next week: questions on the “Path of Despair” vs. the “Path of Happiness” when it comes to projecting layers to coordinate systems in R (i.e. use st_crs always unless the layer just does not have a coordinate system)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Data that varies spatially can be improved by taking location into account\n\nTaking proximity to downtown, nearby school quality, neighborhood characteristics, etc.\nHelps control for neighborhood elements (fixed effects)\n\nConverting csv files to spatial datasets\n\nThe difference between the geographic coordinate systems WGS84 and NAD83 is minimal\nUsing WGS84 as the default is usually acceptable for planning analysis\nFor engineering, you should know EXACTLY which coordinate system it was coded to\n\nClassed maps in R are typically easier to interpret than continuous (default in R)\n\nCheck lecture code for examples on how to make one"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#improving-models-with-spatial-features",
    "href": "weekly-notes/week-06-notes.html#improving-models-with-spatial-features",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Data that varies spatially can be improved by taking location into account\n\nTaking proximity to downtown, nearby school quality, neighborhood characteristics, etc.\nHelps control for neighborhood elements (fixed effects)\n\nConverting csv files to spatial datasets\n\nThe difference between the geographic coordinate systems WGS84 and NAD83 is minimal\nUsing WGS84 as the default is usually acceptable for planning analysis\nFor engineering, you should know EXACTLY which coordinate system it was coded to\n\nClassed maps in R are typically easier to interpret than continuous (default in R)\n\nCheck lecture code for examples on how to make one"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#categorical-variables",
    "href": "weekly-notes/week-06-notes.html#categorical-variables",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nOne is chosen as the reference category, all other classes are compared to the reference category\n\nIn R, the first alphabetical category is chosen by default\nThis can be changed by setting the levels of the column\n\nExamples: neighborhood, neighborhood income category, housing quality category, etc."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#interaction-terms",
    "href": "weekly-notes/week-06-notes.html#interaction-terms",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nEx. square footage makes housing prices go up faster in wealthier neighborhoods than in lower income neighborhoods (different slopes of price vs. sq footage)\nInteraction terms between continuous and categorical variables allow you to control for this difference.\nPolicy Implications\n\nDifferent market segments produce differing effects\nInteraction terms amplify inequalities (large homes in wealthy areas are priced much higher while lower income areas have their home prices )"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#non-linear-modeling",
    "href": "weekly-notes/week-06-notes.html#non-linear-modeling",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Non-Linear Modeling",
    "text": "Non-Linear Modeling\n\nPolynomial regression\n\nWhen straight lines don’t fit your data\n\nCurved residual plots, diminishing returns, accelerated effects, etc.\nEx. House age → depreciation, then vintage value"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#creating-spatial-features",
    "href": "weekly-notes/week-06-notes.html#creating-spatial-features",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Creating Spatial Features",
    "text": "Creating Spatial Features\n\nTobler’s First Law of Geography\n\n“Everything is related to everything else, but near things are more related than distant things”\n\nDistance-based factors matter more the closer you are to them\nTypes of Spatial Variables\n\nBuffer Aggregation → count or sum of events within a defined distance\nk-Nearest Neighbors (kNN) → average distance to k closest events\nDistance to Specific Points → straight-line distance to important locations\n\nFixed Effects\n\nCategorical variables that capture all unmeasured characteristics of a group\nHelpful for situations where you cannot easily capture every aspect of a place’s characteristics with defined variables\nOften, we’ll see a large jump in R2 value when introducing fixed effects"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#cross-validation",
    "href": "weekly-notes/week-06-notes.html#cross-validation",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nDifferent types:\n\nTrain/Test Splite → 80/20 split, simple but unstable method\nk-Fold Cross-Validation → split into k folds, train on k-1, test on 1, repeat across all “folds” allowing each to have a chance at being a training and testing dataset\nLOOCV → leave one observation out at a time (special case of k-fold)\n\nk-Fold CV helps tell us how well models predict NEW data, is more honest than in-sample R2, and helps detect overfitting\n\nCategorical variables require that there’s enough data in each category so that samples always include all examples of the categorical variable\nThis makes k-Fold CV useful typically only in large datasets"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#why-improve-the-model",
    "href": "assignments/midterm/midterm_presentation.html#why-improve-the-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Why Improve the Model?",
    "text": "Why Improve the Model?\nResearch Question\nIdentify which structural, spatial, and socio-economic predictors contribute to a more accurate Automated Valuation Model for the City of Philadelphia.\nMotivation\nImproving the accuracy of residential property tax assessment can mitigate inequity assessment methods, increase transparency in governmental processes, and make analysis more reliable and efficient"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#sale-prices-in-philadelphia",
    "href": "assignments/midterm/midterm_presentation.html#sale-prices-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Sale Prices in Philadelphia",
    "text": "Sale Prices in Philadelphia\nHigher sale prices are concentrated in Central and Northwest Philadelphia"
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#model-performance-by-neighborhood-part-1",
    "href": "assignments/midterm/midterm_presentation.html#model-performance-by-neighborhood-part-1",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 1)",
    "text": "Model Performance by Neighborhood (Part 1)\n\nNeighborhoods in North Philadelphia and West Philadelphia (with the exception of University City), in addition to Chinatown are underpredicted.\nCenter City and parts of Northeast and Northwest Philadelphia are overpredicted."
  },
  {
    "objectID": "assignments/midterm/midterm_presentation.html#model-performance-by-neighborhood-part-2",
    "href": "assignments/midterm/midterm_presentation.html#model-performance-by-neighborhood-part-2",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 2)",
    "text": "Model Performance by Neighborhood (Part 2)\n\nThis suggests local factors not included in the model are having an impact on sale prices.\nA particular area of concern is the underpredicting of sale prices in lower-income neighborhoods in North Philadelphia."
  }
]