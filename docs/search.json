[
  {
    "objectID": "weekly-notes/week-11-notes.html",
    "href": "weekly-notes/week-11-notes.html",
    "title": "Week 11 Notes - Space-Time Prediction",
    "section": "",
    "text": "Cross-Sectional Data → “snapshot” data that is for a specific point in time\nPanel Data → same thing observed over time\nTemporal Features → what happened last hour?\nSpace-Time Interaction → different patterns observable by location and time\nVariant observations change with each row (hour), invariant data does not (day of the week)\nBinning data helps us aggregate data into easier-to-process groups\n\nThe smaller the bin, however, the more sparse (more zeros) the data will become\n\n\n\nTemporal Lags\n\nTemporal Lag → If past data is available, you can use demand trends to predict future demand\n\nDisplace the demand in time by a certain amount (1hr, 12hr, 1day, etc.) and you get a picture of different trends\n\nShort-Term Persistence (smooth demand changes)\nMedium-Term Trends (morning rush building up steam)\nHalf-Day cycle (AM vs. PM patterns)\nDaily Periodicity (conditions at the same time yesterday)\n\n\nTesting/Training datasets will be temporal in nature\n\nTrain on an earlier half of the dataset, test on the later half\n\nNote for Hwk: attempt to use a poisson regression instead of the linear example since we’re dealing with counts of trips"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html",
    "href": "weekly-notes/week-08-notes.html",
    "title": "Week 8 Notes - Predictive Policing",
    "section": "",
    "text": "Poisson Regression\n\nUsed speciifically for data where the model is mostly counts\nPredicting crime through items of disorder (cashed can, loud neighhors, etc.)\n\n\n\nSpatial Weights Matrix:\n\nLocal Mora’s I Forumla: instead of getting a normalized values, local Moran’s I creates a value for each other\nLinear Regression is not good for count variables (1, 2, 3, 4, etc.) since it\nIn a Poisson Distribution, your data’s mean always equals the distribution’s\nCoefficients from a Poisson regression are on a log scale, requiring exponential conversion to interpret\nUnlike OLS, we don’t use residual plots the same way (check slides)\nlink(“log”) is a way for the Poisson and similar non-linear distributions to… Spatial ### Cross-Validation\nLeave out entire spatial groups that, by their nature, influence each other (and therefore will influence the result of k-fold CV) rather than leaving out individual data points within those spatial groups.\n\n\n\nLISA (Local Indicators of Spatial Association)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Data that varies spatially can be improved by taking location into account\n\nTaking proximity to downtown, nearby school quality, neighborhood characteristics, etc.\nHelps control for neighborhood elements (fixed effects)\n\nConverting csv files to spatial datasets\n\nThe difference between the geographic coordinate systems WGS84 and NAD83 is minimal\nUsing WGS84 as the default is usually acceptable for planning analysis\nFor engineering, you should know EXACTLY which coordinate system it was coded to\n\nClassed maps in R are typically easier to interpret than continuous (default in R)\n\nCheck lecture code for examples on how to make one"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#improving-models-with-spatial-features",
    "href": "weekly-notes/week-06-notes.html#improving-models-with-spatial-features",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "",
    "text": "Data that varies spatially can be improved by taking location into account\n\nTaking proximity to downtown, nearby school quality, neighborhood characteristics, etc.\nHelps control for neighborhood elements (fixed effects)\n\nConverting csv files to spatial datasets\n\nThe difference between the geographic coordinate systems WGS84 and NAD83 is minimal\nUsing WGS84 as the default is usually acceptable for planning analysis\nFor engineering, you should know EXACTLY which coordinate system it was coded to\n\nClassed maps in R are typically easier to interpret than continuous (default in R)\n\nCheck lecture code for examples on how to make one"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#categorical-variables",
    "href": "weekly-notes/week-06-notes.html#categorical-variables",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nOne is chosen as the reference category, all other classes are compared to the reference category\n\nIn R, the first alphabetical category is chosen by default\nThis can be changed by setting the levels of the column\n\nExamples: neighborhood, neighborhood income category, housing quality category, etc."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#interaction-terms",
    "href": "weekly-notes/week-06-notes.html#interaction-terms",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Interaction Terms",
    "text": "Interaction Terms\n\nEx. square footage makes housing prices go up faster in wealthier neighborhoods than in lower income neighborhoods (different slopes of price vs. sq footage)\nInteraction terms between continuous and categorical variables allow you to control for this difference.\nPolicy Implications\n\nDifferent market segments produce differing effects\nInteraction terms amplify inequalities (large homes in wealthy areas are priced much higher while lower income areas have their home prices )"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#non-linear-modeling",
    "href": "weekly-notes/week-06-notes.html#non-linear-modeling",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Non-Linear Modeling",
    "text": "Non-Linear Modeling\n\nPolynomial regression\n\nWhen straight lines don’t fit your data\n\nCurved residual plots, diminishing returns, accelerated effects, etc.\nEx. House age → depreciation, then vintage value"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#creating-spatial-features",
    "href": "weekly-notes/week-06-notes.html#creating-spatial-features",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Creating Spatial Features",
    "text": "Creating Spatial Features\n\nTobler’s First Law of Geography\n\n“Everything is related to everything else, but near things are more related than distant things”\n\nDistance-based factors matter more the closer you are to them\nTypes of Spatial Variables\n\nBuffer Aggregation → count or sum of events within a defined distance\nk-Nearest Neighbors (kNN) → average distance to k closest events\nDistance to Specific Points → straight-line distance to important locations\n\nFixed Effects\n\nCategorical variables that capture all unmeasured characteristics of a group\nHelpful for situations where you cannot easily capture every aspect of a place’s characteristics with defined variables\nOften, we’ll see a large jump in R2 value when introducing fixed effects"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#cross-validation",
    "href": "weekly-notes/week-06-notes.html#cross-validation",
    "title": "Week 6 Notes - Spatial Machine Learning & Advanced Regression",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nDifferent types:\n\nTrain/Test Splite → 80/20 split, simple but unstable method\nk-Fold Cross-Validation → split into k folds, train on k-1, test on 1, repeat across all “folds” allowing each to have a chance at being a training and testing dataset\nLOOCV → leave one observation out at a time (special case of k-fold)\n\nk-Fold CV helps tell us how well models predict NEW data, is more honest than in-sample R2, and helps detect overfitting\n\nCategorical variables require that there’s enough data in each category so that samples always include all examples of the categorical variable\nThis makes k-Fold CV useful typically only in large datasets"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "href": "weekly-notes/week-04-notes.html#spatial-data-fundamentals",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "",
    "text": "Shapefile structure: three primary files (.shp = geometry, .dbf = tabular data, .shx = )\nOn some open data portals, there is a way to directly import spatial layers into R (if it’s an ArcGIS Online site) by going to the bottom “I want to use this” and using the API call."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "href": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "title": "Week 4 Notes - Spatial Analysis in R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\nThe Earth is round, maps are flat → we can’t preserve area, distance, and angles simultaneously\nAll projections cause some distortions in the true shape of the Earth\nYou cannot use a projection system that uses lat/lon to calculate area\nThe shape of the Earth is a geoid, an imperfect sphere due to terrain\n\nStep 1: Approximate Earth’s shape with an ellipsoid with a uniform surface\nStep 2: Tie the ellipsoid to the real Earth (create a datum)\nStep 3: Overlay your lat/lon grid\n\nNorth American Datum 1927 (NAD27) → centered on Meades Ranch, Kansas\nNAD83 → earth centered\nWGS84 → also earth centered, but uses a different ellipsoid approximation\n*All of these are Geographic (geodetic) coordinate systems that utilize lat/lon\n\nProjecting a 3D Shape onto a 2D shape\n\nCylindrical Projection → wrapping a rectangular sheet around the ellipsoid with its cylindrical axis parallel to the poles (i.e. “Mercator Projection”)\n\nLine of Tangency refers to where a continuous line on the 3D shape aligns entirely with the x axis of the sheet\n\nTransverse Cylindrical → wrapping the cylinder around the ellipsoid, but now the cylindrical axis is parallel to the equatorial plane\nConic projections → wrapping the sheet around in a cone shape with the line of tangency running through the locations you want to study (reduces distortions)\n\nSADD Acronym → things that can be distorted when projecting\n\nShape, Area, Distance, Direction\n\nProjected Coordinate Systems\n\nUTM (Universal Transverse Mercator) → developed by the military by dividing a Mercator Projection into 6 degree segments (60 total)\n\nTypically denoted by a False Northing and a False Easting\n\nState Plane → each state has its own projection method, highly localized (on a global scale) to minimize distortions\n\nTypically conic\nPA has two, one for northern PA and one for southern PA (both in feet/meters)\n\n\n\nFor quiz next week: questions on the “Path of Despair” vs. the “Path of Happiness” when it comes to projecting layers to coordinate systems in R (i.e. use st_crs always unless the layer just does not have a coordinate system)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "href": "weekly-notes/week-02-notes.html#algorithms-in-public-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "",
    "text": "What are Algorithms?\n\nSimply, a set of instructions. Typically involve rules and criteria that need to be met to achieve certain outcomes.\nEx. cooking recipes, directions, decision trees, computer programs that process data to make predictions\nAlgorithms (in theory) assist in overcoming the bias of human decision-makers by creating a set rule set\n\nTerminology\n\nInputs → functions, predictors, independent variables, x, etc.\nOutputs → labels, outcomes, dependent variables, y, etc.\nData Science → computer science/engineering focus on algorithms and methods\nData Analytics → application of data science methods to other disciplines\nMachine Learning → algorithms for classification and prediction that learn from data\n\nReal-world examples\n\nCriminal Justice → recidivism risk scores for bail and sentencing decisions\nHousing & Finance → mortgage lending and tenant screening algorithms\nHealthcare → patient care prioritization and resource allocation\n\nIssues with algorithms\n\nInherent bias in the data inputs and how models were created around that biased data"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#public-sector-context",
    "href": "weekly-notes/week-02-notes.html#public-sector-context",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Public Sector Context",
    "text": "Public Sector Context\n\nHow Governments Use Data\n\nLong history of government data collection and a massive amount of public data has become available over the past couple decades\nA desire for algorithmic efficiency often results in some actions that are not typically considered desireable (i.e. mass firings to save costs)\n\nData Analytics Subjectivity\n\nEvery step involves human choices that are inherently subjective\n\nEx. Removal/ommission, recoding, how results are interpreted\n\nHealthcare Algorithm Bias Example\n\nAn algorithm to identify high-risk patients systematically excluded Black patients\nThe model used healthcare costs as a proxy for need, but Black patients typically incurred lower costs due to “systemic inequities in access” to healthcare\n\ni.e. Black patients were not treated as often or as extensively, resulting in less healthcare costs overall\n\nTherefore, patients who were more likely to seek/afford treatment were prioritized over those who could not\n\nCOMPAS Recidivism Preciction Example\n\nBiased policing patterns producing more Black arrests resulted in an algorithm 2x as likely to falsely flag Black defendants as high risk\n\nDutch Welfare Fraud Detection Example\n\nDisproportionately targeted vulnerable populations, violated EU privacy laws\n\n\nActivity in class\n\nTopic: Graduation Probability\nProxy: GPA\nBlind Spot:\nHarm: High-functioning students might encounter a major health event/family-related issue that prevents graduation. Students doing poorly in classes might have a strong support network and simply graduate with a lower GPA.\nGuardrail: Potential survey of source of support to judge student health and support network."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#census-data-foundations",
    "href": "weekly-notes/week-02-notes.html#census-data-foundations",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Census Data Foundations",
    "text": "Census Data Foundations\n\nCensus mandated in the constitution by James Madison to ensure accurate representation in government\nDecennial Census vs. American Community Survey\n\nDecennial Census → Every 10 years, 9 basic questions (incl. age, race, sex, housing), it’s a constitutional requirement, determines political representation\nACS → 3% of households sampled every year, detailed questions (income, education, employment, housing costs), replaced the old “long form” in 2005\n\n1-year estimates (areas &gt; 65,000 people) have the most current data, smallest sample size\n5-year estimates (all areas) have more reliable data since it is aggregated across multiple years and has a larger sample\nThe ACS has a Margin of Error (MOE) that is important to keep an eye on\n\nLarge MOE’s relative to estimate indicate less reliable values\n\n\n\nCensus Geographic Hierarchy\n\nTract &gt; Block Group &gt; Block\nAt smaller geographies (i.e. blocks) mathematical noise is added to protect privacy\n\n“Even objective data involves subjective choices about privacy vs. accuracy”\n\nCensus boundaries change over time, and historical census data doesn’t always match up with modern boundaries\n\nNHGIS provides historical census data with consistent boundaries over time, good for longitudinal studies"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "href": "weekly-notes/week-02-notes.html#tidycensus-tips",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Tidycensus Tips",
    "text": "Tidycensus Tips\n\nget_acs\n\nCreating a list of c(var_name = “var_code”) automatically renames the columns in the resulting df"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nI still have lingering technical quirks with Quarto\n\nAreas needing more practice\n\nQuarto"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nWhile algorithms are incredibly useful for streamlining policy-making by creating a standard set of rules and criteria for various forms of decision-making, it’s important to keep in mind that the data used to build these models/algorithms and the ways in which they can be deployed can be inherently biased and lead to discriminatory results."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Intro to Algorithms",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nDiving more deeply into the terminology of the world of algorithms, as well as learning some more census-related tips that’ll be helpful in the future\n\nHow I’ll apply this knowledge\n\nImproving how I approach the process of model-building and looking more deeply into where sources of potential bias might lie."
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Name: Henry Sywulak-Herr Date: 11-10-2025\n\n\nPatient ID: 024\nTruth (Red dot?): [ ] YES (has disease) [x] NO (no disease)\nModel Prediction (Probability): 0.10\nDeadly Variant? (Star): [ ] YES [x] NO\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.50 → QUARANTINE\nWhere should you go? [ ] Quarantine Table\n[x] Stay at regular table\n\n\n\nBased on where you went and your truth:\n[ ] True Positive (TP) - Correctly quarantined (have disease, high prob)\n[ ] False Positive (FP) - Incorrectly quarantined (no disease, high prob)\n[x] True Negative (TN) - Correctly not quarantined (no disease, low prob)\n[ ] False Negative (FN) - Incorrectly not quarantined (have disease, low prob)\n\n\n\nWork with your table to count:\n           Predicted Positive   Predicted Negative\n           (Quarantined)       (Not Quarantined)\n\nActually \nPositive   TP = 12         FN = 7\n(Red Dot)\n\nActually\nNegative   FP = 5         TN = 13\n(No Dot)\n\n\n\nSensitivity (True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{12}{12 + 7} = 0.632\\]\nInterpretation: What % of actually sick people did we catch → 63.2%\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{13}{13 + 5} = 0.722\\]\nInterpretation: What % of healthy people did we correctly not quarantine → 72.2%\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{12}{12 + 5} = 0.706\\]\nInterpretation: Of those quarantined, what % actually had disease → 70.6%\nAccuracy: \\[\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}} = \\frac{12 + 13}{37} = 0.676\\]\nInterpretation: What % did we classify correctly overall → 67.6%\n\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.30 → QUARANTINE\nWhere should you go? [ ] Quarantine Table [x] Stay at regular table\nDid your classification change from Round 1? [ ] YES [ ] NO\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = 19         FN = 1\n\nActually\nNegative   FP = 10         TN = 7\n\n\n\nSensitivity: 0.95 (Did this go up ↑ or down ↓ from Round 1?) went way up\nSpecificity: 0.41 (Did this go up ↑ or down ↓ from Round 1?) went way down\nPrecision: 0.66 went slightly down\nAccuracy: 0.70 went slightly up\n\n\n\nHow many people with stars (deadly variant) were caught? - Round 1 (threshold 0.50): _______ - Round 2 (threshold 0.30): _______\nDid lowering the threshold help catch deadly variants? ⃝ YES ⃝ NO\n\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.70 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______\nSpecificity: _______\nPrecision: _______\nAccuracy: _______\n\n\n\n\n\nFill in the table below with your results:\n\n\n\n\n\n\n\n\n\nMetric\nThreshold = 0.30\nThreshold = 0.50\nThreshold = 0.70\n\n\n\n\nSensitivity\n\n\n\n\n\nSpecificity\n\n\n\n\n\nPrecision\n\n\n\n\n\nFalse Positives (FP)\n\n\n\n\n\nFalse Negatives (FN)\n\n\n\n\n\n\n\n\n\nAs threshold increases (0.30 → 0.50 → 0.70), what happens to sensitivity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nAs threshold increases, what happens to specificity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nCan we maximize BOTH sensitivity and specificity at the same time?\n⃝ YES ⃝ NO\nWhat is the fundamental trade-off?\n\n\n\n\n\n\n\n\n\n\nIf you were a False Positive (quarantined when healthy):\nHow did it feel to be quarantined unnecessarily?\n\n\nWhat if this meant missing work for 2 weeks without pay? Or being unable to attend an important event?\n\n\nIf you were a False Negative (missed when actually sick):\nHow did it feel to be sent back when you actually had the disease?\n\n\nWhat if you had the deadly variant (star)? What are the consequences of being missed?\n\n\n\n\n\nScenario A: Rare, extremely deadly disease (like Ebola) - Disease is rare but 70% fatality rate - Treatment is available and effective if caught early - Quarantine costs $1,000/person, treatment costs $5,000\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\nScenario B: Common, mild illness (like common cold) - Disease is common but not serious (just annoying) - No treatment available - Quarantine means missing work (costs $2,000 in lost wages)\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\n\n\n\nWhich metric is MOST important for Scenario A (deadly disease)?\n⃝ Sensitivity - Don’t miss any sick people\n⃝ Specificity - Don’t quarantine healthy people\n⃝ Accuracy - Overall correct\n⃝ Precision - When we say “sick,” be sure\nWhy?\n\n\nWhich metric is MOST important for Scenario B (mild illness)?\n⃝ Sensitivity\n⃝ Specificity\n⃝ Accuracy\n⃝ Precision\nWhy?\n\n\n\n\n\nThink about COVID-19 testing. Rapid tests had lower sensitivity than PCR tests.\nWhen might you prefer a rapid test (lower sensitivity)?\n\n\nWhen would you insist on PCR test (higher sensitivity)?\n\n\n\n\n\nWhat if the model’s predictions were systematically wrong for certain groups?\nFor example: The model gives lower probabilities for women even when they have the disease.\nWhat would happen?\n\n\nHow could we detect this problem?\n\n\nWhat should we do about it?\n\n\n\n\n\n\n\nWrite 3 key things you learned from this exercise:\n\n\n\n\n\n\n\n\nOne question you still have:"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#your-patient-information",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#your-patient-information",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Patient ID: 024\nTruth (Red dot?): [ ] YES (has disease) [x] NO (no disease)\nModel Prediction (Probability): 0.10\nDeadly Variant? (Star): [ ] YES [x] NO"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#round-1-threshold-0.50",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#round-1-threshold-0.50",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.50 → QUARANTINE\nWhere should you go? [ ] Quarantine Table\n[x] Stay at regular table\n\n\n\nBased on where you went and your truth:\n[ ] True Positive (TP) - Correctly quarantined (have disease, high prob)\n[ ] False Positive (FP) - Incorrectly quarantined (no disease, high prob)\n[x] True Negative (TN) - Correctly not quarantined (no disease, low prob)\n[ ] False Negative (FN) - Incorrectly not quarantined (have disease, low prob)\n\n\n\nWork with your table to count:\n           Predicted Positive   Predicted Negative\n           (Quarantined)       (Not Quarantined)\n\nActually \nPositive   TP = 12         FN = 7\n(Red Dot)\n\nActually\nNegative   FP = 5         TN = 13\n(No Dot)\n\n\n\nSensitivity (True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{12}{12 + 7} = 0.632\\]\nInterpretation: What % of actually sick people did we catch → 63.2%\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{13}{13 + 5} = 0.722\\]\nInterpretation: What % of healthy people did we correctly not quarantine → 72.2%\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{12}{12 + 5} = 0.706\\]\nInterpretation: Of those quarantined, what % actually had disease → 70.6%\nAccuracy: \\[\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}} = \\frac{12 + 13}{37} = 0.676\\]\nInterpretation: What % did we classify correctly overall → 67.6%"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#round-2-threshold-0.30",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#round-2-threshold-0.30",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.30 → QUARANTINE\nWhere should you go? [ ] Quarantine Table [x] Stay at regular table\nDid your classification change from Round 1? [ ] YES [ ] NO\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = 19         FN = 1\n\nActually\nNegative   FP = 10         TN = 7\n\n\n\nSensitivity: 0.95 (Did this go up ↑ or down ↓ from Round 1?) went way up\nSpecificity: 0.41 (Did this go up ↑ or down ↓ from Round 1?) went way down\nPrecision: 0.66 went slightly down\nAccuracy: 0.70 went slightly up\n\n\n\nHow many people with stars (deadly variant) were caught? - Round 1 (threshold 0.50): _______ - Round 2 (threshold 0.30): _______\nDid lowering the threshold help catch deadly variants? ⃝ YES ⃝ NO"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#round-3-threshold-0.70",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#round-3-threshold-0.70",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.70 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______\nSpecificity: _______\nPrecision: _______\nAccuracy: _______"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#comparison-across-thresholds",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#comparison-across-thresholds",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Fill in the table below with your results:\n\n\n\n\n\n\n\n\n\nMetric\nThreshold = 0.30\nThreshold = 0.50\nThreshold = 0.70\n\n\n\n\nSensitivity\n\n\n\n\n\nSpecificity\n\n\n\n\n\nPrecision\n\n\n\n\n\nFalse Positives (FP)\n\n\n\n\n\nFalse Negatives (FN)\n\n\n\n\n\n\n\n\n\nAs threshold increases (0.30 → 0.50 → 0.70), what happens to sensitivity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nAs threshold increases, what happens to specificity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nCan we maximize BOTH sensitivity and specificity at the same time?\n⃝ YES ⃝ NO\nWhat is the fundamental trade-off?"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#reflection-questions",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#reflection-questions",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "If you were a False Positive (quarantined when healthy):\nHow did it feel to be quarantined unnecessarily?\n\n\nWhat if this meant missing work for 2 weeks without pay? Or being unable to attend an important event?\n\n\nIf you were a False Negative (missed when actually sick):\nHow did it feel to be sent back when you actually had the disease?\n\n\nWhat if you had the deadly variant (star)? What are the consequences of being missed?\n\n\n\n\n\nScenario A: Rare, extremely deadly disease (like Ebola) - Disease is rare but 70% fatality rate - Treatment is available and effective if caught early - Quarantine costs $1,000/person, treatment costs $5,000\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\nScenario B: Common, mild illness (like common cold) - Disease is common but not serious (just annoying) - No treatment available - Quarantine means missing work (costs $2,000 in lost wages)\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\n\n\n\nWhich metric is MOST important for Scenario A (deadly disease)?\n⃝ Sensitivity - Don’t miss any sick people\n⃝ Specificity - Don’t quarantine healthy people\n⃝ Accuracy - Overall correct\n⃝ Precision - When we say “sick,” be sure\nWhy?\n\n\nWhich metric is MOST important for Scenario B (mild illness)?\n⃝ Sensitivity\n⃝ Specificity\n⃝ Accuracy\n⃝ Precision\nWhy?\n\n\n\n\n\nThink about COVID-19 testing. Rapid tests had lower sensitivity than PCR tests.\nWhen might you prefer a rapid test (lower sensitivity)?\n\n\nWhen would you insist on PCR test (higher sensitivity)?\n\n\n\n\n\nWhat if the model’s predictions were systematically wrong for certain groups?\nFor example: The model gives lower probabilities for women even when they have the disease.\nWhat would happen?\n\n\nHow could we detect this problem?\n\n\nWhat should we do about it?"
  },
  {
    "objectID": "labs/lab_week10/student_worksheet_classification_exercise.html#key-takeaways",
    "href": "labs/lab_week10/student_worksheet_classification_exercise.html#key-takeaways",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Write 3 key things you learned from this exercise:\n\n\n\n\n\n\n\n\nOne question you still have:"
  },
  {
    "objectID": "labs/lab0/lab0_template.html",
    "href": "labs/lab0/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7 - Variable types: Character (chr), Double (dbl) - Problematic names: The columns that have spaces in their names require quotes around them while scripting"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\ncar_df %&gt;% head(10)\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: In Markdown, the identifier icon in the top left changes from “A tibble: 50,000 x 7” to “Description: df [50,000 x 7]” for tibbles and dfs, respectively. When rendering the website, printing a data frame prints all rows and does not allow for scrolling across columns. Instead, it prints extra columns in another row."
  },
  {
    "objectID": "labs/lab0/lab0_template.html#selecting-columns",
    "href": "labs/lab0/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nselect(.data = car_data, Model, Mileage)\n\n# A tibble: 50,000 × 2\n   Model      Mileage\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Fiesta      127300\n 2 718 Cayman   57850\n 3 Mondeo       39190\n 4 RAV4        210814\n 5 Polo        127869\n 6 Focus        33603\n 7 Mondeo       86686\n 8 Prius        30663\n 9 Polo         73470\n10 Focus       262514\n# ℹ 49,990 more rows\n\n# Select Manufacturer, Price, and Fuel type\ncar_data %&gt;% select(Manufacturer, Price, `Fuel type`)\n\n# A tibble: 50,000 × 3\n   Manufacturer Price `Fuel type`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n 1 Ford          3074 Petrol     \n 2 Porsche      49704 Petrol     \n 3 Ford         24072 Diesel     \n 4 Toyota        1705 Hybrid     \n 5 VW            4101 Petrol     \n 6 Ford         29204 Petrol     \n 7 Ford         14350 Diesel     \n 8 Toyota       30297 Hybrid     \n 9 VW            9977 Petrol     \n10 Ford          1049 Diesel     \n# ℹ 49,990 more rows\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data %&gt;% select(-`Engine size`)\n\n# A tibble: 50,000 × 6\n   Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta     Petrol                       2002  127300  3074\n 2 Porsche      718 Cayman Petrol                       2016   57850 49704\n 3 Ford         Mondeo     Diesel                       2014   39190 24072\n 4 Toyota       RAV4       Hybrid                       1988  210814  1705\n 5 VW           Polo       Petrol                       2006  127869  4101\n 6 Ford         Focus      Petrol                       2018   33603 29204\n 7 Ford         Mondeo     Diesel                       2010   86686 14350\n 8 Toyota       Prius      Hybrid                       2015   30663 30297\n 9 VW           Polo       Petrol                       2012   73470  9977\n10 Ford         Focus      Diesel                       1992  262514  1049\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#renaming-columns",
    "href": "labs/lab0/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;% \n  rename(year = `Year of manufacture`)\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: year no longer has spaces"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;%\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\ncar_data %&gt;% select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorize-cars",
    "href": "labs/lab0/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is luxury (use case_when)\ncar_data &lt;- car_data %&gt;% \n  mutate(price_category = case_when(Price &lt; 15000 ~ \"budget\",\n                                     Price &gt;= 15000 & Price &lt; 30000 ~ \"midrange\",\n                                     .default = \"luxury\"))\n\n# Check your categories select the new column and show it\ncar_data %&gt;% select(Price, price_category)\n\n# A tibble: 50,000 × 2\n   Price price_category\n   &lt;dbl&gt; &lt;chr&gt;         \n 1  3074 budget        \n 2 49704 luxury        \n 3 24072 midrange      \n 4  1705 budget        \n 5  4101 budget        \n 6 29204 midrange      \n 7 14350 budget        \n 8 30297 luxury        \n 9  9977 budget        \n10  1049 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-filtering",
    "href": "labs/lab0/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;% filter(Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;% filter(Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter(price_category == \"luxury\" & Mileage &lt; 30000)\n\n# A tibble: 3,257 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,247 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Ford OR Porsche\ncar_data %&gt;% filter(Manufacturer %in% c(\"Ford\", \"Porsche\"))\n\n# A tibble: 17,568 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta               1   Petrol       2002  127300  3074    23\n 2 Porsche      718 Cayman           4   Petrol       2016   57850 49704     9\n 3 Ford         Mondeo               1.6 Diesel       2014   39190 24072    11\n 4 Ford         Focus                1.4 Petrol       2018   33603 29204     7\n 5 Ford         Mondeo               1.8 Diesel       2010   86686 14350    15\n 6 Ford         Focus                2   Diesel       1992  262514  1049    33\n 7 Ford         Mondeo               1.6 Diesel       1996   77584  5667    29\n 8 Porsche      911                  2.6 Petrol       2009   66273 41963    16\n 9 Porsche      911                  3.5 Petrol       2005  151556 19747    20\n10 Ford         Focus                1   Hybrid       2010   85131 12472    15\n# ℹ 17,558 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\ncar_data %&gt;% filter(Price &gt; 20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter(`Fuel type` == \"Diesel\" & age &lt; 10)\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2,040 vehicles"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#basic-summaries",
    "href": "labs/lab0/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel &lt;- car_data %&gt;% \n  group_by(`Fuel type`) %&gt;% \n  summarize(avg_mileage = mean(Mileage, na.rm = T))\n\navg_mileage_by_fuel\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncount_by_manufacturer &lt;- car_data %&gt;% \n  group_by(Manufacturer) %&gt;% \n  summarize(count = n())\n\ncount_by_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\nprice_cat_freq_table &lt;- car_data %&gt;%\n  group_by(price_category) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(freq = count / sum(count))\n\nprice_cat_freq_table\n\n# A tibble: 3 × 3\n  price_category count  freq\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;\n1 budget         34040 0.681\n2 luxury          6179 0.124\n3 midrange        9781 0.196"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)\n\n\n\n\n\nEmail: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry Sywulak-Herr\nBackground: Second-Year City Planning student concentrating in STIP. Philly (suburbs) native! Went to Penn for my Undergrad (BA Environmental Science, Minor in Chemistry).\nWhy I’m Taking This Course: I want to solidify the modeling background I already have, learn more about data visualization, and how these skills can be applied to a broad range of fields - but primarily transportation :)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [hssherr@upenn.edu]\nGitHub: [@hssherr]"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "",
    "text": "This technical appendix documents the full workflow used to engineer and visualize spatial features for predicting residential housing prices in Philadelphia.\nCode\noptions(scipen = 999)\n\n# Packages\nif(!require(pacman)){install.packages(\"pacman\"); library(pacman, quietly = T)}\n\n\nLoading required package: pacman\n\n\nWarning: package 'pacman' was built under R version 4.4.3\n\n\nCode\np_load(knitr, sf, tidyverse, tidycensus, tigris, here, dplyr, FNN, ggplot2, scales, patchwork, caret, Hmisc, stargazer)\n\n# Files\nsf_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE)\nnhoods &lt;- st_read(\"./data/philadelphia-neighborhoods.geojson\", quiet = TRUE)"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.1 Data Preparation",
    "text": "1.1 Data Preparation\nWe apply several filters to the property data to for quality and relevance. First, we restrict our analysis to residential properties sold between 2023 and 2024, excluding any other property categories. Second, we remove properties with sale prices below $10, as these are abnormal prices for residential properties.\nTo work with Github file size limits, the data is further trimmed of irrelevant columns.\n\n\nCode\n# Restrict to residential only\nresidential_categories &lt;- c(\n  \"APARTMENTS &gt; 4 UNITS\",\n  \"MULTI FAMILY\",\n  \"SINGLE FAMILY\",\n  \"MIXED USE\"\n)\nresidential_data &lt;- sf_data %&gt;%\n  filter(category_code_description %in% residential_categories,\n         year(sale_date) %in% c(2023, 2024),\n         mailing_city_state == \"PHILADELPHIA PA\",\n         sale_price &gt; 10\n         )\n\ntable(residential_data$category_code_description)\n\n# Making sure the file saved to the repo is the trimmed data (to stay below GitHub data limits)\nst_write(residential_data, \"./data/OPA_data.geojson\", driver = \"GeoJSON\", delete_dsn = TRUE, quiet = TRUE)\nfile.exists(\"./data/OPA_data.geojson\")\nOPA_raw &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE) %&gt;% \n  st_transform(2272)\n\n# OPA_data -&gt; cutting mostly NA columns or irrelevant columns for this model.\nOPA_raw &lt;- OPA_raw %&gt;%\n  select(-c(\n  cross_reference, date_exterior_condition, exempt_land, fireplaces, fuel, garage_type, house_extension, mailing_address_2, mailing_care_of, market_value_date, number_of_rooms, other_building, owner_2, separate_utilities, sewer, site_type, street_direction, suffix, unfinished, unit, utility\n  ))\n\nnames(OPA_raw)\n\n\nThe property sales data was gathered from the OPA properties public data set from the City of Philadelphia. This data set was 32 columns and 583,825 observations. This file was too large for our shared GitHub work space so it was reduced by filtering for residential properties, years 2023 and 2024, location within Philadelphia, and sale price over 10 since some were NA, 1, or 10. This was just enough to get the most basic and general data to work with that ran with GitHub size limits. This reduced the size to 22121 observations. The original geojson file was overwritten and named OPA_data.\nProperties selected for residential included apartments &gt;4 units, single family, multi-family, and mixed use. Mixed use was left in as there are still residential unit to account yet add more complex property types to our total data set when comparing sale price and other aspects such as total area to other observations. These properties should also be cross referenced with zoning codes for future research.\nWe left mixed use in during this process to give us the most general data set representation. There was also limited data cleaning other than omitting columns that were mostly NA. This gave our model the most general data set to work with despite lower future RMSE values. Future research would be needed to most accurately assess the choices of losing data and a more generalized Philadelphia housing market verses very clean data and more specific Philadelphia housing market that may omit certain aspects of the housing market like data in lower income areas or multi use residential aspects. This could have also been conducted in grouping NA values and sparse categories. More complexity could be accounted for in future work.\nThis was our start simple and add complexity approach. Our original to final OPA data set went from 583,825 to 22,121 observations and from 32 to 68 variables."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#exploratory-data-analysis",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#exploratory-data-analysis",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.2 Exploratory Data Analysis",
    "text": "1.2 Exploratory Data Analysis\nBelow are selected property variables—Total Livable Area, Bedrooms, Bathrooms, and Age—in relation to Sale Price. Properties with excessive square footage (&gt;10,000 sqft), missing bedroom or bathroom data, over 12 bathrooms with low sale prices, or implausible construction years were removed to reduce skew and data errors. This additional filtering was kept for the rest of the analysis in this report.\n\n\nCode\n# filter out outliers from the dataset\nOPA_data &lt;- OPA_raw %&gt;%\n  filter(\n    total_livable_area &lt;= 10000,\n    year_built &gt; 1800,\n    !is.na(number_of_bathrooms),\n    !is.na(number_of_bedrooms),\n    number_of_bathrooms &lt; 12,\n  ) %&gt;%\n  mutate(\n    year_built = as.numeric(year_built),\n    building_age = 2025 - year_built\n  )\n\np1 &lt;- ggplot(OPA_data, aes(x = total_livable_area, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  scale_x_continuous(labels = comma_format()) +\n  labs(\n    title = \"Sale Price vs. Total Livable Area\",\n    x = \"Total Livable Area (sq ft)\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np2 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bedrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bedrooms\",\n    x = \"Number of Bedrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np3 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bathrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bathrooms\",\n    x = \"Number of Bathrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np4 &lt;- ggplot(OPA_data, aes(x = building_age, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Age\",\n    x = \"Age\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\n# Combine plots\n(p1 | p2) / (p3 | p4)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#feature-engineering",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#feature-engineering",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.3 Feature Engineering",
    "text": "1.3 Feature Engineering\n\n\nCode\nOPA_data &lt;- OPA_data %&gt;%\n  mutate(\n    # convert to numeric before interactions\n    total_livable_area = as.numeric(total_livable_area),\n    census_tract = as.numeric(as.character(census_tract)),\n    year_built = as.numeric(year_built),\n    total_area = as.numeric(total_area),\n    market_value = as.numeric(market_value),\n    number_of_bedrooms = as.numeric(number_of_bedrooms),\n\n    # building code and total area\n    int_type_tarea = as.numeric(as.factor(building_code_description)) * total_area,\n\n    # market and livable area\n    int_value_larea = market_value * total_livable_area,\n\n    # market and total area\n    int_value_tarea = market_value * total_area,\n\n    # livable area and exterior condition\n    int_larea_econd = total_livable_area * as.numeric(as.factor(exterior_condition)),\n\n    # livable area and interior condition\n    int_larea_icond = total_livable_area * as.numeric(as.factor(interior_condition)),\n\n    # livable area and bedrooms\n    int_larea_beds = total_livable_area * number_of_bedrooms\n  )\n\n\n\n\nCode\npa_crs &lt;- 2272  \nneighbor_points &lt;- st_transform(OPA_data, pa_crs)\n\nnrow(nhoods)\n\nst_crs(neighbor_points)\nnhoods &lt;- st_transform(nhoods, 2272)\n\n#joining houses to neighborhoods\nneighbor_points &lt;- neighbor_points %&gt;%\n  st_join(., nhoods, join = st_intersects)\n\n# one property doesn't lie in any neighborhood\nneighbor_points &lt;- neighbor_points[!is.na(neighbor_points$NAME),]\n\n#results \nneighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  count(NAME) %&gt;%\n  arrange(desc(n))\n\n\n\n\nCode\n#spatial joins\nprice_by_nhood &lt;- neighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(NAME) %&gt;%\n  dplyr::summarize(\n    median_price = median(sale_price, na.rm = TRUE),\n    n_sales = n()\n  )\n\nnhoods_prices &lt;- nhoods %&gt;%\n  left_join(., price_by_nhood, by = \"NAME\")\n\n#setting median house price classes\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    price_class = cut(median_price,\n                      breaks = c(0, 400000, 600000, 800000, 1000000, Inf),\n                      labels = c(\"Under $400k\", \"$400k-$600k\", \"$600k-$800k\", \n                                 \"$800k-$1M\", \"Over $1M\"),\n                      include.lowest = TRUE)\n  )\n\n#mapping\nggplot() +\n  geom_sf(data = nhoods_prices, aes(fill = price_class), \n          color = \"white\", size = 0.5) +\n  scale_fill_brewer(\n    name = \"Median Price\",\n    palette = \"YlOrRd\",\n    na.value = \"grey90\",\n    direction = 1\n  ) +\n  labs(\n    title = \"Median Home Prices by Philadelphia Neighborhood\",\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  head(10)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(n_sales)) %&gt;%\n  head(5)\n\n\n\n\nCode\n# Define wealthy as &gt;=$420,000 which is 1.5x city median of 279,900\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(median_price &gt;= 420000, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\nnhoods_prices %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n  wealthy_neighborhood   n\n1          Not Wealthy 123\n2              Wealthy  26\n3                 &lt;NA&gt;  10\n\n\nCode\nneighbor_points &lt;- neighbor_points %&gt;%\n  left_join(.,\n            nhoods_prices %&gt;%\n              st_drop_geometry %&gt;%\n              select(NAME, wealthy_neighborhood),\n            by = \"NAME\")\n\n# Still add neighbor points to OPA data\n\n\nHouseholds were denoted as wealthy if their median household price was over $420,000, which is 1.5x city median of 279,900. This term will be used in an interaction in Model 4 to account for theoretical differences in wealthy neighborhoods, such as inflated costs for additional home amenities such as bedrooms, bathrooms, or livable floor area."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation-1",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "2.1 Data Preparation",
    "text": "2.1 Data Preparation\n\n\nCode\n# link variables and aliases\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\",\n          \"med_age\" = \"B01002_001\")\n\n# the FIPS code for the state of PA is 42\nfips_pa &lt;- 42\n\n\nVariables pulled from the census include total population, median household income, and median age.\n\n\nCode\n# retrieve data from the ACS for 2023\ndemo_vars_pa &lt;- get_acs(geography = \"tract\",\n                        variable = vars,\n                        year = 2023,\n                        state = fips_pa,\n                        output = \"wide\",\n                        geometry = T,\n                        progress_bar = F) %&gt;% \n  st_transform(2272)\n\n# separate NAME column into its constituent parts\ndemo_vars_pa &lt;- demo_vars_pa %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n# filter out Philadelphia tracts\ndemo_vars_phl &lt;- demo_vars_pa %&gt;%\n  filter(COUNTY == \"Philadelphia\")\n\n\n\n\nCode\n# plot cenusus variables to compare\nplot(demo_vars_phl[,\"pop_totE\"],\n     main = \"Total Population\",\n     breaks = seq(0, 10500, 500),\n     nbreaks = 21)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_hh_incE\"],\n     main = \"Median Household Income\",\n     breaks = seq(0, 200000, 10000),\n     nbreaks = 20)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_ageE\"],\n     main = \"Median Age\",\n     breaks = seq(0, 75, 5),\n     nbreaks = 15)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# get NA counts per column\nna_counts &lt;- sapply(demo_vars_phl, function(x) {sum(is.na(x))})\nkable(t(as.data.frame(na_counts)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nTRACT\nCOUNTY\nSTATE\npop_totE\npop_totM\nmed_hh_incE\nmed_hh_incM\nmed_ageE\nmed_ageM\ngeometry\n\n\n\n\nna_counts\n0\n0\n0\n0\n0\n0\n27\n27\n17\n17\n0\n\n\n\n\n\nCode\n# filter out all rows that have at least one column with an na value\nna_index &lt;- !complete.cases(demo_vars_phl %&gt;% st_drop_geometry())\ndemo_vars_phl_na &lt;- demo_vars_phl[na_index,]\nkable(head(demo_vars_phl_na, 5) %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101980902\n9809.02\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101980800\n9808.00\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n\n\n\n27 and 17 census tracts have a value of NA for median household income and median age, respectively. For the 17 census tracts where there is no reported population, the median household income and median age will be set to 0. The remaining 10 census tracts that have population but no reported median household income will be omitted from the dataset.\n\n\nCode\n# create a dataset with NAs replaced with zero where applicable\ndemo_vars_phl_rep &lt;- demo_vars_phl %&gt;% \n  mutate(med_hh_incE = case_when(pop_totE == 0 & is.na(med_hh_incE) ~ 0,\n                                 .default = med_hh_incE),\n         med_ageE = case_when(pop_totE == 0 & is.na(med_ageE) ~ 0,\n                                 .default = med_ageE))\n\n# final cleaned dataset without the 10 census tracts that have population values but have NA values for Median Household Income\ndemo_vars_phl_clean &lt;- demo_vars_phl_rep[complete.cases(demo_vars_phl_rep %&gt;%\n                                                          select(-ends_with(\"M\")) %&gt;%\n                                                          st_drop_geometry()),]\n\n# table with the omitted census tracts\ndemo_vars_phl_omit &lt;- demo_vars_phl_rep[!complete.cases(demo_vars_phl_rep %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry()),]\nkable(demo_vars_phl_omit %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F, caption = \"Census Tracts Omitted from Analysis due to Data Unavailability\")\n\n\n\nCensus Tracts Omitted from Analysis due to Data Unavailability\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n42101014800\n148.00\nPhiladelphia\nPennsylvania\n892\nNA\n40.9\n\n\n42101027700\n277.00\nPhiladelphia\nPennsylvania\n5489\nNA\n36.9\n\n\n42101030100\n301.00\nPhiladelphia\nPennsylvania\n6446\nNA\n37.2\n\n\n42101989100\n9891.00\nPhiladelphia\nPennsylvania\n1240\nNA\n29.7\n\n\n42101989300\n9893.00\nPhiladelphia\nPennsylvania\n160\nNA\n30.5\n\n\n42101020500\n205.00\nPhiladelphia\nPennsylvania\n3383\nNA\n33.3\n\n\n42101980200\n9802.00\nPhiladelphia\nPennsylvania\n396\nNA\n74.9\n\n\n\n\n\n\n\nCode\n# join census variables to the OPA data\nOPA_data &lt;- st_join(OPA_data, demo_vars_phl_clean %&gt;% select(pop_totE, med_hh_incE, med_ageE))\n\n# isolate NA rows and plot where they are\ncensusNAs &lt;- OPA_data[is.na(OPA_data$med_hh_incE),]\n\ncensus_plt1 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry) +\n  geom_sf(data = censusNAs, size = 0.15) +\n  theme_void() +\n  labs(title = \"Properties without Census Data\")\ncensus_plt2 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry, fill = \"black\", color = \"transparent\") +\n  theme_void() +\n  labs(title = \"Census Tracts with Data (Black)\")\n\n(census_plt1 | census_plt2)\n\n\n\n\n\n\n\n\n\nOf the 22121 properties in the dataset after cleaning and omitting outliers, 248 - approximately 1.1% of the dataset - have no associated census data due to the lack of a Median Household Income value for those census tracts. Comparing plots of property locations without census data and that of census tracts which have data confirms this spatial relationship."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation-2",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#data-preparation-2",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.1 Data Preparation",
    "text": "3.1 Data Preparation\nThis stage prepares and validates the OpenDataPhilly spatial datasets used to engineer neighborhood-level variables for the housing model.\nSteps Performed\n\nTransformed all spatial datasets to EPSG 2272 (NAD83 / PA South ftUS) for consistent distance measurements.\nRemoved invalid geometries, dropped Z/M values, and converted all housing geometries to points.\nImported and projected OpenDataPhilly amenities:\n\nTransit Stops\nHospitals\nParks & Recreation Sites\nSchools Parcels (centroids created from polygon features)\nCrime Incidents (2023 and 2024 combined)\n\n\n\n\nCode\n#CRS & radii\npa_crs &lt;- 2272    # NAD83 / PA South (ftUS)\nmi_to_ft   &lt;- 5280\nr_park_ft   &lt;- 0.25 * mi_to_ft\nr_crime_ft  &lt;- 0.50 * mi_to_ft\nr_school_ft &lt;- 0.50 * mi_to_ft\n\n# turn off spherical geometry (makes buffer/join ops faster)\nsf::sf_use_s2(FALSE)\n\n## CONVERT SALES DATA TO POINTS ##\nOPA_points &lt;- st_transform(OPA_data, pa_crs)\n\n#Drop Z/M if present\nst_geometry(OPA_points) &lt;- st_zm(st_geometry(OPA_points), drop = TRUE, what = \"ZM\")\n\n#Make geometries valid\nst_geometry(OPA_points) &lt;- st_make_valid(st_geometry(OPA_points))\n\n#Ensure POINT geometry (works for points/lines/polygons/collections)\nst_geometry(OPA_points) &lt;- st_point_on_surface(st_geometry(OPA_points))\n\n#Add sale ID\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(sale_id = dplyr::row_number())\n\n#read & project layers\ntransit   &lt;- st_read(\"./data/Transit_Stops_(Spring_2025)/Transit_Stops_(Spring_2025).shp\", quiet = TRUE) |&gt; st_transform(pa_crs)\nhospitals &lt;- st_read(\"./data/Hospitals.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\nparksrec  &lt;- st_read(\"./data/PPR_Program_Sites.geojson\", quiet = TRUE)|&gt; st_transform(pa_crs)\nschools_polygons   &lt;- st_read(\"./data/Schools_Parcels.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\ncrime_2023     &lt;- st_read(\"./data/crime_incidents_2023/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\ncrime_2024     &lt;- st_read(\"./data/crime_incidents_2024/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\n\n#combine 2023 & 2024 crime datasets\ncrime &lt;- rbind(crime_2023, crime_2024)\n\n#create centroids for schools dataset\nschools &lt;- if (any(st_geometry_type(schools_polygons) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_centroid(schools_polygons, )\n} else {\n  schools_polygons\n}\n\n#crop transit data to philadelphia\nphilly_boundary &lt;- st_union(nhoods)\n\ntransit &lt;- st_filter(transit, philly_boundary, .predicate = st_within)"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#exploratory-data-analysis-1",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#exploratory-data-analysis-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.2 Exploratory Data Analysis",
    "text": "3.2 Exploratory Data Analysis\nExploratory plots and maps examine the raw accessibility patterns across Philadelphia before feature engineering.\n\n\nCode\n# Transit stops (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = transit, size = 0.3, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Transit Stops (SEPTA Spring 2025)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Hospitals (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = hospitals, size = 0.6, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Hospitals\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Parks & Recreation Program Sites (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = parksrec, size = 0.4, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Parks & Recreation Sites\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Schools (centroids of polygons) — raw\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = schools, size = 0.4, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Schools (Centroids)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Crime points are huge; sampling for speed\nset.seed(5080)\ncrime_quick &lt;- if (nrow(crime) &gt; 30000) dplyr::slice_sample(crime, n = 30000) else crime\n\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = crime_quick, size = 0.1, alpha = 0.25) +\n  labs(title = \"Raw Layer Check: Crime Incidents (sampled if large)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n3.2.1 Interpretation\n\nTransit Stops: Dense corridors radiate from Center City, showing strong transit coverage.\nHospitals: Sparse but geographically balanced.\nParks & Recreation: uneven distribution,\nSchools: evenly distributed across most neighborhoods\nCrime: Visibly concentrated, confirming the need for log-transformed counts"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#feature-engineering-1",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#feature-engineering-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.3 Feature Engineering",
    "text": "3.3 Feature Engineering\nSpatial features were derived using two complementary approaches: k-Nearest Neighbor (kNN) and buffer-based counts, depending on whether accessibility was best captured as proximity or exposure.\n\n\nCode\n#| message: false\n#| warning: false\n\n#clean sales data\nsales_xy &lt;- st_coordinates(OPA_points)\nok_sales  &lt;- complete.cases(sales_xy)\nOPA_points &lt;- OPA_points[ok_sales, ]    # keep only rows with valid XY\nsales_xy  &lt;- st_coordinates(OPA_points) # refresh coordinates\n\ntransit_xy &lt;- st_coordinates(transit)\nhosp_xy    &lt;- st_coordinates(hospitals)\n\n# feature 1 - distance to nearest transit stop (ft)\nknn_tr &lt;- FNN::get.knnx(\n  data  = st_coordinates(transit),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_transit_ft = as.numeric(knn_tr$nn.dist[,1]))\n\n# feature 2 - distance to nearest hospital (ft)\nknn_hp &lt;- FNN::get.knnx(\n  data  = st_coordinates(hospitals),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_hospital_ft = as.numeric(knn_hp$nn.dist[,1]))\n\n\n\n\nCode\n# feature 3 - parks/rec sites within 0.25 mi (count)\nrel_parks &lt;- sf::st_is_within_distance(OPA_points, parksrec, dist = r_park_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(parks_cnt_0p25mi = lengths(rel_parks))\n\n# feature 4 - crime count within 0.5 mi (per square mile)\ncrime_buffer &lt;- st_buffer(OPA_points, dist = r_crime_ft)\n\nrel_crime &lt;- st_intersects(crime_buffer, crime, sparse = TRUE)\n\n# count number of crimes\ncrime_cnt &lt;- lengths(rel_crime)\n\nrm(rel_crime)\n\nOPA_points &lt;- OPA_points |&gt;\n  mutate(\n    crime_cnt_0p5mi     = crime_cnt,\n    log1p_crime_cnt_0p5 = log1p(crime_cnt_0p5mi)\n  )\n\n# feature 5 - schools within 0.5 mi (using centroids)\nrel_sch &lt;- sf::st_is_within_distance(OPA_points, schools, dist = r_school_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(schools_cnt_0p5mi = lengths(rel_sch))\n\n\n\n3.3.1 Summary Table and Justification\n\n\n\n\n\n\n\n\n\nFeature\nMethod\nParameter\nTheoretical Rationale\n\n\n\n\nDistance to Nearest Transit Stop\nkNN (k = 1)\n–\nCaptures ease of access to public transport; nearest stop approximates walkability and job access.\n\n\nDistance to Nearest Hospital\nkNN (k = 1)\n–\nReflects accessibility to health care and emergency services; proximity adds perceived security for households.\n\n\nParks & Rec Sites within 0.25 mi\nBuffer Count\nr = 0.25 mi\nMeasures exposure to green space and recreational facilities within a 5-minute walk; positive amenity effect on property value.\n\n\nCrime Incidents within 0.5 mi\nBuffer Count\nr = 0.5 mi\nRepresents local safety environment; higher crime counts reduce housing desirability.\n\n\nSchools within 0.5 mi\nBuffer Count\nr = 0.5 mi\nReflects educational access and family appeal; clustering of schools often raises residential demand.\n\n\nPopulation\nCensus\n–\nRepresents the present residential demand within a census tract\n\n\nMedian Household Income\nCensus\n–\nIndicative of the ability of present residents of a census tract to afford housing\n\n\nMedian Age\nCensus\n–\nMeasure of the dominant age group in a census tract (i.e. high student or elderly population)\n\n\n\n\n\n3.3.2 Feature Validation and Visualization\n\n\nCode\n## Transit Accessibility\nggplot(OPA_points, aes(x = dist_nearest_transit_ft)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Transit Stop\",\n       x = \"Feet to Nearest Stop\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_transit_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"plasma\", labels = comma) +\n  labs(title = \"Transit Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Hospital Proximity\nggplot(OPA_points, aes(x = dist_nearest_hospital_ft)) +\n  geom_histogram(fill = \"darkorange\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Hospital\",\n       x = \"Feet to Nearest Hospital\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_hospital_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"magma\", labels = comma) +\n  labs(title = \"Hospital Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Parks & Recreation\nggplot(OPA_points, aes(x = parks_cnt_0p25mi)) +\n  geom_histogram(fill = \"seagreen\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Parks & Rec Sites Within 0.25 mi\",\n       x = \"Count within 0.25 mi\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = parks_cnt_0p25mi), size = 0.6) +\n  scale_color_viridis_c(option = \"viridis\") +\n  labs(title = \"Proximity to Parks & Recreation (0.25 mi Buffer)\",\n       color = \"Parks Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Crime Counts\nggplot(OPA_points, aes(x = crime_cnt_0p5mi)) +\n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Crime Incidents Within 0.5 mi\",\n       x = \"Crime Count (2023–2024)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = log1p_crime_cnt_0p5), size = 0.6) +\n  scale_color_viridis_c(option = \"inferno\") +\n  labs(title = \"Crime Exposure (log-transformed within 0.5 mi)\",\n       color = \"log(1+Crime Count)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Schools Accessibility\nggplot(OPA_points, aes(x = schools_cnt_0p5mi)) +\n  geom_histogram(fill = \"purple\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Schools Within 0.5 mi\",\n       x = \"School Count (0.5 mi Buffer)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = schools_cnt_0p5mi), size = 0.6) +\n  scale_color_viridis_c(option = \"cividis\") +\n  labs(title = \"School Accessibility (0.5 mi Buffer)\",\n       color = \"Schools Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nTransit proximity: Most parcels are within 500 ft of a stop, confirming strong transit coverage across Philadelphia.\nHospital proximity: Right-skewed distribution, consistent with limited facility count.\nParks access: Sparse exposure (mostly 0–1 within 0.25 mi), highlighting recreational inequities.\nCrime exposure: Wide variation, clustered along high-density corridors; log-transformed to stabilize scale.\nSchool proximity: Uniform urban coverage with typical parcels having 4-7 schools within 0.5 mi.\n\n\n\nCode\nsp_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = T)\n\nstr(sp_data$sale_price)\n\n\n int [1:23611] 389000 20000 309000 185000 399000 50000 70000 50000 30000 230000 ...\n\n\nCode\nsp_data_filtered &lt;- sp_data %&gt;%\n  mutate(sale_price = as.numeric(sale_price)) %&gt;%\n  filter(sale_price &gt; 1000)\n\nggplot(sp_data_filtered, aes(x = sale_price)) + \n  geom_histogram(\n    binwidth = 20000, \n    fill = \"grey\",\n    color = \"black\"\n  ) +\n  labs(\n    title = \"Histogram of Sale Prices in Philadelphia\", \n    x = \"Sale Price\",\n    y = \"Count of Homes\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(0, 2000000), ylim = c(0, 3000))\n\n\n\n\n\n\n\n\n\nCode\nsummary(sp_data_filtered$sale_price)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    1500   170000   269000   348626   395000 30000000 \n\n\nCode\nsp_data_filtered &lt;- sp_data_filtered %&gt;%\n  mutate(\n    sale_price = as.numeric(sale_price),\n    sale_price_capped = pmin(sale_price, quantile(sale_price, 0.99, na.rm = TRUE))\n  )\n\nggplot(sp_data_filtered) +\n  geom_sf(aes(color = sale_price_capped), size = 0.6, alpha = 0.7) +\n  scale_color_viridis_c(labels = label_dollar(), name = \"Sale Price (USD)\") +\n  labs(title = \"Philadelphia Sale Prices\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# function to check for statistically significant correlations between independent variables\nsig_corr &lt;- function(dat, dep_var) {\n  # remove the independent variable from the dataset\n  dat_corr &lt;- dat %&gt;% select(-all_of(dep_var))\n  \n  # run a correlation matrix for the independent vars\n  correlation_matrix &lt;- rcorr(as.matrix(dat_corr))\n  values &lt;- correlation_matrix$r\n  vifs &lt;- apply(values, 1, function(x){\n    return(round(1/(1-abs(x)), 2))\n  })\n  \n  values_df &lt;- values %&gt;% as.data.frame()\n  vifs_df &lt;- vifs %&gt;% as.data.frame()\n  \n  # convert correlation coefficients and p-values to long format\n  corCoeff_df &lt;- correlation_matrix$r %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corVIF_df &lt;- vifs %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corPval_df &lt;- correlation_matrix$P %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  # merge long format data\n  corMerge &lt;- list(\n    corCoeff_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\") %&gt;% as.data.frame(),\n    corVIF_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"vif_factor\") %&gt;% as.data.frame(),\n    corPval_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"p_value\") %&gt;% as.data.frame()) %&gt;%\n    reduce(left_join, by = c(\"var1\", \"var2\"))\n  \n  # filter to isolate unique pairs, then rows with correlation &gt; 0.5 and p &lt; 0.05\n  corUnfiltered &lt;- corMerge %&gt;% \n    filter(var1 != var2) %&gt;% \n    rowwise() %&gt;% \n    filter(var1 &lt; var2) %&gt;% \n    ungroup() %&gt;% \n    as.data.frame()\n  \n  corFiltered &lt;- corUnfiltered %&gt;% \n    filter(abs(vif_factor) &gt; 3 & p_value &lt; 0.05) %&gt;% \n    arrange(desc(abs(correlation)))\n  \n  # save the raw correlation values and the filtered variable pairs\n  final &lt;- set_names(list(values_df, vifs_df, corUnfiltered, corFiltered),\n                     c(\"R2\", \"VIF\", \"AllCor\", \"SelCor\"))\n  \n  return(final)\n}\n\n# create a dataset with just modeling variables\nOPA_modelvars &lt;- OPA_points %&gt;% select(sale_price, total_livable_area, building_age, number_of_bedrooms, number_of_bathrooms,\n                                       pop_totE, med_hh_incE, med_ageE,\n                                       dist_nearest_transit_ft, dist_nearest_hospital_ft, parks_cnt_0p25mi, log1p_crime_cnt_0p5, schools_cnt_0p5mi,\n                                       )\n\n# calculate VIFs and determine potentially troublesome correlations between variables\nvif_check &lt;- sig_corr(OPA_modelvars %&gt;% st_drop_geometry(), dep_var = \"sale_price\")\n\nkable(vif_check[[\"VIF\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_livable_area\nbuilding_age\nnumber_of_bedrooms\nnumber_of_bathrooms\npop_totE\nmed_hh_incE\nmed_ageE\ndist_nearest_transit_ft\ndist_nearest_hospital_ft\nparks_cnt_0p25mi\nlog1p_crime_cnt_0p5\nschools_cnt_0p5mi\n\n\n\n\ntotal_livable_area\nInf\n1.22\n1.70\n1.99\n1.14\n1.24\n1.07\n1.07\n1.01\n1.03\n1.22\n1.03\n\n\nbuilding_age\n1.22\nInf\n1.01\n1.32\n1.04\n1.23\n1.18\n1.21\n1.12\n1.05\n1.43\n1.24\n\n\nnumber_of_bedrooms\n1.70\n1.01\nInf\n2.26\n1.02\n1.09\n1.05\n1.05\n1.03\n1.01\n1.07\n1.02\n\n\nnumber_of_bathrooms\n1.99\n1.32\n2.26\nInf\n1.12\n1.25\n1.03\n1.02\n1.04\n1.01\n1.09\n1.01\n\n\npop_totE\n1.14\n1.04\n1.02\n1.12\nInf\n1.33\n1.16\n1.09\n1.18\n1.00\n1.01\n1.08\n\n\nmed_hh_incE\n1.24\n1.23\n1.09\n1.25\n1.33\nInf\n1.13\n1.01\n1.13\n1.04\n1.31\n1.03\n\n\nmed_ageE\n1.07\n1.18\n1.05\n1.03\n1.16\n1.13\nInf\n1.15\n1.27\n1.15\n1.70\n1.25\n\n\ndist_nearest_transit_ft\n1.07\n1.21\n1.05\n1.02\n1.09\n1.01\n1.15\nInf\n1.25\n1.11\n1.72\n1.44\n\n\ndist_nearest_hospital_ft\n1.01\n1.12\n1.03\n1.04\n1.18\n1.13\n1.27\n1.25\nInf\n1.10\n1.85\n1.66\n\n\nparks_cnt_0p25mi\n1.03\n1.05\n1.01\n1.01\n1.00\n1.04\n1.15\n1.11\n1.10\nInf\n1.25\n1.23\n\n\nlog1p_crime_cnt_0p5\n1.22\n1.43\n1.07\n1.09\n1.01\n1.31\n1.70\n1.72\n1.85\n1.25\nInf\n2.46\n\n\nschools_cnt_0p5mi\n1.03\n1.24\n1.02\n1.01\n1.08\n1.03\n1.25\n1.44\n1.66\n1.23\n2.46\nInf\n\n\n\n\n\nNone of the variables tested have a significant VIF score that is above 3, indicating that there is little concern of multicollinearity in the models moving forward."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-1-structural-terms",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-1-structural-terms",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.1 Model 1: Structural Terms",
    "text": "4.1 Model 1: Structural Terms\nOur first model uses structural property characteristics to build a multiple linear regression, regressing sale price on total livable area, number of bedrooms, number of bathrooms, and building age.\n\n\nCode\nmodel1_data &lt;- na.omit(OPA_points)\n\nmodel1 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n\n  data = model1_data\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age, data = model1_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1063434   -92640   -20598    58880  7759292 \n\nCoefficients:\n                      Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)         -11217.250  12625.853  -0.888               0.374    \ntotal_livable_area     221.516      5.126  43.217 &lt;0.0000000000000002 ***\nnumber_of_bedrooms  -34668.316   3239.669 -10.701 &lt;0.0000000000000002 ***\nnumber_of_bathrooms  70054.014   3967.307  17.658 &lt;0.0000000000000002 ***\nbuilding_age           144.447    104.814   1.378               0.168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 233700 on 10324 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2759 \nF-statistic: 984.7 on 4 and 10324 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-2-incorporation-of-census-data",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-2-incorporation-of-census-data",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.2 Model 2: Incorporation of Census Data",
    "text": "4.2 Model 2: Incorporation of Census Data\nOur second model builds on the structural property characteristics regression by incorporating census tract–level variables, including population, median household income, and median age.\n\n\nCode\nmodel2_data &lt;- na.omit(OPA_points)\n\nmodel2 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n\n    pop_totE +\n    med_hh_incE +\n    med_ageE,            \n    \n  data = model2_data\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + pop_totE + med_hh_incE + \n    med_ageE, data = model2_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-811594  -68780  -14242   37407 7882319 \n\nCoefficients:\n                        Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)         -146801.0740   22092.0331  -6.645      0.0000000000319 ***\ntotal_livable_area      182.7335       4.9502  36.914 &lt; 0.0000000000000002 ***\nnumber_of_bedrooms   -17516.7805    3085.6817  -5.677      0.0000000140948 ***\nnumber_of_bathrooms   57541.2262    3746.3467  15.359 &lt; 0.0000000000000002 ***\nbuilding_age            101.4828     101.8382   0.997                0.319    \npop_totE                 -7.5113       1.3699  -5.483      0.0000000427396 ***\nmed_hh_incE               2.5838       0.0747  34.587 &lt; 0.0000000000000002 ***\nmed_ageE                496.0580     357.9537   1.386                0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 219700 on 10321 degrees of freedom\nMultiple R-squared:  0.3601,    Adjusted R-squared:  0.3596 \nF-statistic: 829.6 on 7 and 10321 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-3-incorporation-of-spatial-features",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-3-incorporation-of-spatial-features",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.3 Model 3: Incorporation of Spatial Features",
    "text": "4.3 Model 3: Incorporation of Spatial Features\n\n\nCode\nmodel3_data &lt;- na.omit(OPA_points)\n\nmodel3 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n    \n  data = model3_data\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5, data = model3_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-918678  -69433  -13317   39370 7885217 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)              -289203.14616   43841.08957  -6.597\ntotal_livable_area           163.80852       5.66558  28.913\nnumber_of_bedrooms        -14331.54545    3085.57464  -4.645\nnumber_of_bathrooms        56708.75377    3736.10271  15.179\nbuilding_age                -222.32246     113.51444  -1.959\ntotal_area                     5.78704       0.77110   7.505\npop_totE                      -6.85323       1.38220  -4.958\nmed_hh_incE                    2.68603       0.08168  32.884\nmed_ageE                    1082.79304     372.86446   2.904\ndist_nearest_transit_ft        1.13603       7.45510   0.152\ndist_nearest_hospital_ft      -4.89926       0.77986  -6.282\nparks_cnt_0p25mi            3092.03921    3616.67427   0.855\nlog1p_crime_cnt_0p5        21979.86267    4391.65064   5.005\n                                     Pr(&gt;|t|)    \n(Intercept)                0.0000000000441242 ***\ntotal_livable_area       &lt; 0.0000000000000002 ***\nnumber_of_bedrooms         0.0000034479488026 ***\nnumber_of_bathrooms      &lt; 0.0000000000000002 ***\nbuilding_age                          0.05019 .  \ntotal_area                 0.0000000000000666 ***\npop_totE                   0.0000007228149059 ***\nmed_hh_incE              &lt; 0.0000000000000002 ***\nmed_ageE                              0.00369 ** \ndist_nearest_transit_ft               0.87889    \ndist_nearest_hospital_ft   0.0000000003472200 ***\nparks_cnt_0p25mi                      0.39260    \nlog1p_crime_cnt_0p5        0.0000005680776718 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 218400 on 10316 degrees of freedom\nMultiple R-squared:  0.3679,    Adjusted R-squared:  0.3672 \nF-statistic: 500.4 on 12 and 10316 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.4 Model 4: Incorporation of Interactions and Fixed Effects",
    "text": "4.4 Model 4: Incorporation of Interactions and Fixed Effects\n\n\nCode\n# join data separately here to avoid conflicts with earlier code blocks\nOPA_points_copy &lt;- left_join(OPA_points,\n                             neighbor_points %&gt;%\n                               select(parcel_number, wealthy_neighborhood) %&gt;%\n                               st_drop_geometry(),\n                             by = \"parcel_number\")\n\n\n\n\nCode\nmodel4_data &lt;- na.omit(OPA_points_copy)\n\nmodel4 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n    \n                     \n  data = model4_data\n)\n\nsummary(model4)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5 + number_of_bathrooms * \n    wealthy_neighborhood + int_type_tarea + int_value_larea + \n    int_value_tarea + int_larea_econd + int_larea_icond + int_larea_beds, \n    data = model4_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1346085   -55380   -11966    28218  7763690 \n\nCoefficients:\n                                                         Estimate\n(Intercept)                                     124512.2529592717\ntotal_livable_area                                 157.6273524102\nnumber_of_bedrooms                               31208.7841915060\nnumber_of_bathrooms                              24997.9383903477\nbuilding_age                                      -369.4190652424\ntotal_area                                           9.5617113327\npop_totE                                            -3.8679981835\nmed_hh_incE                                          1.0393743579\nmed_ageE                                           435.9742538792\ndist_nearest_transit_ft                              1.1607100282\ndist_nearest_hospital_ft                            -3.4937314721\nparks_cnt_0p25mi                                  -471.3368815190\nlog1p_crime_cnt_0p5                              -9758.6877799857\nwealthy_neighborhoodWealthy                      39818.8812951697\nint_type_tarea                                      -0.0163412165\nint_value_larea                                      0.0001564457\nint_value_tarea                                     -0.0000081686\nint_larea_econd                                    -10.3285787884\nint_larea_icond                                    -12.2381816682\nint_larea_beds                                     -14.9092798920\nnumber_of_bathrooms:wealthy_neighborhoodWealthy  58001.7136402215\n                                                       Std. Error t value\n(Intercept)                                      45454.5022193101   2.739\ntotal_livable_area                                  14.7964439643  10.653\nnumber_of_bedrooms                                4850.3805607397   6.434\nnumber_of_bathrooms                               3766.3281923183   6.637\nbuilding_age                                       106.2530116469  -3.477\ntotal_area                                           1.4114779417   6.774\npop_totE                                             1.2822166727  -3.017\nmed_hh_incE                                          0.0911237754  11.406\nmed_ageE                                           345.6926256018   1.261\ndist_nearest_transit_ft                              6.8915594246   0.168\ndist_nearest_hospital_ft                             0.7205731337  -4.849\nparks_cnt_0p25mi                                  3343.6898579606  -0.141\nlog1p_crime_cnt_0p5                               4322.8360013350  -2.257\nwealthy_neighborhoodWealthy                      14956.5415753626   2.662\nint_type_tarea                                       0.0101374871  -1.612\nint_value_larea                                      0.0000057824  27.056\nint_value_tarea                                      0.0000007365 -11.091\nint_larea_econd                                      2.6046336489  -3.965\nint_larea_icond                                      2.0327578830  -6.020\nint_larea_beds                                       2.0097460026  -7.418\nnumber_of_bathrooms:wealthy_neighborhoodWealthy   7671.1953640371   7.561\n                                                            Pr(&gt;|t|)    \n(Intercept)                                                  0.00617 ** \ntotal_livable_area                              &lt; 0.0000000000000002 ***\nnumber_of_bedrooms                                0.0000000001295545 ***\nnumber_of_bathrooms                               0.0000000000335729 ***\nbuilding_age                                                 0.00051 ***\ntotal_area                                        0.0000000000131872 ***\npop_totE                                                     0.00256 ** \nmed_hh_incE                                     &lt; 0.0000000000000002 ***\nmed_ageE                                                     0.20728    \ndist_nearest_transit_ft                                      0.86625    \ndist_nearest_hospital_ft                          0.0000012618714105 ***\nparks_cnt_0p25mi                                             0.88790    \nlog1p_crime_cnt_0p5                                          0.02400 *  \nwealthy_neighborhoodWealthy                                  0.00777 ** \nint_type_tarea                                               0.10700    \nint_value_larea                                 &lt; 0.0000000000000002 ***\nint_value_tarea                                 &lt; 0.0000000000000002 ***\nint_larea_econd                                   0.0000737487629116 ***\nint_larea_icond                                   0.0000000017982736 ***\nint_larea_beds                                    0.0000000000001278 ***\nnumber_of_bathrooms:wealthy_neighborhoodWealthy   0.0000000000000434 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201400 on 10308 degrees of freedom\nMultiple R-squared:  0.4631,    Adjusted R-squared:  0.462 \nF-statistic: 444.5 on 20 and 10308 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\n#there is only a premium on wealth neighborhood for total area, total livable area, and number of bathrooms that are significant. There is also a significant value for int_value_larea just from interacting the OPA data itsself which just assesses market value and size scalability."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#comparison-of-model-performance",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Appendix.html#comparison-of-model-performance",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.5 Comparison of Model Performance",
    "text": "4.5 Comparison of Model Performance\nWe can evaluate performance by conducting a 10-fold cross-validation of the 4 models, and comparing their RMSE, MAE, and \\(R^2\\).\n\n\nCode\n# Define 10-fold CV\ntrain_control &lt;- trainControl(\n  method = \"cv\",\n  number = 10,\n  savePredictions = \"final\"\n)\n\n\n\n\nCode\n# Model 1: Structural Features Only\nmodel1_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel1_cv\n\n\nLinear Regression \n\n10329 samples\n    5 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9296, 9297, 9296, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  230140.4  0.2865262  115174.1\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 2: Structural + Census\nmodel2_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    pop_totE +\n    med_hh_incE +\n    med_ageE,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel2_cv\n\n\nLinear Regression \n\n10329 samples\n    8 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9296, 9296, 9297, 9297, 9297, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  215430.3  0.3799429  90217.97\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 3: Structural + Census + Spatial\nmodel3_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel3_cv\n\n\nLinear Regression \n\n10329 samples\n   13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9295, 9295, 9296, 9298, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  211021.2  0.4053308  90144.36\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 4: Structural + Census + Spatial + Interaction\nmodel4_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n  \n  data = na.omit(OPA_points_copy),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel4_cv\n\n\nLinear Regression \n\n10329 samples\n   20 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9297, 9296, 9296, 9296, 9296, 9297, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  194649.2  0.4958671  71748.91\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\ncv_results &lt;- data.frame(\n  Model = c(\"Model 1\", \n            \"Model 2\", \n            \"Model 3\", \n            \"Model 4\"),\n  RMSE = c(\n        model1_cv$results$RMSE,\n        model2_cv$results$RMSE,\n        model3_cv$results$RMSE,\n        model4_cv$results$RMSE\n      ),\n  \n  log_RMSE = c(\n        log(model1_cv$results$RMSE),\n        log(model2_cv$results$RMSE),\n        log(model3_cv$results$RMSE),\n        log(model4_cv$results$RMSE)\n      ),\n  \n    MAE = c(\n        model1_cv$results$MAE,\n        model2_cv$results$MAE,\n        model3_cv$results$MAE,\n        model4_cv$results$MAE\n      ),\n  R_squared = c(\n        model1_cv$results$Rsquared,\n        model2_cv$results$Rsquared,\n        model3_cv$results$Rsquared,\n        model4_cv$results$Rsquared\n      )\n)\n\nprint(cv_results)\n\n\n    Model     RMSE log_RMSE       MAE R_squared\n1 Model 1 230140.4 12.34644 115174.13 0.2865262\n2 Model 2 215430.3 12.28039  90217.97 0.3799429\n3 Model 3 211021.2 12.25971  90144.36 0.4053308\n4 Model 4 194649.2 12.17895  71748.91 0.4958671\n\n\n\n\nCode\n# create model coefficient table in stargazer\nmodels_list &lt;- list(model1, model2, model3, model4)\nmodels_summary_table &lt;- stargazer(models_list, type = \"text\", style = \"default\")\n\n\n\n=============================================================================================================================================================\n                                                                                             Dependent variable:                                             \n                                                -------------------------------------------------------------------------------------------------------------\n                                                                                                 sale_price                                                  \n                                                           (1)                        (2)                         (3)                         (4)            \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\ntotal_livable_area                                      221.516***                 182.734***                 163.809***                  157.627***         \n                                                         (5.126)                    (4.950)                     (5.666)                    (14.796)          \n                                                                                                                                                             \nnumber_of_bedrooms                                    -34,668.320***             -17,516.780***             -14,331.550***               31,208.780***       \n                                                       (3,239.669)                (3,085.682)                 (3,085.575)                 (4,850.381)        \n                                                                                                                                                             \nnumber_of_bathrooms                                   70,054.010***              57,541.230***               56,708.750***               24,997.940***       \n                                                       (3,967.307)                (3,746.347)                 (3,736.103)                 (3,766.328)        \n                                                                                                                                                             \nbuilding_age                                             144.447                    101.483                    -222.322*                  -369.419***        \n                                                        (104.814)                  (101.838)                   (113.514)                   (106.253)         \n                                                                                                                                                             \ntotal_area                                                                                                     5.787***                    9.562***          \n                                                                                                                (0.771)                     (1.411)          \n                                                                                                                                                             \npop_totE                                                                           -7.511***                   -6.853***                   -3.868***         \n                                                                                    (1.370)                     (1.382)                     (1.282)          \n                                                                                                                                                             \nmed_hh_incE                                                                         2.584***                   2.686***                    1.039***          \n                                                                                    (0.075)                     (0.082)                     (0.091)          \n                                                                                                                                                             \nmed_ageE                                                                            496.058                  1,082.793***                   435.974          \n                                                                                   (357.954)                   (372.864)                   (345.693)         \n                                                                                                                                                             \ndist_nearest_transit_ft                                                                                          1.136                       1.161           \n                                                                                                                (7.455)                     (6.892)          \n                                                                                                                                                             \ndist_nearest_hospital_ft                                                                                       -4.899***                   -3.494***         \n                                                                                                                (0.780)                     (0.721)          \n                                                                                                                                                             \nparks_cnt_0p25mi                                                                                               3,092.039                   -471.337          \n                                                                                                              (3,616.674)                 (3,343.690)        \n                                                                                                                                                             \nlog1p_crime_cnt_0p5                                                                                          21,979.860***               -9,758.688**        \n                                                                                                              (4,391.651)                 (4,322.836)        \n                                                                                                                                                             \nwealthy_neighborhoodWealthy                                                                                                              39,818.880***       \n                                                                                                                                         (14,956.540)        \n                                                                                                                                                             \nint_type_tarea                                                                                                                              -0.016           \n                                                                                                                                            (0.010)          \n                                                                                                                                                             \nint_value_larea                                                                                                                            0.0002***         \n                                                                                                                                           (0.00001)         \n                                                                                                                                                             \nint_value_tarea                                                                                                                           -0.00001***        \n                                                                                                                                           (0.00000)         \n                                                                                                                                                             \nint_larea_econd                                                                                                                           -10.329***         \n                                                                                                                                            (2.605)          \n                                                                                                                                                             \nint_larea_icond                                                                                                                           -12.238***         \n                                                                                                                                            (2.033)          \n                                                                                                                                                             \nint_larea_beds                                                                                                                            -14.909***         \n                                                                                                                                            (2.010)          \n                                                                                                                                                             \nnumber_of_bathrooms:wealthy_neighborhoodWealthy                                                                                          58,001.710***       \n                                                                                                                                          (7,671.195)        \n                                                                                                                                                             \nConstant                                               -11,217.250              -146,801.100***             -289,203.100***             124,512.300***       \n                                                       (12,625.850)               (22,092.030)               (43,841.090)                (45,454.500)        \n                                                                                                                                                             \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nObservations                                              10,329                     10,329                     10,329                      10,329           \nR2                                                        0.276                      0.360                       0.368                       0.463           \nAdjusted R2                                               0.276                      0.360                       0.367                       0.462           \nResidual Std. Error                              233,656.600 (df = 10324)   219,730.600 (df = 10321)   218,431.100 (df = 10316)    201,398.000 (df = 10308)  \nF Statistic                                     984.706*** (df = 4; 10324) 829.572*** (df = 7; 10321) 500.372*** (df = 12; 10316) 444.490*** (df = 20; 10308)\n=============================================================================================================================================================\nNote:                                                                                                                             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nCode\n# plot predicted vs actual value plots\nggplot(model1_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 1 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model2_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 2 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model3_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 3 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model4_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 4 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Load Packages & Set Seed\nCode\nlibrary(pacman)\np_load(tidyverse, sf, viridis, terra, spdep, FNN, MASS, spatstat.geom, spatstat.explore,\n       viridis, patchwork, knitr, kableExtra, classInt)\n\nset.seed(1234)"
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#introduction",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#introduction",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis analysis will create a spatial predictive model for burglaries in Chicago, IL based on 311 calls that report the locations of vacant and/or abandoned buildings. The hypothesis behind this 311 call category choice is that reports of vacant buildings could indicate areas of the city that are in poor repair (broken lighting, poor street cleaning/maintenance, etc.) and with less functional surveillance equipment in the area. Please note that there are inherent biases in the use of 311 call data. For example, calls might be less likely to be made by marginalized communities who have been historically persecuted by law enforcement compared to more white, affluent communities. Furthermore, knowledge of how to make 311 calls or what the system’s intended purpose is might not be equally distributed across Chicago, leading to a spatial imbalance of calls across the region."
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-1-data-loading-exploration",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-1-data-loading-exploration",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 1: Data Loading & Exploration",
    "text": "Part 1: Data Loading & Exploration\nLoading Chicago police districts for cross-validation later on, police beats as a smaller administrative boundary, and the boundary of Chicago to create a fishnet, utilizing the coordinate system ESRI: 102271 (Illinois State Plane East, NAD83, US Feet).\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\", quiet = T) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\", quiet = T) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\", quiet = T) %&gt;%\n  st_transform('ESRI:102271')\n\n\nLoad burglaries data:\n\n\nCode\nburglaries &lt;- st_read(\"./data/burglaries.shp\", quiet = T) %&gt;% \n  st_transform('ESRI:102271')\n\n# filter out 2018 burglaries to isolate 2017 data (no other years present)\n# spatially filter to those within Chicago's boundary\nburglaries_2017 &lt;- burglaries %&gt;% \n  filter(year(Date) != 2018) %&gt;% \n  st_filter(chicagoBoundary, .predicate = st_within)\n\ncat(\"Number of Burglaries:\", nrow(burglaries_2017))\n\n\nNumber of Burglaries: 7455\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = chicagoBoundary, color = \"grey20\", fill = \"transparent\") +\n  geom_sf(data = burglaries_2017, color = \"maroon\", size = 0.25, alpha = 0.1) +\n  labs(title = \"Spatial Distribution of Burglaries\",\n       subtitle = \"Chicago, IL\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nLoad 311 Vacant and/or Abandoned Building reports. This dataset will be referred to as “Vacant Building Reports” for the remainder of this report:\n\n\nCode\nreports &lt;- read_csv(\"./data/Vacant_and_Abandoned_Buildings_Reported.csv\", show_col_types = F)\n\n# filter out only 2017 reports\nreports_2017 &lt;- reports %&gt;% \n  filter(endsWith(`DATE SERVICE REQUEST WAS RECEIVED`, \"2017\"))\n\n# filter out 2018 reports for later\nreports_2018 &lt;- reports %&gt;% \n  filter(endsWith(`DATE SERVICE REQUEST WAS RECEIVED`, \"2018\"))\n\n\n\n\nCode\n# remove rows without coordinate data and create a spatial point sf of the reports df\n# filter dataset to the extent of Chicago's boundary\nreports_2017_sf &lt;- reports_2017 %&gt;% \n  filter(!is.na(LATITUDE) & !is.na(LONGITUDE)) %&gt;% \n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"),\n           crs = 4326) %&gt;% \n  st_transform(\"ESRI:102271\") %&gt;% \n  st_filter(chicagoBoundary, .predicate = st_within)\n\n# filter dataset to the extent of Chicago's boundary\nreports_2018_sf &lt;- reports_2018 %&gt;% \n  filter(!is.na(LATITUDE) & !is.na(LONGITUDE)) %&gt;% \n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"),\n           crs = 4326) %&gt;% \n  st_transform(\"ESRI:102271\") %&gt;% \n  st_filter(chicagoBoundary, .predicate = st_within)\n\ncat(\"Number of Reports of Vacant/Abandoned Buildings (2017):\", nrow(reports_2017_sf), \"\\n\\n\")\n\n\nNumber of Reports of Vacant/Abandoned Buildings (2017): 3649 \n\n\nCode\ncat(\"Number of Reports of Vacant/Abandoned Buildings (2018):\", nrow(reports_2018_sf))\n\n\nNumber of Reports of Vacant/Abandoned Buildings (2018): 3842\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = chicagoBoundary, color = \"grey20\", fill = \"transparent\") +\n  geom_sf(data = reports_2017_sf, color = \"orange\", size = 0.1, alpha = 0.1) +\n  geom_sf(data = burglaries_2017, color = \"maroon\", size = 0.25, alpha = 0.2) +\n  labs(title = \"Spatial Distribution of Burglaries and Vacant Building Reports\",\n       subtitle = \"Chicago, IL\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nVisually, burglaries seem to be spatially correlated to the locations of vacant building reports. However, the extent of these reports seems to extend beyond the greatest concentrations of burglaries, which could influence the model later on."
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-2-fishnet-grid-creation",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-2-fishnet-grid-creation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 2: Fishnet Grid Creation",
    "text": "Part 2: Fishnet Grid Creation\n\n\nCode\n# create a 500x500m fishnet based on the chicago boundary\nfishnet &lt;- st_make_grid(chicagoBoundary,\n                        cellsize = 500,\n                        square = T) %&gt;% \n  st_as_sf() %&gt;% \n  mutate(fish_id = row_number()) %&gt;% \n  rename(geometry = x) %&gt;%\n  st_as_sf() %&gt;% \n  st_filter(chicagoBoundary, .predicate = st_intersects)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = chicagoBoundary, color = \"grey20\", fill = \"transparent\") +\n  geom_sf(data = fishnet, color = \"grey50\", fill = \"transparent\") +\n  labs(title = \"500x500m Fishnet Grid for Chicago\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nNow that a fishnet has been created, we can aggregate burglaries and vacant building reports to them, visualize them more effectively and model their correlation with burglaries.\n\n\nCode\n# transform 2017 data \nfishnet_2017_data_long &lt;- fishnet_data %&gt;%\n  dplyr::select(-cnt_reports_2018) %&gt;% \n  pivot_longer(cols = c(cnt_burglaries_2017, cnt_reports_2017), names_to = \"var\", values_to = \"value\")\n\nggplot(fishnet_2017_data_long) +\n  geom_sf(aes(fill = value), color = NA) +\n  facet_wrap(~ var) +\n  labs(title = \"Distribution of Burglaries and Reports across Chicago\",\n       subtitle = \"Visualized as the Square Root of Counts per Fishnet Cell\") +\n  scale_fill_viridis_c(option = \"plasma\", trans = \"sqrt\",\n                       name = \"Sqrt of Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nWhile there seem to be matching areas of high counts in South Chicago, there is a notable lack of reports in North Chicago that do not match higher counts of burglaries in the same area.\n\n\nCode\n# summary statistics for burglaries\ncat(\"Summary Statistics for Burglaries (2017):\\n\")\n\n\nSummary Statistics for Burglaries (2017):\n\n\nCode\nsummary(fishnet_data$cnt_burglaries_2017)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.033   5.000  40.000 \n\n\nCode\ncat(\"Percent of Cells with No Burglaries: \",\n    round(sum(fishnet_data$cnt_burglaries_2017 == 0) / nrow(fishnet_data), 3)*100,\n    \"%\\n\\n\",\n    sep = \"\")\n\n\nPercent of Cells with No Burglaries: 32.1%\n\n\nCode\n# summary statistics for reports 2017\ncat(\"Summary Statistics for Vacant Building Reports (2017):\\n\")\n\n\nSummary Statistics for Vacant Building Reports (2017):\n\n\nCode\nsummary(fishnet_data$cnt_reports_2017)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.485   1.000  24.000 \n\n\nCode\ncat(\"Percent of Cells with No Vacant Building Reports: \",\n    round(sum(fishnet_data$cnt_reports_2017 == 0) / nrow(fishnet_data), 3)*100,\n    \"%\\n\\n\",\n    sep = \"\")\n\n\nPercent of Cells with No Vacant Building Reports: 64.4%\n\n\nCode\n# summary statistics for reports 2018\ncat(\"Summary Statistics for Vacant Building Reports (2018):\\n\")\n\n\nSummary Statistics for Vacant Building Reports (2018):\n\n\nCode\nsummary(fishnet_data$cnt_reports_2018)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   1.563   2.000  33.000 \n\n\nCode\ncat(\"Percent of Cells with No Vacant Building Reports: \",\n    round(sum(fishnet_data$cnt_reports_2018 == 0) / nrow(fishnet_data), 3)*100,\n    \"%\\n\\n\",\n    sep = \"\")\n\n\nPercent of Cells with No Vacant Building Reports: 60.7%\n\n\nThere are approximately twice as many fishnet cells for both 2017 and 2018 that have no vacant building reports compared to those that have no burglaries, which is indicative of the wider spatial distribution of burglaries."
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-3-spatial-features",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-3-spatial-features",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 3: Spatial Features",
    "text": "Part 3: Spatial Features\n\n3.1 - Kernel Density Estimation of Burglaries\nCalculate a Kernel Density Estimation (KDE) baseline as a null hypothesis to demonstrate a scenario where burglaries occur in 2018 simply where they happened to occur in 2017. KDE spreads the value of a point (a single burglary) over a radius (1 kilometer) with the greatest share of the value close to the center of the point. By taking the mean value within a fishnet cell, we can get a distance-weighted, smoothed sense for how concentrated burglaries are in an area.\n\n\nCode\n# convert burglaries to ppp format for use with the spatstat package\nburglaries_2017_ppp &lt;- as.ppp(st_coordinates(burglaries_2017),\n                         W = as.owin(st_bbox(chicagoBoundary)))\n\n# calculate KDE values for burglary points\nkde_burglaries &lt;- density.ppp(burglaries_2017_ppp,\n                              sigma = 1000,\n                              edge = T)\n\n# convert KDE values to a raster\nkde_raster &lt;- rast(kde_burglaries)\n\n# extract mean KDE values from the spatial raster to the fishnet grid\nfishnet_data &lt;- fishnet_data %&gt;% \n  mutate(kde_value = terra::extract(kde_raster,\n                                    vect(fishnet_data),\n                                    fun = mean,\n                                    na.rm = T)[,2])\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet_data, aes(fill = kde_value), color = NA) +\n  scale_fill_viridis_c(name = \"KDE Value\",\n                       option = \"plasma\") +\n  labs(title = \"Kernel Density Estimation Baseline\",\n       subtitle = \"Simple spatial smoothing of burglary locations\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThe KDE yields a map of burglary hot spots, where the influence of each burglary has been smoothed out and averaged. This provides a good sense of where generally burglaries happen, but this method also over-estimates where burglaries happen due to the indiscriminate nature of smoothing over a uniform 1km buffer.\n\n\n3.2 - k-Nearest Neighbor to Reports\n\n\nCode\n# calculate coordinates of fishnet cells and the report locations\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet_data))\nreports_coords &lt;- st_coordinates(st_centroid(reports_2017_sf))\n\n# calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(reports_coords, fishnet_coords, k = 3)\n\n# join values to the fishnet\nfishnet_data &lt;- fishnet_data %&gt;% \n  mutate(knn_reports_2017 = rowMeans(nn_result$nn.dist))\n\nsummary(fishnet_data$knn_reports_2017)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   3.622  290.991  613.944  788.606 1069.304 4773.328 \n\n\nThe mean distance from the center of a fishnet cell to a report of a vacant building is approximately 790 meters. The value for k-nearest neighbor is an indicator of the average distance to the closest three reports of a vacant building. This is more descriptive than a simple count within a fishnet cell’s boundary since, for those that have no reports within them, there will still be a descriptive distance value to the nearest reports.\n\n\n3.3 - Distance to Hot Spots (Local Moran’s I)\n\n\nCode\ncalc_local_morans &lt;- function(data, var, k) {\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = T)\n  \n  local_moran &lt;- localmoran(data[[var]], weights)\n  \n  mean_val &lt;- mean(data[[var]], na.rm = T)\n  \n  data %&gt;% \n    mutate(local_i = local_moran[,1],\n           p_value = local_moran[,5],\n           is_sig = p_value &lt; 0.05,\n           moran_class = case_when(!is_sig ~ \"Not Significant\",\n                                   local_i &gt; 0 & .data[[var]] &gt; mean_val ~ \"High-High\",\n                                   local_i &gt; 0 & .data[[var]] &lt;= mean_val ~ \"Low-Low\",\n                                   local_i &lt; 0 & .data[[var]] &gt; mean_val ~ \"High-Low\",\n                                   local_i &lt; 0 & .data[[var]] &lt;= mean_val ~ \"Low-High\",\n                                   TRUE ~ \"Not Significant\"))\n}\n\n# calculate local moran's I for vacant building reports in 2017\nfishnet_data &lt;- calc_local_morans(data = fishnet_data,\n                                  var = \"cnt_reports_2017\",\n                                  k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet_data, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Vacant/Abandoned Building Report Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nhotspots &lt;- fishnet_data %&gt;% \n  filter(moran_class == \"High-High\") %&gt;% \n  st_centroid()\n\n# calculate distance from each cell to the nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet_data &lt;- fishnet_data %&gt;% \n    mutate(dist_to_hotspot = as.numeric(st_distance(st_centroid(fishnet_data),\n                                                    hotspots %&gt;% \n                                                      st_union())))\n  cat(\"Number of hot spot cells:\", nrow(hotspots), \"\\n\\n\")\n} else{\n  fishnet_data &lt;- fishnet_data %&gt;% \n    mutate(dist_to_hotspot = 0)\n  cat(\"No significant hot spots found \\n\\n\")\n}\n\n\nNumber of hot spot cells: 278 \n\n\nCode\ncat(\"Summary statistics of Distance to Hot Spot:\\n\")\n\n\nSummary statistics of Distance to Hot Spot:\n\n\nCode\nsummary(fishnet_data$dist_to_hotspot)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    1000    3000    3863    6000   14577 \n\n\nIn this scenario, Local Moran’s I is a measurement of how spatially correlated burglary counts and report counts are, with fishnet cells that themselves have high values for both variables and are located near fishnet cells with similarly high values are denoted as “high-high” cells, or hot spots. Cells for which the converse is true (low values for both) are referred to as cold spots. Analyzing how significant this spatial correlation is and calculating the distance of each fishnet cell to its closest neighboring hot spot cell allows for us to understand how each cell is related to more regional spatial patterns, as opposed to the k-nearest neighbors variable which illustrates hyper-localized spatial patterns."
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-4-count-regression-models",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-4-count-regression-models",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 4: Count Regression Models",
    "text": "Part 4: Count Regression Models\n\n4.1 - Join police districts for cross-validation later on\n\n\nCode\nfishnet_data_pd &lt;- st_join(fishnet_data,\n                        policeDistricts,\n                        join = st_within,\n                        left = T) %&gt;% \n  filter(!is.na(District))\n\nggplot() +\n  geom_sf(data = fishnet_data, fill = \"grey90\", color = \"grey30\", alpha = 0.5) +\n  geom_sf(data = fishnet_data_pd, fill = \"maroon\", color = NA, alpha = 0.3) +\n  labs(title = \"Visualizing Fishnet Cell Loss\",\n       subtitle = \"Black = Original Fishnet, Red = Trimmed Fishnet\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Fishnet Cell Count Prior to Filtering:\", nrow(fishnet_data), \"\\n\\n\")\n\n\nFishnet Cell Count Prior to Filtering: 2458 \n\n\nCode\ncat(\"Fishnet Cell Count After Filtering:\", nrow(fishnet_data_pd), \"\\n\\n\")\n\n\nFishnet Cell Count After Filtering: 1708 \n\n\nCode\ncat(\"Cell Loss Percent: \",\n    (1-round(nrow(fishnet_data_pd)/nrow(fishnet_data), 3))*100,\n    \"%\",\n    sep = \"\")\n\n\nCell Loss Percent: 30.5%\n\n\nIn order to accurately perform spatial cross-validation later in the analysis, we need to isolate only those fishnet cells that are entirely within each of Chicago’s police districts. This results in an approximately 30% loss in data, with 750 cells omitted.\n\n\n4.2 - Poisson Regression\nPoisson Regression is optimal for this modeling approach, since the dependent variable (burglaries) is a count variable.\n\n\nCode\n# extract variables to be used for modeling and remove spatial column\nfishnet_model &lt;- fishnet_data_pd %&gt;%\n  st_drop_geometry() %&gt;% \n  dplyr::select(fish_id,\n                District,\n                cnt_burglaries_2017,\n                cnt_reports_2017,\n                knn_reports_2017,\n                dist_to_hotspot) %&gt;% \n  na.omit()\n\n\n\n\nCode\n# fit poisson regression model for 2017\nmodel_poisson &lt;- fishnet_model %&gt;% \n  glm(cnt_burglaries_2017 ~ cnt_reports_2017 + knn_reports_2017 + dist_to_hotspot,\n      data = .,\n      family = \"poisson\")\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = cnt_burglaries_2017 ~ cnt_reports_2017 + knn_reports_2017 + \n    dist_to_hotspot, family = \"poisson\", data = .)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       1.679e+00  2.974e-02  56.455   &lt;2e-16 ***\ncnt_reports_2017  3.806e-02  3.501e-03  10.871   &lt;2e-16 ***\nknn_reports_2017 -9.788e-04  4.977e-05 -19.666   &lt;2e-16 ***\ndist_to_hotspot   6.984e-06  6.046e-06   1.155    0.248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6708.8  on 1707  degrees of freedom\nResidual deviance: 5229.5  on 1704  degrees of freedom\nAIC: 9296.8\n\nNumber of Fisher Scoring iterations: 6\n\n\nA poisson regression analysis yields coefficients that are positive for the count of reports and distance to the nearest hot spot cell variables, while negative for the k-nearest neighbor variable. These coefficients make sense for the count variable (greater count of reports = increased likelihood of burglaries) and the k-nearest neighbor variable (greater distance to a report = greater of burglaries). However, the coefficient for distance to a hot spot does not make sense (increased distance to a hots pot = more instances of burglaries). This coefficient also is not statistically significant (p &gt; 0.05) and has a high standard error value equivalent to 86.5% of the coefficient.\n\n\n6.2 - Check for Overdispersion\n\n\nCode\ndispersion &lt;- sum(residuals(model_poisson,\n                            type = \"pearson\")^2) / model_poisson$df.residual\ncat(\"Dispersion Parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion Parameter: 3.34 \n\n\nCode\ncat(\"Rule of Thumb: &gt;1.5 typically suggests overdispersion \\n\")\n\n\nRule of Thumb: &gt;1.5 typically suggests overdispersion \n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"✓ Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n⚠ Overdispersion detected! Consider Negative Binomial model.\n\n\nThis model violates the assumption that the mean of the dependent variable is equal to its variance, which is typically not true for crime datasets. A negative binomial model should be calculated to see if it predicts better.\n\n\n6.3 - Negative Binomial Regression\n\n\nCode\n# fit a negative binomial model\nmodel_nb &lt;- fishnet_data_pd %&gt;%\n  glm.nb(cnt_burglaries_2017 ~ cnt_reports_2017 + knn_reports_2017 + dist_to_hotspot,\n         data = .)\n\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = cnt_burglaries_2017 ~ cnt_reports_2017 + knn_reports_2017 + \n    dist_to_hotspot, data = ., init.theta = 1.469874947, link = log)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       1.626e+00  5.547e-02  29.323  &lt; 2e-16 ***\ncnt_reports_2017  4.536e-02  8.117e-03   5.589 2.29e-08 ***\nknn_reports_2017 -9.871e-04  7.590e-05 -13.005  &lt; 2e-16 ***\ndist_to_hotspot   2.042e-05  1.029e-05   1.984   0.0473 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.4699) family taken to be 1)\n\n    Null deviance: 2419.3  on 1707  degrees of freedom\nResidual deviance: 1962.7  on 1704  degrees of freedom\nAIC: 7772.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.4699 \n          Std. Err.:  0.0839 \n\n 2 x log-likelihood:  -7762.2070 \n\n\nCode\n# compare the AIC scores of both models (lower is better)\ncat(\"\\n\\nModel Comparison:\\n\", \n    \"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\",\n    \"Negative Binomial AIC:\", round(AIC(model_nb), 1))\n\n\n\n\nModel Comparison:\n Poisson AIC: 9296.8 \n Negative Binomial AIC: 7772.2\n\n\nThe negative binomial model has an improved AIC value, and while its coefficients follow the same sign pattern as before where the sign for distance to the nearest hot spot doesn’t make sense, it is now significant (p &lt; 0.05) with a standard error that is slightly improved at 50.4% of the coefficient value. This tells us that the negative binomial model is better at modeling burglary counts since it introduces a dispersion parameter that relaxes the assumption of the poisson regression that mean = variance."
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-5-spatial-cross-validation-2017",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-5-spatial-cross-validation-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 5: Spatial Cross-Validation (2017)",
    "text": "Part 5: Spatial Cross-Validation (2017)\nIn order to prevent information leakage between neighboring fishnet cells in a normal cross-validation procedure due their spatial relationships to each other, the following code performs a Leave-One-Group-Out (LOGO) Cross-Validation that will divide the dataset up based on police district and progressively leave each one out one by one, training a negative binomial model on the remaining districts.\n\n\nCode\ndistricts &lt;- unique(fishnet_model$District)\ncv_results &lt;- tibble()\n\nfor (i in seq_along(districts)) {\n  test_district &lt;- districts[i]\n  \n  # split into training and testing data\n  train_data &lt;- fishnet_model %&gt;% filter(District != test_district)\n  test_data &lt;- fishnet_model %&gt;% filter(District == test_district)\n  \n  # fit model on training data\n  model_cv &lt;- fishnet_model %&gt;%\n    glm.nb(cnt_burglaries_2017 ~ \n           cnt_reports_2017 + \n           knn_reports_2017 + \n           dist_to_hotspot,\n           data = .)\n  \n  # predict on test data\n  test_data &lt;- test_data %&gt;% \n    mutate(pred = predict(model_cv, test_data, type = \"response\"))\n  \n  # calculate metrics\n  mae &lt;- mean(abs(test_data$cnt_burglaries_2017 - test_data$pred))\n  rmse &lt;- sqrt(mean((test_data$cnt_burglaries_2017 - test_data$pred)^2))\n  \n  # store results\n  cv_results &lt;- bind_rows(cv_results,\n                          tibble(fold = i,\n                                 test_district = test_district,\n                                 n_test = nrow(test_data),\n                                 mae = mae,\n                                 rmse = rmse))\n}\n\n\n\n\nCode\n# view results of the spatial cross-validation\ncv_results %&gt;% \n  arrange(desc(mae)) %&gt;% \n  kable(digits = 2,\n       caption = \"LOGO CV Results by Police District\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nLOGO CV Results by Police District\n\n\nfold\ntest_district\nn_test\nmae\nrmse\n\n\n\n\n7\n3\n43\n5.45\n7.12\n\n\n12\n12\n73\n3.58\n4.99\n\n\n6\n7\n52\n3.15\n3.78\n\n\n17\n14\n46\n3.13\n4.58\n\n\n4\n6\n63\n3.01\n4.21\n\n\n14\n11\n43\n3.01\n3.43\n\n\n8\n2\n56\n2.72\n3.39\n\n\n16\n25\n85\n2.69\n3.73\n\n\n2\n4\n235\n2.54\n3.69\n\n\n1\n5\n98\n2.44\n2.84\n\n\n15\n18\n30\n2.42\n4.08\n\n\n5\n8\n197\n2.42\n3.38\n\n\n9\n9\n107\n2.42\n2.85\n\n\n11\n1\n28\n2.37\n3.01\n\n\n18\n19\n63\n2.30\n3.03\n\n\n13\n15\n32\n2.24\n2.82\n\n\n10\n10\n63\n2.11\n2.76\n\n\n3\n22\n112\n2.06\n2.58\n\n\n22\n24\n41\n1.99\n2.80\n\n\n21\n20\n30\n1.73\n2.10\n\n\n20\n17\n82\n1.70\n2.11\n\n\n19\n16\n129\n1.66\n1.97"
  },
  {
    "objectID": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-6-model-evaluation",
    "href": "assignments/assignment4/Sywulak-Herr_Henry_assignment4.html#part-6-model-evaluation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 6: Model Evaluation",
    "text": "Part 6: Model Evaluation\n\n6.1 - Generate Final Predictions\n\n\nCode\n# fit final model on all data\nfinal_model &lt;- fishnet_model %&gt;% \n  glm.nb(cnt_burglaries_2017 ~ cnt_reports_2017 + knn_reports_2017 + dist_to_hotspot,\n         data = .)\n\n# add predictions to the fishnet\nfishnet_data_pd &lt;- fishnet_data_pd %&gt;% \n  mutate(pred_nb = predict(final_model, fishnet_model, type = \"response\")[match(fish_id, fishnet_model$fish_id)])\n\n# add KDE predictions (normalized to the same scale as counts)\nkde_sum &lt;- sum(fishnet_data_pd$kde_value, na.rm = T)\ncount_sum &lt;- sum(fishnet_data_pd$cnt_burglaries_2017, na.rm = T)\nfishnet_data_pd &lt;- fishnet_data_pd %&gt;% mutate(pred_kde = (kde_value/kde_sum) * count_sum)\n\n\n\n\nCode\n# create a summary table of the model's coefficients\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n5.086\n0.055\n29.323\n0.000\n\n\ncnt_reports_2017\n1.046\n0.008\n5.589\n0.000\n\n\nknn_reports_2017\n0.999\n0.000\n-13.005\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n1.984\n0.047\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts.\n\n\n\n\n\n\n\n\n\n\nThe final model coefficients, when exponentiated, indicate that within a fishnet cell an increase of one report of an abandoned/vacant building results in a 4.6% increase in burglaries, a one unit increase in mean distance from the centroid of a fishnet cell to the nearest three reports of an abandoned/vacant building results in a 0.1% decrease in burglaries, and distance to the nearest hot spot has no effect on the number of burglaries within that fishnet cell.\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet_data_pd, aes(fill = cnt_burglaries_2017), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_void()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet_data_pd, aes(fill = pred_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_void()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet_data_pd, aes(fill = pred_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_void()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\nIt does not appear that the negative binomial model is doing a good job of predicting burglaries in North Chicago, as was previously noted in this report. The negative binomial model also generally seems to be under-predicting counts across all of Chicago.\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet_data_pd %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(pred_nb), !is.na(pred_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(cnt_burglaries_2017 - pred_nb)),\n    model_rmse = sqrt(mean((cnt_burglaries_2017 - pred_nb)^2)),\n    kde_mae = mean(abs(cnt_burglaries_2017 - pred_kde)),\n    kde_rmse = sqrt(mean((cnt_burglaries_2017 - pred_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.51\n3.46\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nThe more complex negative binomial model does not outperform a simple KDE baseline, with Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) values both less for the KDE baseline compared to the NB model. This indicates that the added complexity of the NB model might not be worth it when based off of 311 calls that report Vacant/Abandoned Buildings.\n\n\nCode\n# Calculate errors\nfishnet_data_pd &lt;- fishnet_data_pd %&gt;%\n  mutate(\n    error_nb = cnt_burglaries_2017 - pred_nb,\n    error_kde = cnt_burglaries_2017 - pred_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet_data_pd, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_void()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet_data_pd, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"plasma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_void()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe model tends to under-predict burglaries in North Chicago, though there are pockets of under-prediction in South Chicago as well. The worst over-prediction occurred in Central Chicago and South Chicago, where reports of vacant buildings were most common. This reveals that a dataset that models for burglaries needs to have a similar extent of data, as the 311 call category chosen for this analysis only spatially overlapped with burglaries in a few locations. Recall that of all fishnet cells, 32.1% didn’t have any value for burglaries while a much greater 64.4% of cells had no value for reports, producing a lot of counts of 0 for reports across a variety of values for burglaries.\nThe model coefficients and this analysis of prediction accuracy indicate that while this modeling approach could be improved upon with additional/more complete data to predict burglary distribution better, this model in its current form should not be used to predict for other years."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#scenario",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Vermont Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#learning-objectives",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-instructions",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-retrieval",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\nvt_data1 &lt;- get_acs(state = my_state,\n                    geography = \"county\",\n                    variables = c(\"med_hh_inc\" = \"B19013_001\",\n                                  \"tot_pop\" = \"B01003_001\"),\n                    year = 2022,\n                    survey = \"acs5\",\n                    output = \"wide\")\n\n# Clean the county names to remove state name and \"County\"\nvt_data1_trim &lt;- vt_data1 %&gt;% \n  mutate(NAME = str_remove(NAME, \" County, Vermont\"))\n\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nvt_data1_trim %&gt;% head(5)\n\n# A tibble: 5 × 6\n  GEOID NAME       med_hh_incE med_hh_incM tot_popE tot_popM\n  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 50001 Addison          85870        2958    37434       NA\n2 50003 Bennington       68558        2903    37326       NA\n3 50005 Caledonia        62964        2734    30418       NA\n4 50007 Chittenden       89494        2286   168309       NA\n5 50009 Essex            55247        3679     5976       NA"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nvt_data1_MOEpct &lt;- vt_data1_trim %&gt;%\n  mutate(med_hh_inc_Mpct = med_hh_incM/med_hh_incE * 100,\n         med_hh_inc_conf = case_when(med_hh_inc_Mpct &lt; 5 ~ \"High Confidence\",\n                                     med_hh_inc_Mpct &gt;= 5 & med_hh_inc_Mpct &lt;= 10 ~ \"Moderate Confidence\",\n                                     med_hh_inc_Mpct &gt; 10 ~ \"Low Confidence\",\n                                     .default = NA))\n# Create a summary showing count of counties in each reliability category\nvt_data1_reliability &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  summarize(count = n())\n\nvt_data1_reliability\n\n# A tibble: 3 × 2\n  med_hh_inc_conf     count\n  &lt;chr&gt;               &lt;int&gt;\n1 High Confidence         9\n2 Low Confidence          1\n3 Moderate Confidence     4\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nvt_data1_top5Mpct &lt;- vt_data1_MOEpct %&gt;%\n  arrange(desc(med_hh_inc_Mpct)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(-c(GEOID, tot_popM))\n\n# Format as table with kable() - include appropriate column names and caption\nkable(vt_data1_top5Mpct, col.names = c(\"County\", \"Median Household Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"), format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian Household Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nLamoille\n69886\n5846\n25977\n8.37\nModerate Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\nFranklin\n73633\n4436\n50101\n6.02\nModerate Confidence\n\n\nWindham\n65473\n3331\n45857\n5.09\nModerate Confidence\n\n\n\n\n\n Data Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]   Counties that have a high MOE percent have greater uncertainty in their estimates, likely due to sampling issues (possibly due to relatively low county population) or deriving from how the results are aggregated over multiple years (though this is less of an issue for the ACS5 as it is for the ACS1 or ACS3). Algorithms that are trained/rely on ACS5 income data for Maine could more readily over- or underestimate the actual median household income in counties such as Waldo, Lincoln, Knox, and others that have a high MOE percentage relative to the estimate."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# some counties produced \nselected_counties &lt;- vt_data1_MOEpct %&gt;% \n  group_by(med_hh_inc_conf) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nkable(selected_counties %&gt;% select(-c(GEOID, tot_popM)),\n      col.names = c(\"County\", \"Median HH Income\", \"Margin of Error\", \"Total Population\", \"MOE Percent\", \"Confidence Ranking\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMargin of Error\nTotal Population\nMOE Percent\nConfidence Ranking\n\n\n\n\nAddison\n85870\n2958\n37434\n3.44\nHigh Confidence\n\n\nGrand Isle\n86639\n10729\n7335\n12.38\nLow Confidence\n\n\nEssex\n55247\n3679\n5976\n6.66\nModerate Confidence\n\n\n\n\n\nComment on the output: [write something :)]   While Addison and Grand Isle counties have similar median household incomes (~$86k), the reliability of their estimates differs significantly (Addison MOE Pct = 3.4%, Grand Isle MOE Pct = 12.4%). The large difference in population between Addison county (~37k people) and Great Isle County (~7k people) likely factored into this difference in reliability."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\ndemo_vars &lt;- c(\"race_white\" = \"B03002_003\", \n               \"race_black\" = \"B03002_004\", \n               \"race_hispLatino\" = \"B03002_012\", \n               \"tot_pop\" = \"B03002_001\")\n# define counties to be selected\nmy_counties &lt;- str_remove(selected_counties$GEOID, \"50\")\n\n# Use get_acs() to retrieve tract-level data\nvt_data2 &lt;- get_acs(geography = \"tract\", \n                    variables = demo_vars,\n                    year = 2022, \n                    output = \"wide\", \n                    state = my_state,\n                    county = my_counties)\n\n# Hint: You may need to specify county codes in the county parameter\n\n# Add readable tract and county name columns using str_extract() or similar\nvt_data2_sep &lt;- vt_data2 %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nvt_data2_pcts &lt;- vt_data2_sep %&gt;%\n  mutate(white_pct = (race_whiteE/tot_popE)*100,\n         black_pct = (race_blackE/tot_popE)*100,\n         hispLatino_pct = (race_hispLatinoE/tot_popE)*100)"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\nvt_data2_hiPctHispLat &lt;- vt_data2_pcts %&gt;% filter(hispLatino_pct == max(hispLatino_pct))\npaste0(\"County with the Maximum Hispanic/Latino Percentage: \", vt_data2_hiPctHispLat$COUNTY, \" County\")\n\n[1] \"County with the Maximum Hispanic/Latino Percentage: Addison County\"\n\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\nvt_data2_avgDemo &lt;- vt_data2_pcts %&gt;% \n  group_by(COUNTY) %&gt;% \n  summarise(tract_count = n(),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\n# Show: number of tracts, average percentage for each racial/ethnic group\n# Create a nicely formatted table of your results using kable()\nkable(vt_data2_avgDemo, col.names = c(\"County\", \"Tract Count\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pct\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTract Count\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pct\n\n\n\n\nAddison\n10\n91.4\n0.8869\n2.50\n\n\nEssex\n3\n94.0\n0.0335\n1.54\n\n\nGrand Isle\n2\n91.0\n1.0907\n2.00"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\nvt_data2_MOEpct &lt;- vt_data2_sep %&gt;%\n  mutate(race_white_Mpct = (race_whiteM/race_whiteE)*100,\n         race_black_Mpct = race_blackM/race_blackE*100,\n         race_hispLatinoMpct = race_hispLatinoM/race_hispLatinoE*100)\n         \n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\nvt_data2_reliability &lt;- vt_data2_MOEpct %&gt;% \n  mutate(race_all_conf = ifelse(race_white_Mpct &gt; 15 | race_black_Mpct &gt; 15 | race_hispLatinoMpct &gt; 15, 1, 0))\n\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\ndata.frame(total_tracts = length(vt_data2_reliability$race_all_conf),\n           flagged_tracts = sum(vt_data2_reliability$race_all_conf),\n           flagger_tracts_pct = sum(vt_data2_reliability$race_all_conf)/length(vt_data2_reliability$race_all_conf)*100)\n\n  total_tracts flagged_tracts flagger_tracts_pct\n1           15             15                100"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nMOE_issue_stats &lt;- vt_data2_reliability %&gt;%\n  group_by(race_all_conf) %&gt;% \n  summarise(avg_pop = mean(tot_popE),\n            avg_white_pct = sum(race_whiteE)/sum(tot_popE)*100,\n            avg_black_pct = sum(race_blackE)/sum(tot_popE)*100,\n            avg_hispLatino_pct = sum(race_hispLatinoE)/sum(tot_popE)*100)\n\nkable(MOE_issue_stats, col.names = c(\"Confidence Category\", \"Avg Tract Pop\", \"Avg White Pct\", \"Avg Black Pct\", \"Avg Hispanic/Latino Pop\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nConfidence Category\nAvg Tract Pop\nAvg White Pct\nAvg Black Pct\nAvg Hispanic/Latino Pop\n\n\n\n\n1\n3383\n91.7\n0.816\n2.31\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\nThe selected Vermont counties - Addison, Essex, and Grand Isle - have such low proportions of Black and Hispanic/Latino residents that no tested census tract has a margin of error percent below 15 across all race categories (I ran the analysis with two additional counties and saw a similar pattern). Many of the census tracts even have margins of error greater than their corresponding estimates. This will inherently limit the viability of data for certain race categories within Vermont. The average population across all flagged census tract was just shy of 3400. Since every tract was flagged, this value is equal to the average population across all examined census tracts, indicating that Vermont’s low population outside of city centers also contributes to increased unreliability of estimates."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]\nFive of the fourteen counties in Vermont have a median household income higher greater than the estimate for median household income at the national level in 2022 ($74,580, according to the Census Bureau). When comparing margin of error percents (calculated as the margin of error divided by the estimate value), median household income estimates at the county level proved to be reasonably reliable, with nine of the fourteen counties having MOE percent values below 5 percent. However, race-categorized population estimates at the census tract level for non-white racial groups proved to have much larger MOE percent values - some as much as 300 percent of the estimate - which indicates far less reliable estimate values.\nMinority communities in Vermont face the greatest risk of algorithmic bias, as their populations are theoretically too low within the state for a survey such as the ACS5 to produce accurate, viable estimates for their populations. Any model built off of this data will have likely have a bias towards white residents of the state and further exacerbate disparities in social service funding and outreach program allocation.\nLow population in certain non-urban counties in Vermont and generally low population proportions of racial minorities in these counties are the largest contributing factors to data quality issues and bias risk in this analysis. Across all census tracts within Addison, Essex, and Grand Isle counties in Vermont, the total census tract population ranged from 1089 to 5197, a difference that likely contributed to varying qualities of each tract’s sample. Despite averaging samples over 5 years, the ACS5 still is a generally poor method of estimating very small values for specific variables.\nIt is recommended that more concrete data estimates for the populations of racial and ethnic minorities be obtained for the state of Vermont, which could be accomplished either through utilizing more accurate estimation methods (i.e. the Decennial Census), though this has limitations since our analysis year is 2022. Population values at the tract level from the Decennial Census between the years 2010 and 2020 could be used to project into 2030 in order to obtain values for 2022. Step-Down projection methods using population trends at the county or state level could also be utilized to improve predictions."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\ncounty_table &lt;- vt_data1_MOEpct %&gt;%\n  select(-c(GEOID, med_hh_incM, tot_popE, tot_popM)) %&gt;% \n  mutate(algthm_rec = case_when(med_hh_inc_conf == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n                                med_hh_inc_conf == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n                                med_hh_inc_conf == \"Low Confidence\" ~ \"Requires manual review or additional data\"))\n  \n\n# Format as a professional table with kable()\nkable(county_table,\n      col.names = c(\"County\", \"Median HH Income\", \"MOE Pct\", \"Confidence Rating\", \"Algorithm Recommendation\"),\n      format.args = list(round(3)))\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian HH Income\nMOE Pct\nConfidence Rating\nAlgorithm Recommendation\n\n\n\n\nAddison\n85870\n3.44\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBennington\n68558\n4.23\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCaledonia\n62964\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChittenden\n89494\n2.55\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n55247\n6.66\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n73633\n6.02\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGrand Isle\n86639\n12.38\nLow Confidence\nRequires manual review or additional data\n\n\nLamoille\n69886\n8.37\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nOrange\n74534\n3.64\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n63981\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRutland\n62641\n4.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n77278\n3.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWindham\n65473\n5.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWindsor\n69492\n4.24\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\n\nAddison\nBennington\nCaledonia\nChittenden\nOrange\nOrleans\nRutland\nWashington\nWindsor\n\nThese counties have a Margin of Error percent below 5, which indicates that their estimates are large enough relative to their Margin of Error as to be considered a stable, reasonably representative value of the population.\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\n\nEssex\nFranklin\nLamoille\nWindham\n\nSince these counties have a slightly larger MOE percent, they are less concrete of an estimate of median household income and thus could require supplemental information or intense monitoring of the results. This could take the form of an annual/biannual/monthly equity review to assess the demographics of individuals being served directly by any improvements to social service funding and outreach programs allocation.\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\n\nGrand Isle\n\nThis could take the form of a dedicated survey of household income in Vermont that is more comprehensive than that of the ACS5, or developing adjustments/weights to make up for the lack of information in Vermont based on similar states that have more reliable estimates."
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nWhat are the underlying contributing factors for the dominance of white residents in Vermont?\nAs a continuation of a previous suggestion: how would introducing something such as a population projection (inherent uncertainty) based on decennial census data (decently reliable in itself) contribute to/detract from this analysis?"
  },
  {
    "objectID": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-checklist",
    "href": "assignments/assignment1/Sywulak-Herr_Henry_assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(pacman)\np_load(tidyverse, tidycensus, tigris, sf, knitr)\n\n# Load spatial data\nfips_code_pa &lt;- 42\ndata_year &lt;- 2022\n\ncounties &lt;- counties(state = fips_code_pa,\n                     year = data_year,\n                     progress_bar = F)\ntracts &lt;- tracts(state = fips_code_pa,\n                 year = data_year,\n                 progress_bar = F)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = T)\n\ndata_list &lt;- mget(c(\"counties\", \"tracts\", \"hospitals\"))\n\n# Check that all data loaded correctly\npaste0(\"Hospital Count = \", nrow(hospitals))\n\n[1] \"Hospital Count = 223\"\n\npaste0(\"Census Tract Count = \", nrow(tracts))\n\n[1] \"Census Tract Count = 3446\"\n\ncrs_df &lt;- data.frame(Datum = sapply(data_list, function(x) {st_crs(x, parameters = T)$Name}),\n                     EPSG = sapply(data_list, function(x) {st_crs(x, parameters = T)$epsg}))\n\nkable(crs_df)\n\n\n\n\n\nDatum\nEPSG\n\n\n\n\ncounties\nNAD83\n4269\n\n\ntracts\nNAD83\n4269\n\n\nhospitals\nWGS 84\n4326\n\n\n\n\n\n\n\n\n\n\nvariable_names &lt;- tidycensus::load_variables(2022, \"acs5\")\n\n# Get demographic data from ACS\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\")\n\npop_over65_vars &lt;- c(\"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\",\n              \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\")\n\npopTot_medInc &lt;- get_acs(geography = \"tract\",\n                     variable = vars,\n                     year = data_year,\n                     state = fips_code_pa,\n                     output = \"wide\")\n\npop_over65 &lt;- get_acs(geography = \"tract\",\n                      variable = pop_over65_vars,\n                      year = data_year,\n                      state = fips_code_pa)\n\npop_over65_sum &lt;- pop_over65 %&gt;% \n  group_by(GEOID) %&gt;% \n  summarise(pop_elderlyE = sum(estimate),\n            pop_elderlyM = sum(moe))\n\ndemo_vars &lt;- left_join(popTot_medInc, pop_over65_sum, by = \"GEOID\")\n\n# Join to tract boundaries\ndemo_vars_sf &lt;- left_join(demo_vars, tracts %&gt;% select(GEOID), by = \"GEOID\") %&gt;% \n  st_as_sf()\n\n# Separate out Tract/County names\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n\n# Answers to questions\npaste0(\"ACS Year = \", data_year)\n\n[1] \"ACS Year = 2022\"\n\npaste0(\"Count of Tracts with Missing Income Data = \", sum(is.na(demo_vars_sf$med_hh_incE)))\n\n[1] \"Count of Tracts with Missing Income Data = 63\"\n\npaste0(\"Median Income across All PA Census Tracts = \", median(demo_vars_sf$med_hh_incE, na.rm = T))\n\n[1] \"Median Income across All PA Census Tracts = 70188\"\n\n\n\n\n\n\n\n# Filter for vulnerable tracts based on criteria\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(pop_elderly_pct = round((pop_elderlyE / pop_totE)*100, 2),\n         low_inc = as.integer(med_hh_incE &lt; 50000),\n         high_elderly = as.integer(pop_elderly_pct &gt; 16.8),\n         vulnerable = case_when(low_inc == 1 & high_elderly == 1 ~ 1,\n                                .default = 0))\n\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\nkable(head(vulnerable_tracts %&gt;% \n             select(TRACT, COUNTY, pop_totE, med_hh_incE, pop_elderlyE, pop_elderly_pct) %&gt;% \n             st_drop_geometry()),\n      col.names = c(\"Tract\", \"County\", \"Total Population\", \"Median HH Inc ($)\", \"Elderly Population\", \"Percent Elderly\"),\n      caption = \"Example tracts with low median household income and a high percentage of elderly residents:\")\n\n\nExample tracts with low median household income and a high percentage of elderly residents:\n\n\n\n\n\n\n\n\n\n\nTract\nCounty\nTotal Population\nMedian HH Inc ($)\nElderly Population\nPercent Elderly\n\n\n\n\n315.02\nAdams\n3908\n46109\n698\n17.86\n\n\n305.00\nAllegheny\n3044\n36932\n544\n17.87\n\n\n501.00\nAllegheny\n1561\n30036\n345\n22.10\n\n\n506.00\nAllegheny\n1960\n42951\n407\n20.77\n\n\n509.00\nAllegheny\n1380\n12740\n236\n17.10\n\n\n709.00\nAllegheny\n4196\n49421\n721\n17.18\n\n\n\n\n\nAccording to an article in Philadelphia Today, the household poverty threshold in 2023 was $30,000. While this is the official threshold - and likely higher than it was in 2022 - healthcare costs and a lack of consistent income post-retirement for elderly folks likely result in additional financial strain even if they have income levels slightly greater than $30,000. For this analysis, a low-income threshold of $50,000 was chosen instead to encompass these groups of residents.\nBetween 2010 and 2020, the people aged 65 and older in the United States exceeded 1 in 6 residents (16.8%). This percentage was used as the threshold for having an abnormally high elderly population.\n\n# count of tracts that meet vulnerability critetia\npaste0(\"Count of Vulnerable Tracts = \", nrow(vulnerable_tracts))\n\n[1] \"Count of Vulnerable Tracts = 258\"\n\n# percent of census tracts considered vulnerable\npaste0(\"Percentage of PA Tracts Considered Vulnerable = \", round((nrow(vulnerable_tracts)/nrow(demo_vars_sf))*100, 2), \"%\")\n\n[1] \"Percentage of PA Tracts Considered Vulnerable = 7.49%\"\n\n\n\n\n\n\nA projected coordinate system centered around southern Pennsylvania (EPSG 2272) was chosen for this analysis in order to accurately calculate distances to the nearest hospital, unlike the original geographic coordinate systems each layer was in previously which are in units of decimal degrees. This coordinate system is based on the NAD83 datum and has units of US Survey Feet.\n\n# Transform to appropriate projected CRS\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  st_transform(2272)\nhospitals &lt;- hospitals %&gt;% \n  st_transform(2272)\n\n# Calculate distance matrix for each tract centroid to the nearest hospital\nhospital_dist_matrix &lt;- demo_vars_sf %&gt;%\n  st_centroid() %&gt;% \n  st_distance(hospitals)\n\nhospital_dist_min &lt;- apply(hospital_dist_matrix, 1, min)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  mutate(hospital_dist = round((hospital_dist_min)/5280, 2))\n\n# Recreate vulnerable tracts df with hospital_dist variable\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\n# Average distance to hospitals\npaste0(\"Average Distance from Vulnerable Tracts to the Nearest Hospital = \",\n       round(mean(vulnerable_tracts$hospital_dist), 2), \n       \" mi\")\n\n[1] \"Average Distance from Vulnerable Tracts to the Nearest Hospital = 3.24 mi\"\n\n# Maximum distance to hospitals\npaste0(\"Maximum Distance from Tract Centroids to Nearest Hospital = \",\n       max(vulnerable_tracts$hospital_dist),\n       \" mi\")\n\n[1] \"Maximum Distance from Tract Centroids to Nearest Hospital = 18.67 mi\"\n\n# Count of vulnerable tracts more than 15 mi from the nearest hospital\npaste0(\"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = \",\n       sum(vulnerable_tracts$hospital_dist &gt; 15))\n\n[1] \"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = 7\"\n\n\n\n\n\n\n“Underserved” tracts are defined here as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\n# Number of underserved tracts\npaste0(\"Number of Underserved Tracts = \", sum(vulnerable_tracts$underserved))\n\n[1] \"Number of Underserved Tracts = 7\"\n\n# Percentage of vulnerable tracts that are underserved\npaste0(\"Percentage of Vulnerable Tracts that are Underserved = \",\n       round(sum(vulnerable_tracts$underserved)/nrow(vulnerable_tracts)*100, 2),\n       \"%\")\n\n[1] \"Percentage of Vulnerable Tracts that are Underserved = 2.71%\"\n\n\nThe percentage of vulnerable tracts that are underserved is rather low in Pennsylvania, which is surprising. Pennsylvania has many rural communities with sparsely distributed resources, including hospitals. The distribution of underserved census tracts in Pennsylvania do appear to be in rural, central PA counties (see the table below for the counties associated with these seven tracts); however, it is puzzling how few of them there are.\n\n# Table of the counties associated with the seven vulnerable tracts that are underserved in PA\nkable(vulnerable_tracts %&gt;% \n        filter(underserved == 1) %&gt;%\n        select(TRACT, COUNTY) %&gt;%\n        st_drop_geometry(),\n      col.names = c(\"Tract\", \"County\"),\n      caption = \"Underserved Vulnerable Tracts in PA\")\n\n\nUnderserved Vulnerable Tracts in PA\n\n\nTract\nCounty\n\n\n\n\n9601.00\nCameron\n\n\n3306.00\nClearfield\n\n\n3316.00\nClearfield\n\n\n5301.00\nForest\n\n\n5302.00\nForest\n\n\n702.01\nJuniata\n\n\n3003.11\nMonroe\n\n\n\n\n\n\n\n\n\n\n# Spatial join tracts to counties\n# Spatial joins were not used because spatial inconsistencies between the layers were producing duplicate instances of tracts across multiple counties despite everything being in the same crs. Tabular joins were more effective.\n\ncounties &lt;- counties %&gt;% \n  st_transform(2272)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\ndemo_vars &lt;- demo_vars_sf %&gt;% \n  st_drop_geometry()\n\ndemo_vars_cty &lt;- left_join(counties %&gt;% select(GEOID, NAME),\n                         demo_vars, by = c(\"NAME\" = \"COUNTY\"))\n\n# Aggregate statistics by county\ndemo_vars_cty_stats &lt;- demo_vars_cty %&gt;% \n  group_by(NAME) %&gt;% \n  summarise(pop_tot_agg = sum(pop_totE, na.rm = T),\n            med_hh_inc_agg = median(med_hh_incE, na.rm = T), \n            pop_elderly_agg = sum(pop_elderlyE, na.rm = T),\n            low_inc_cnt = sum(low_inc, na.rm = T),\n            vulnerable_cnt = sum(vulnerable, na.rm = T),\n            underserved_cnt = sum(underserved, na.rm = T),\n            hospital_dist_avg = round(mean(hospital_dist, na.rm = T), 2),\n            pop_vulnerable = sum(pop_elderlyE*vulnerable)) %&gt;% \n  mutate(pop_elderly_pct_agg = round((pop_elderly_agg/pop_tot_agg)*100, 2),\n         underserved_pct = round((underserved_cnt/vulnerable_cnt)*100, 2))\n\nAfter conducting this analysis, there are only 5 counties that have underserved census tracts: Cameron, Clearfield, Forest, Juniata, and Monroe.\n\nunderserved_pct_top &lt;- demo_vars_cty_stats %&gt;%\n  select(NAME, underserved_pct) %&gt;% \n  arrange(-underserved_pct) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(underserved_pct_top,\n      col.names = c(\"County\", \"% of Vulnerable Tracts Underserved\"))\n\n\n\n\nCounty\n% of Vulnerable Tracts Underserved\n\n\n\n\nCameron\n100.00\n\n\nForest\n100.00\n\n\nMonroe\n100.00\n\n\nClearfield\n66.67\n\n\nJuniata\n50.00\n\n\n\n\n\nThere are also only five counties that have an average distance to the nearest hospital greater than 15 miles: Cameron, Forest, Juniata, Pike, and Sullivan. However, due to the nature of aggregating tract-level data to the county level, Pike and Sullivan have no vulnerable tracts and thus have no vulnerable population (which was calculated by isolating tracts considered vulnerable and summing their elderly population). If considering living far from hospitals to include any distance greater than 10 miles instead of 15, the top five counties with the greatest vulnerable population are:\n\nvulnerable_pop_top &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;%\n  arrange(-pop_vulnerable) %&gt;% \n  select(NAME, pop_vulnerable) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(vulnerable_pop_top, col.names = c(\"County\", \"Vulnerable Population\"))\n\n\n\n\nCounty\nVulnerable Population\n\n\n\n\nClearfield\n2083\n\n\nForest\n1593\n\n\nJuniata\n1332\n\n\nBradford\n751\n\n\nCameron\n428\n\n\n\n\n\nThere is a pattern that counties with underserved elderly populations typically lie in the more rural areas of central and northeastern Pennsylvania, far from major city centers like Philadelphia and Pittsburgh that have high concentrations of concentrated hospitals and medical services.\n\ndemo_vars_cty_stats &lt;- demo_vars_cty_stats %&gt;% \n  mutate(underserved_cnt_factor = as.factor(underserved_cnt))\nplot(demo_vars_cty_stats[,\"underserved_cnt_factor\"],\n     pal = c(\"grey\", \"green4\", \"steelblue\"),\n     main = \"Count of Underserved Census Tracts\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop10_priority &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;% \n  arrange(-pop_vulnerable) %&gt;% \n  head(10) %&gt;% \n  select(NAME, pop_tot_agg, med_hh_inc_agg, pop_elderly_pct_agg, pop_vulnerable, hospital_dist_avg) %&gt;% \n  st_drop_geometry()\n\n# Create and format priority counties table\nkable(top10_priority,\n      col.names = c(\"County\", \"Total Population\", \"Median HH Income ($)\", \"% Elderly Population\", \"Vulnerable Population\", \"Mean Distance to Nearest Hospital (mi)\"), format.args = list(big.mark = \",\"),\n      caption = \"Top Ten Counties to Target for Healthcare Investment\")\n\n\nTop Ten Counties to Target for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population\nMedian HH Income ($)\n% Elderly Population\nVulnerable Population\nMean Distance to Nearest Hospital (mi)\n\n\n\n\nClearfield\n79,707\n54,033.5\n21.07\n2,083\n13.13\n\n\nForest\n6,959\n45,728.0\n22.89\n1,593\n18.40\n\n\nJuniata\n23,535\n64,173.0\n20.56\n1,332\n15.01\n\n\nBradford\n60,159\n58,866.0\n21.74\n751\n10.18\n\n\nCameron\n4,536\n45,279.5\n27.98\n428\n19.07\n\n\nMonroe\n168,128\n81,127.0\n18.27\n314\n10.05\n\n\nPerry\n45,941\n74,562.5\n19.37\n0\n13.26\n\n\nPike\n58,996\n79,479.5\n22.99\n0\n19.22\n\n\nPotter\n16,390\n54,904.0\n24.87\n0\n11.02\n\n\nSnyder\n39,797\n66,042.0\n19.83\n0\n14.47"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\n# Load required packages\nlibrary(pacman)\np_load(tidyverse, tidycensus, tigris, sf, knitr)\n\n# Load spatial data\nfips_code_pa &lt;- 42\ndata_year &lt;- 2022\n\ncounties &lt;- counties(state = fips_code_pa,\n                     year = data_year,\n                     progress_bar = F)\ntracts &lt;- tracts(state = fips_code_pa,\n                 year = data_year,\n                 progress_bar = F)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = T)\n\ndata_list &lt;- mget(c(\"counties\", \"tracts\", \"hospitals\"))\n\n# Check that all data loaded correctly\npaste0(\"Hospital Count = \", nrow(hospitals))\n\n[1] \"Hospital Count = 223\"\n\npaste0(\"Census Tract Count = \", nrow(tracts))\n\n[1] \"Census Tract Count = 3446\"\n\ncrs_df &lt;- data.frame(Datum = sapply(data_list, function(x) {st_crs(x, parameters = T)$Name}),\n                     EPSG = sapply(data_list, function(x) {st_crs(x, parameters = T)$epsg}))\n\nkable(crs_df)\n\n\n\n\n\nDatum\nEPSG\n\n\n\n\ncounties\nNAD83\n4269\n\n\ntracts\nNAD83\n4269\n\n\nhospitals\nWGS 84\n4326\n\n\n\n\n\n\n\n\n\n\nvariable_names &lt;- tidycensus::load_variables(2022, \"acs5\")\n\n# Get demographic data from ACS\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\")\n\npop_over65_vars &lt;- c(\"B01001_020\", \"B01001_021\", \"B01001_022\", \"B01001_023\", \"B01001_024\", \"B01001_025\",\n              \"B01001_044\", \"B01001_045\", \"B01001_046\", \"B01001_047\", \"B01001_048\", \"B01001_049\")\n\npopTot_medInc &lt;- get_acs(geography = \"tract\",\n                     variable = vars,\n                     year = data_year,\n                     state = fips_code_pa,\n                     output = \"wide\")\n\npop_over65 &lt;- get_acs(geography = \"tract\",\n                      variable = pop_over65_vars,\n                      year = data_year,\n                      state = fips_code_pa)\n\npop_over65_sum &lt;- pop_over65 %&gt;% \n  group_by(GEOID) %&gt;% \n  summarise(pop_elderlyE = sum(estimate),\n            pop_elderlyM = sum(moe))\n\ndemo_vars &lt;- left_join(popTot_medInc, pop_over65_sum, by = \"GEOID\")\n\n# Join to tract boundaries\ndemo_vars_sf &lt;- left_join(demo_vars, tracts %&gt;% select(GEOID), by = \"GEOID\") %&gt;% \n  st_as_sf()\n\n# Separate out Tract/County names\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n\n# Answers to questions\npaste0(\"ACS Year = \", data_year)\n\n[1] \"ACS Year = 2022\"\n\npaste0(\"Count of Tracts with Missing Income Data = \", sum(is.na(demo_vars_sf$med_hh_incE)))\n\n[1] \"Count of Tracts with Missing Income Data = 63\"\n\npaste0(\"Median Income across All PA Census Tracts = \", median(demo_vars_sf$med_hh_incE, na.rm = T))\n\n[1] \"Median Income across All PA Census Tracts = 70188\"\n\n\n\n\n\n\n\n# Filter for vulnerable tracts based on criteria\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(pop_elderly_pct = round((pop_elderlyE / pop_totE)*100, 2),\n         low_inc = as.integer(med_hh_incE &lt; 50000),\n         high_elderly = as.integer(pop_elderly_pct &gt; 16.8),\n         vulnerable = case_when(low_inc == 1 & high_elderly == 1 ~ 1,\n                                .default = 0))\n\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\nkable(head(vulnerable_tracts %&gt;% \n             select(TRACT, COUNTY, pop_totE, med_hh_incE, pop_elderlyE, pop_elderly_pct) %&gt;% \n             st_drop_geometry()),\n      col.names = c(\"Tract\", \"County\", \"Total Population\", \"Median HH Inc ($)\", \"Elderly Population\", \"Percent Elderly\"),\n      caption = \"Example tracts with low median household income and a high percentage of elderly residents:\")\n\n\nExample tracts with low median household income and a high percentage of elderly residents:\n\n\n\n\n\n\n\n\n\n\nTract\nCounty\nTotal Population\nMedian HH Inc ($)\nElderly Population\nPercent Elderly\n\n\n\n\n315.02\nAdams\n3908\n46109\n698\n17.86\n\n\n305.00\nAllegheny\n3044\n36932\n544\n17.87\n\n\n501.00\nAllegheny\n1561\n30036\n345\n22.10\n\n\n506.00\nAllegheny\n1960\n42951\n407\n20.77\n\n\n509.00\nAllegheny\n1380\n12740\n236\n17.10\n\n\n709.00\nAllegheny\n4196\n49421\n721\n17.18\n\n\n\n\n\nAccording to an article in Philadelphia Today, the household poverty threshold in 2023 was $30,000. While this is the official threshold - and likely higher than it was in 2022 - healthcare costs and a lack of consistent income post-retirement for elderly folks likely result in additional financial strain even if they have income levels slightly greater than $30,000. For this analysis, a low-income threshold of $50,000 was chosen instead to encompass these groups of residents.\nBetween 2010 and 2020, the people aged 65 and older in the United States exceeded 1 in 6 residents (16.8%). This percentage was used as the threshold for having an abnormally high elderly population.\n\n# count of tracts that meet vulnerability critetia\npaste0(\"Count of Vulnerable Tracts = \", nrow(vulnerable_tracts))\n\n[1] \"Count of Vulnerable Tracts = 258\"\n\n# percent of census tracts considered vulnerable\npaste0(\"Percentage of PA Tracts Considered Vulnerable = \", round((nrow(vulnerable_tracts)/nrow(demo_vars_sf))*100, 2), \"%\")\n\n[1] \"Percentage of PA Tracts Considered Vulnerable = 7.49%\"\n\n\n\n\n\n\nA projected coordinate system centered around southern Pennsylvania (EPSG 2272) was chosen for this analysis in order to accurately calculate distances to the nearest hospital, unlike the original geographic coordinate systems each layer was in previously which are in units of decimal degrees. This coordinate system is based on the NAD83 datum and has units of US Survey Feet.\n\n# Transform to appropriate projected CRS\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  st_transform(2272)\nhospitals &lt;- hospitals %&gt;% \n  st_transform(2272)\n\n# Calculate distance matrix for each tract centroid to the nearest hospital\nhospital_dist_matrix &lt;- demo_vars_sf %&gt;%\n  st_centroid() %&gt;% \n  st_distance(hospitals)\n\nhospital_dist_min &lt;- apply(hospital_dist_matrix, 1, min)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;% \n  mutate(hospital_dist = round((hospital_dist_min)/5280, 2))\n\n# Recreate vulnerable tracts df with hospital_dist variable\nvulnerable_tracts &lt;- demo_vars_sf %&gt;% filter(vulnerable == 1)\n\n# Average distance to hospitals\npaste0(\"Average Distance from Vulnerable Tracts to the Nearest Hospital = \",\n       round(mean(vulnerable_tracts$hospital_dist), 2), \n       \" mi\")\n\n[1] \"Average Distance from Vulnerable Tracts to the Nearest Hospital = 3.24 mi\"\n\n# Maximum distance to hospitals\npaste0(\"Maximum Distance from Tract Centroids to Nearest Hospital = \",\n       max(vulnerable_tracts$hospital_dist),\n       \" mi\")\n\n[1] \"Maximum Distance from Tract Centroids to Nearest Hospital = 18.67 mi\"\n\n# Count of vulnerable tracts more than 15 mi from the nearest hospital\npaste0(\"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = \",\n       sum(vulnerable_tracts$hospital_dist &gt; 15))\n\n[1] \"Number of Vulnerable Tracts &gt;15 mi from the Nearest Hospital = 7\"\n\n\n\n\n\n\n“Underserved” tracts are defined here as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\n# Number of underserved tracts\npaste0(\"Number of Underserved Tracts = \", sum(vulnerable_tracts$underserved))\n\n[1] \"Number of Underserved Tracts = 7\"\n\n# Percentage of vulnerable tracts that are underserved\npaste0(\"Percentage of Vulnerable Tracts that are Underserved = \",\n       round(sum(vulnerable_tracts$underserved)/nrow(vulnerable_tracts)*100, 2),\n       \"%\")\n\n[1] \"Percentage of Vulnerable Tracts that are Underserved = 2.71%\"\n\n\nThe percentage of vulnerable tracts that are underserved is rather low in Pennsylvania, which is surprising. Pennsylvania has many rural communities with sparsely distributed resources, including hospitals. The distribution of underserved census tracts in Pennsylvania do appear to be in rural, central PA counties (see the table below for the counties associated with these seven tracts); however, it is puzzling how few of them there are.\n\n# Table of the counties associated with the seven vulnerable tracts that are underserved in PA\nkable(vulnerable_tracts %&gt;% \n        filter(underserved == 1) %&gt;%\n        select(TRACT, COUNTY) %&gt;%\n        st_drop_geometry(),\n      col.names = c(\"Tract\", \"County\"),\n      caption = \"Underserved Vulnerable Tracts in PA\")\n\n\nUnderserved Vulnerable Tracts in PA\n\n\nTract\nCounty\n\n\n\n\n9601.00\nCameron\n\n\n3306.00\nClearfield\n\n\n3316.00\nClearfield\n\n\n5301.00\nForest\n\n\n5302.00\nForest\n\n\n702.01\nJuniata\n\n\n3003.11\nMonroe\n\n\n\n\n\n\n\n\n\n\n# Spatial join tracts to counties\n# Spatial joins were not used because spatial inconsistencies between the layers were producing duplicate instances of tracts across multiple counties despite everything being in the same crs. Tabular joins were more effective.\n\ncounties &lt;- counties %&gt;% \n  st_transform(2272)\n\ndemo_vars_sf &lt;- demo_vars_sf %&gt;%\n  mutate(underserved = case_when((vulnerable == 1 & hospital_dist &gt; 15) ~ 1,\n                                 .default = 0))\n\ndemo_vars &lt;- demo_vars_sf %&gt;% \n  st_drop_geometry()\n\ndemo_vars_cty &lt;- left_join(counties %&gt;% select(GEOID, NAME),\n                         demo_vars, by = c(\"NAME\" = \"COUNTY\"))\n\n# Aggregate statistics by county\ndemo_vars_cty_stats &lt;- demo_vars_cty %&gt;% \n  group_by(NAME) %&gt;% \n  summarise(pop_tot_agg = sum(pop_totE, na.rm = T),\n            med_hh_inc_agg = median(med_hh_incE, na.rm = T), \n            pop_elderly_agg = sum(pop_elderlyE, na.rm = T),\n            low_inc_cnt = sum(low_inc, na.rm = T),\n            vulnerable_cnt = sum(vulnerable, na.rm = T),\n            underserved_cnt = sum(underserved, na.rm = T),\n            hospital_dist_avg = round(mean(hospital_dist, na.rm = T), 2),\n            pop_vulnerable = sum(pop_elderlyE*vulnerable)) %&gt;% \n  mutate(pop_elderly_pct_agg = round((pop_elderly_agg/pop_tot_agg)*100, 2),\n         underserved_pct = round((underserved_cnt/vulnerable_cnt)*100, 2))\n\nAfter conducting this analysis, there are only 5 counties that have underserved census tracts: Cameron, Clearfield, Forest, Juniata, and Monroe.\n\nunderserved_pct_top &lt;- demo_vars_cty_stats %&gt;%\n  select(NAME, underserved_pct) %&gt;% \n  arrange(-underserved_pct) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(underserved_pct_top,\n      col.names = c(\"County\", \"% of Vulnerable Tracts Underserved\"))\n\n\n\n\nCounty\n% of Vulnerable Tracts Underserved\n\n\n\n\nCameron\n100.00\n\n\nForest\n100.00\n\n\nMonroe\n100.00\n\n\nClearfield\n66.67\n\n\nJuniata\n50.00\n\n\n\n\n\nThere are also only five counties that have an average distance to the nearest hospital greater than 15 miles: Cameron, Forest, Juniata, Pike, and Sullivan. However, due to the nature of aggregating tract-level data to the county level, Pike and Sullivan have no vulnerable tracts and thus have no vulnerable population (which was calculated by isolating tracts considered vulnerable and summing their elderly population). If considering living far from hospitals to include any distance greater than 10 miles instead of 15, the top five counties with the greatest vulnerable population are:\n\nvulnerable_pop_top &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;%\n  arrange(-pop_vulnerable) %&gt;% \n  select(NAME, pop_vulnerable) %&gt;% \n  head(5) %&gt;% \n  st_drop_geometry()\n\nkable(vulnerable_pop_top, col.names = c(\"County\", \"Vulnerable Population\"))\n\n\n\n\nCounty\nVulnerable Population\n\n\n\n\nClearfield\n2083\n\n\nForest\n1593\n\n\nJuniata\n1332\n\n\nBradford\n751\n\n\nCameron\n428\n\n\n\n\n\nThere is a pattern that counties with underserved elderly populations typically lie in the more rural areas of central and northeastern Pennsylvania, far from major city centers like Philadelphia and Pittsburgh that have high concentrations of concentrated hospitals and medical services.\n\ndemo_vars_cty_stats &lt;- demo_vars_cty_stats %&gt;% \n  mutate(underserved_cnt_factor = as.factor(underserved_cnt))\nplot(demo_vars_cty_stats[,\"underserved_cnt_factor\"],\n     pal = c(\"grey\", \"green4\", \"steelblue\"),\n     main = \"Count of Underserved Census Tracts\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ntop10_priority &lt;- demo_vars_cty_stats %&gt;% \n  filter(hospital_dist_avg &gt; 10) %&gt;% \n  arrange(-pop_vulnerable) %&gt;% \n  head(10) %&gt;% \n  select(NAME, pop_tot_agg, med_hh_inc_agg, pop_elderly_pct_agg, pop_vulnerable, hospital_dist_avg) %&gt;% \n  st_drop_geometry()\n\n# Create and format priority counties table\nkable(top10_priority,\n      col.names = c(\"County\", \"Total Population\", \"Median HH Income ($)\", \"% Elderly Population\", \"Vulnerable Population\", \"Mean Distance to Nearest Hospital (mi)\"), format.args = list(big.mark = \",\"),\n      caption = \"Top Ten Counties to Target for Healthcare Investment\")\n\n\nTop Ten Counties to Target for Healthcare Investment\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population\nMedian HH Income ($)\n% Elderly Population\nVulnerable Population\nMean Distance to Nearest Hospital (mi)\n\n\n\n\nClearfield\n79,707\n54,033.5\n21.07\n2,083\n13.13\n\n\nForest\n6,959\n45,728.0\n22.89\n1,593\n18.40\n\n\nJuniata\n23,535\n64,173.0\n20.56\n1,332\n15.01\n\n\nBradford\n60,159\n58,866.0\n21.74\n751\n10.18\n\n\nCameron\n4,536\n45,279.5\n27.98\n428\n19.07\n\n\nMonroe\n168,128\n81,127.0\n18.27\n314\n10.05\n\n\nPerry\n45,941\n74,562.5\n19.37\n0\n13.26\n\n\nPike\n58,996\n79,479.5\n22.99\n0\n19.22\n\n\nPotter\n16,390\n54,904.0\n24.87\n0\n11.02\n\n\nSnyder\n39,797\n66,042.0\n19.83\n0\n14.47"
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\n\nggplot() +\n  geom_sf(\n    data = demo_vars_cty_stats,\n    aes(fill = underserved_pct),\n    color = \"transparent\"\n    ) +\n  scale_fill_binned(\n    name = \"Underserved Pct (%)\",\n    breaks = seq(0, 100, by = 20),\n    low = \"#e5f5e0\",\n    high = \"#006d2c\",\n    na.value = \"grey75\"\n    ) +\n  geom_sf(data = hospitals, aes(color = \"Hospitals\"), size = 0.75) +\n  scale_color_manual(name = \"Hospital Locations\",\n                     values = c(\"Hospitals\" = \"black\")\n                     ) +\n  theme_void() +\n  labs(title = \"Percent of Census Tracts Classified as Underserved Within a County\",\n       subtitle = \"Relative to Hospital Locations\")\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\n# Create detailed tract-level map\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            mutate(fill_category = case_when(\n              underserved == 1 ~ \"Underserved\",\n              vulnerable == 1 ~ \"Vulnerable\",\n              vulnerable == 0 ~ \"Not Vulnerable\",\n              .default = \"No Data\"\n            )),\n          aes(fill = factor(fill_category, levels = c(\"Underserved\",\n                                                      \"Vulnerable\",\n                                                      \"Not Vulnerable\")\n                            )\n              ),\n          color = \"transparent\") +\n  scale_fill_manual(name = \"Underserved | Vulnerability\",\n                    values = c(\"Underserved\" = \"red3\",\n                               \"Vulnerable\" = \"gold2\",\n                               \"Not Vulnerable\" = \"grey75\",\n                               \"No Data\" = \"grey65\")\n                    ) +\n  geom_sf(data = counties,\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = hospitals,\n          aes(color = \"Hospitals\"),\n          size = 0.75) +\n   scale_color_manual(name = \"Hospital Locations\",\n                     values = c(\"Hospitals\" = \"black\")\n                     ) +\n  theme_void() +\n  labs(title = \"Spatial Comparison of Underserved and Vulnerable Census Tracts\",\n       subtitle = \"Relative to County Boundaries and Hospitals\")\n\n\n\n\n\n\n\n\n\n\n\nChart 1: Distribution Analysis\n\n# Create distribution visualization\nggplot() +\n  geom_histogram(data = demo_vars_sf %&gt;% filter(vulnerable == 1),\n                 aes(hospital_dist),\n                 bins = 50,\n                 na.rm = T, fill = \"steelblue\",\n                 colour = \"grey30\") +\n  labs(title = \"Histogram Distribution of Distances to the Nearest Hospital\",\n       subtitle = \"From Census Tract Centroids\",\n       x = \"Distance to the Nearest Hospital (mi)\",\n       y = \"Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe distribution is right-skewed with a majority of census tract centroids lying within 4-5 miles of the closest hospital. The maximum distance from a hospital is slightly less than 20 miles, and there are a considerable number of outliers between 5 and 20 miles."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-3-additional-data-analysis",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#part-3-additional-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Additional Data Analysis",
    "text": "Part 3: Additional Data Analysis\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nAnalysis\n\n# Load additional dataset(s)\npop_phl &lt;- demo_vars_sf %&gt;%\n  filter(COUNTY == \"Philadelphia\") %&gt;% \n  select(GEOID, TRACT, COUNTY, STATE, pop_totE, pop_totM)\n\nfirehouses &lt;- st_read(\"./data/Fire_Dept_Facilities.geojson\", quiet = T) %&gt;% \n  st_transform(st_crs(pop_phl))\n\npaste0(\"Number of Fire Stations in PHL = \", nrow(firehouses))\n\n[1] \"Number of Fire Stations in PHL = 69\"\n\n\nThe dataset loaded above was sourced from OpenDataPhilly’s Fire Department Facilities dataset, created in 2014. Despite the age of the dataset, it’s assumed that fire stations are relatively permanent fixtures in communities and likely have persisted into the year of the population data (2022). Through this assumption, this spatial analysis will seek to illustrate how well/poorly fire stations in Philadelphia are adapted to the current population patterns within the city.\nThe firehouse dataset was originally in a geographic WGS84 coordinate system (EPSG 4326), which necessitated transforming it to a projected coordinate system. The CRS used in the previous analysis (EPSG 2272) was intended for use in Southern Pennsylvania, and so the CRS from the population dataset was called instead of using the EPSG code to ensure they’d be the same.\n\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            filter(COUNTY == \"Philadelphia\") %&gt;%\n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey75\") +\n  geom_sf(data = demo_vars_cty %&gt;% \n            filter(NAME == \"Philadelphia\") %&gt;% \n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = firehouses,\n          aes(color = \"Firehouses\"),\n          size = 3) +\n  scale_color_manual(name = \"\",\n                     values = c(\"Firehouses\"= \"red3\")) +\n  theme_void() +\n  labs(title = \"Firehouse Locations in Philadelphia\",\n       subtitle = \"Relative to Census Tracts\")\n\n\n\n\n\n\n\n\nNote: one fire station is located outside the southern boundary of Philadelphia County. This is likely a fire station dedicated to serving the Philadelphia International Airport, since most of the airport lies within Delaware County. This airport will be omitted from this analysis partially due to its location, but also because it likely primarily (or almost exclusively) serves the airport, and from past experience analyzing this area the population density is far lower due a high concentration of both airport and industrial land uses.\n\n# remove firehouses outside the boundary of Philadelphia County\nfirehouses &lt;- st_filter(firehouses, demo_vars_cty %&gt;% filter(NAME == \"Philadelphia\"))\n\n\nPose a research question\nDoes the distribution of fire stations in Philadelphia as registered in 2014 (assumed to be the same, if not similar, to that of today) adequately cover the population of the city as of 2022? Which fire stations serve the lowest population density and where are they located in the city?\n\nConduct spatial analysis\n\n# Create a two mile buffer zone around every fire station in Philadelphia County\nfirehouses_2mi &lt;- st_buffer(firehouses, dist = (2*5280))\n\n# Calculate areas of census tracts to perform an areal interpolation\npop_phl &lt;- pop_phl %&gt;% \n  mutate(area_tract = as.numeric(st_area(.)))\n\n# perform an intersection to get every partial overlap of each buffer with the surrounding census tracts\n# calculate the areas of these intersections and get their area, then divide by the area of the corresponding census tract to get an area ratio\n# multiply this ratio to the total population of the census tract to get a partial population count for each intersection\n# NOTE: assumes even population distribution across every census tract\nfirehouses_int &lt;- st_intersection(firehouses_2mi, pop_phl) %&gt;% \n  mutate(area_int = as.numeric(st_area(.)),\n         area_frac = area_int/area_tract,\n         pop_int = pop_totE * area_frac)\n\n# sum population fractions to each buffer to get the total serviced population\nfirehouses_sum &lt;- firehouses_int %&gt;% \n  group_by(objectid) %&gt;% \n  summarise(pop_served = sum(pop_int))\n\n# join results back to the point dataset to map\nfirehouses &lt;- left_join(firehouses, firehouses_sum %&gt;% st_drop_geometry(), by = \"objectid\")\n\nfirehouses_stats &lt;- firehouses %&gt;%\n  st_drop_geometry %&gt;%\n  summarise(Minimum = min(pop_served, na.rm = T),\n            Mean = mean(pop_served, na.rm = T),\n            Median = median(pop_served, na.rm = T),\n            Maximum = max(pop_served, na.rm = T),\n            Total = sum(pop_served, na.rm = T)) %&gt;% \n  kable(digits = 2,\n        format.args = list(big.mark = \",\"),\n        caption = \"Summary Statistics of Population Served by Firehouses in Philadelphia\")\n\nfirehouses_stats\n\n\nSummary Statistics of Population Served by Firehouses in Philadelphia\n\n\nMinimum\nMean\nMedian\nMaximum\nTotal\n\n\n\n\n26,670.02\n161,730.4\n176,151.6\n276,796.7\n10,997,666\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = demo_vars_sf %&gt;% \n            filter(COUNTY == \"Philadelphia\") %&gt;%\n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey75\") +\n  geom_sf(data = demo_vars_cty %&gt;% \n            filter(NAME == \"Philadelphia\") %&gt;% \n            select(geometry),\n          fill = \"transparent\",\n          color = \"grey35\") +\n  geom_sf(data = firehouses %&gt;% mutate(pop_10k = pop_served/10000),\n          aes(fill = pop_10k),\n          shape = 21,\n          color = \"grey25\",\n          size = 3) +\n  scale_fill_binned(\n    name = \"Population Served (10k)\",\n    breaks = seq(0, 28, by = 7),\n    limits = c(0, 28),\n    low = \"#EEE5E9\",\n    high = \"red3\",\n    na.value = \"grey75\"\n    ) +\n  theme_void() +\n  labs(title = \"Total Population Served by Fire Stations in Philadelphia\",\n       subtitle = \"Station Locations: 2014, Population Data: 2022\")\n\n\n\n\n\n\n\n\nInterpretation of Findings:\nAccording to the ACS5, the population in Philadelphia County is approximately 1,593,000 people. If every fire station served the median number of people served (176,000 people) and Philadelphia’s population were evenly distributed throughout the city, there would theoretically only need to be 10 fire stations to cover the entirety of the city’s population. This isn’t the whole picture, however, as with all emergency services redundancy and spatial proximity to residents is also critical towards ensuring that all everying is served adequately and rapidly in the event of a fire.\nFire stations in Philadelphia are spread throughout the city, with greater concentrations in Center City and North Philadelphia. The greatest populations served are also in these areas. As fire stations get closer to the border of the county the population served decreases. This is likely due to edge effects where populations in surrounding counties (Montgomery, Delaware, etc.) are not being counted despite these stations still likely assisting them. On the eastern border of the county, this is due to the Delaware River and not due to the data restrictions set at the beginning of this analysis.\nVisually, as the Navy Yard continues to see development in the coming decades and as population density in the far Northeast continues to increase, it might be a good policy decision in the future to encourage the creation of additional fire stations in these areas in order to relieve burden from surrounding stations and potentially improve response times. Further analysis could investigate average response times and/or quantify the burden currently on each station to see where additional stations/resources are needed."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#comments-about-incorporation-of-feedback",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#comments-about-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Comments about incorporation of feedback:",
    "text": "Comments about incorporation of feedback:\nAdditional care was taken to clean up this document, removing instruction text and changing the wording of certain headings. All tables and charts that are produced in the report are intended to have a purpose/serve the narrative of the report. Tweaks were made to the classification criteria at certain points (i.e. changing the threshold of hospital distance from 15 miles to 10 miles) in order to achieve a more insightful result."
  },
  {
    "objectID": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#submission-requirements",
    "href": "assignments/assignment2/Sywulak-Herr_Henry_assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Code\n# load all packages for analysis\nlibrary(pacman)\np_load(tidyverse, lubridate, readxl,\n       sf, tigris, tidycensus, osmdata,\n       riem,\n       viridis, gridExtra, knitr, kableExtra, patchwork)\n\ncolors &lt;- c(\"#93d500\", \"#0082ca\", \"#002169\", \"#e6e6e6\")\n\n\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-packages-themes",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-packages-themes",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Code\n# load all packages for analysis\nlibrary(pacman)\np_load(tidyverse, lubridate, readxl,\n       sf, tigris, tidycensus, osmdata,\n       riem,\n       viridis, gridExtra, knitr, kableExtra, patchwork)\n\ncolors &lt;- c(\"#93d500\", \"#0082ca\", \"#002169\", \"#e6e6e6\")\n\n\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-trips-data",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-trips-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Trips Data",
    "text": "Load Trips Data\n\n\nCode\n# save path to data files\npath &lt;- \"./data/indego-trips/\"\n\n# load all trip csvs for the four quarters\nindego &lt;- lapply(list.files(path, full.names = T),\n                 function(x) {\n                   data &lt;- read_csv(file = x, show_col_types = F)\n                   data &lt;- data %&gt;%\n                     mutate(quarter = sub(\".*-(q[0-9])\\\\.csv$\", \"\\\\1\", x))\n                   return(data)\n                   }\n                 ) %&gt;% do.call(rbind, .)\n\noriginal_trip_total &lt;- nrow(indego)\n\n\nFrom past experience working with Indego bikeshare data and referencing the active station table from their data portal (link), there are many trips that are logged as starting/ending at what’s known as a virtual station, which “is used by staff to check in or check out a bike remotely for a special event or in a situation in which a bike could not otherwise be checked in or out to a station.” These types of trips are edge cases and do not have associated spatial coordinates to associate them with a station point, making them useless for a spatial analysis. The below code checks for this pattern and eliminates those trips from the dataset.\n\nCheck Virtual Stations\n\n\nCode\n# identify where NAs are coming from in the dataset\nna_counts &lt;- colSums((is.na(indego)))\nna_counts[na_counts &gt; 0] %&gt;% \n  as.data.frame() %&gt;%\n  kable(col.names = c(\"Column\", \"Count\"))\n\n\n\n\n\nColumn\nCount\n\n\n\n\nstart_lat\n93\n\n\nstart_lon\n93\n\n\nend_lat\n21562\n\n\nend_lon\n21562\n\n\n\n\n\nCode\n# Confirm that station ID 3000 is always associated with NAs for lat/lon\nstation_3000 &lt;- indego %&gt;% \n  filter((start_station == 3000 | end_station == 3000))\n\ncat(\"Count of Station ID 3000 Trips\",\n    nrow(station_3000))\n\n\nCount of Station ID 3000 Trips 21768\n\n\nThe vast majority of NA coordinates for station 3000 confirms that this is primarily a method for Indego to check abandoned or improperly docked bikes back into the system. Therefore, all trips that involve station 3000 will be removed from the dataset.\n\n\nCode\n# filter out trips starting or ending from station ID 3000\nindego &lt;- indego %&gt;% \n  filter(start_station != 3000 & end_station != 3000)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\\n\\n\",\n    sep = \"\")\n\n\nCumulative Percent of Entries Removed: 1.48%\n\n\nCode\n# check for nas across all columns within the entire dataset\nifelse(anyNA(indego), \"NAs present\", \"No NAs present in the dataset\")\n\n\n[1] \"No NAs present in the dataset\"\n\n\n\n\nCalculate Time Bins\n\n\nCode\nindego &lt;- indego %&gt;% \n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    year = year(interval60),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n\n\n\nCode\n# number of trips\ncat(\"Annual Trip Count:\", nrow(indego), \"\\n\")\n\n\nAnnual Trip Count: 1449210 \n\n\nCode\n# date range\ncat(\"Date range:\", \n    as.character(min(mdy_hm(indego$start_time))), \"to\", \n    as.character(max(mdy_hm(indego$start_time))), \"\\n\")\n\n\nDate range: 2024-01-01 00:04:00 to 2025-03-31 23:48:00 \n\n\nCode\n# unique station ids across both start and end station columns\nunique_stations &lt;- unique(c(indego$start_station, indego$end_station))\ncat(\"Unique stations:\",  length(unique_stations), \"\\n\")\n\n\nUnique stations: 283 \n\n\nCode\n# trip types\nkable(table(indego$trip_route_category),\n      col.names = c(\"Trip Type\", \"Count\"))\n\n\n\n\n\nTrip Type\nCount\n\n\n\n\nOne Way\n1357754\n\n\nRound Trip\n91456\n\n\n\n\n\nCode\n# passholder types\nkable(table(indego$passholder_type),\n      col.names = c(\"Passholder Type\", \"Count\"))\n\n\n\n\n\nPassholder Type\nCount\n\n\n\n\nDay Pass\n65935\n\n\nIndego30\n828446\n\n\nIndego365\n516040\n\n\nIndegoFlex\n4\n\n\nNULL\n1146\n\n\nWalk-up\n37639\n\n\n\n\n\nCode\nbiketypes &lt;- round(table(indego$bike_type)*100 / sum(table(indego$bike_type)), 1)\nkable(biketypes,\n      col.names = c(\"Bike Type\", \"Percent\"))\n\n\n\n\n\nBike Type\nPercent\n\n\n\n\nelectric\n59.2\n\n\nstandard\n40.8\n\n\n\n\n\n\n\nTrip Duration Analysis (2024)\n\n\nCode\n# summarize trip duration by week\nweekly_summary &lt;- indego %&gt;%\n  filter(year == 2024) %&gt;% \n  group_by(week, month) %&gt;% \n  summarise(mean_dur = mean(duration),\n            median_dur = median(duration),\n            min_dur = min(duration),\n            max_dur = max(duration)) %&gt;% \n  pivot_longer(cols = colnames(.)[-c(1:2)], names_to = \"var\", values_to = \"value\")\n\n# create a faceted plot of summary statistics\nggplot() +\n  geom_line(data = weekly_summary, \n            mapping = aes(x = week, y = value),\n            color = colors[1],\n            linewidth = 1) +\n  facet_wrap(~var, scales = \"free_y\") +\n  labs(title = \"Weekly Summary Statistics of Trip Duration\", x = \"Week (1-53)\", y = \"Value\") +\n  scale_x_continuous(breaks = seq(min(indego$week), max(indego$week), 4)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTrip duration across 2024 ranged from just 1 minute to 1,440 minutes (24 hours), which likely correspond to the minimum and maximum time the bikeshare system is able to register. These trips could be the result of people removing and quickly re-docking bikes at the same station for various reasons, or people forgetting to dock or incorrectly docking their bike. This indicates that there are likely some erroneous trip logs in the data that need to be cleaned.\nMean and median duration follow the seasonal pattern of trip counts, with longer trips on average during summer weeks and shorter trips during winter weeks. Week 22 in the month of June saw the greatest mean duration of bike trips of the year at 21.6 minutes, compared to weeks 2 and 5 in January which both saw the lowest mean duration of 12.8 minutes.\n\n\nCode\nhist(log(indego$duration), breaks = 50, col = colors[2], border = colors[4], \n     main = \"Histogram of Log Duration\",\n     xlab = \"Log of Duration in Minutes\",\n     xlim = c(0,8))\n\n\n\n\n\n\n\n\n\nCode\n# x-value of the right side of distribution is roughly 5\n# e^5 is equal to 150, or 2.5 hours trip duration\n\ncumulative_pcts &lt;- data.frame(hour = seq(1:24),\n                              cumulative_pct = sapply(seq(1:24), function(x){\n                                round(sum(indego$duration &lt;= x*60)/nrow(indego)*100, 2)\n                                }))\n\nggplot(data = cumulative_pcts) +\n  geom_line(aes(x = hour, y = cumulative_pct), \n            colour = colors[3],\n            linewidth = 1) +\n  labs(title = \"Cumulative Percent of Trips Within a Given Duration\",\n       x = \"Duration (hrs)\",\n       y = \"Cumulative Pct\") +\n  scale_x_continuous(breaks = seq(0, 24, 2)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIndego bikeshare passes allow for unlimited 30- or 60-minute rides, but exceeding an hour in a single trip incurs a cost of 20-30 cents per minute extra. Given that bike trips with a duration less than 3 hours account for 99.5% of trips, with 99.9% of trips accounted for at 12 hours, it would be reasonable to suggest that any trip length beyond 12 hours (720 minutes) is well beyond the expected use case of Indego’s bikeshare network and should be considered an outlier.\n\n\nCode\n# remove trips greater than 12 hours in duration\nindego &lt;- indego %&gt;% \n  filter(duration &lt; 720)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n\n\nCumulative Percent of Entries Removed: 1.59%\n\n\n\n\nRedockings\n\n\nCode\n# identify which trips started and ended at the same station\n# note that this is a sanity check, all should have trip_route_category == \"Round Trip\"\nround_trips &lt;- indego %&gt;% \n  filter(start_station == end_station)\ntable(round_trips$trip_route_category)\n\n\n\nRound Trip \n     91156 \n\n\nCode\nround_trips_short &lt;- round_trips %&gt;% \n  filter(duration &lt; 30)\n\nhist(round_trips_short$duration,\n     breaks = 30,\n     col = colors[2],\n     border = colors[4],\n     main = \"Histogram of Round Trip Durations (&lt;30 min trips only)\",\n     xlab = \"Duration (mins)\")\n\n\n\n\n\n\n\n\n\nCode\n# simpler Empirical Cumulative Density Function (ecdf) plot used here due to lower quantity of data\necdf(round_trips_short$duration) %&gt;%\n  plot(main = \"ECDF Plot of Round Trip Duration\",\n       xlab = \"Duration (mins)\",\n       ylab = \"CDF\")\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Count of Round Trips Lasting 1 Minute: \",\n    sum(round_trips$duration == 1),\n    \"\\n\\nPercent of Round Trips Lasting 1 Minute: \",\n    sum(round_trips$duration == 1)/nrow(round_trips),\n    \"\\n\\n1-Minute Round Trips - Percent of Original Trip Count: \",\n    sum(round_trips$duration == 1)/original_trip_total*100,\n    sep = \"\")\n\n\nCount of Round Trips Lasting 1 Minute: 32726\n\nPercent of Round Trips Lasting 1 Minute: 0.3590109\n\n1-Minute Round Trips - Percent of Original Trip Count: 2.224778\n\n\nBased on an Empirical Cumulative Density Function (ECDF) plot, round trips lasting one minute or less approximately constitute a majority (~50%) of trips lasting less than 30 minutes. This category of trip also accounts for approximately 2.2% of the original trips in the dataset. This is a significant amount of trips that could end up skewing the final model results, since bike redockings are not indicative of actual bikeshare trip behavior. Redockings can often result from rider confusion and can happen more than one minute after beginning a trip, so out of an abundance of caution all round trips less than five minutes will be removed from the dataset.\n\n\nCode\n# filter out round trips less than 5 minutes long\nindego &lt;- indego %&gt;% \n  filter(!(duration &lt; 5 & trip_route_category == \"Round Trip\"))\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n\n\nCumulative Percent of Entries Removed: 4.29%"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#trip-patterns",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#trip-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Trip Patterns",
    "text": "Trip Patterns\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = colors[1], linewidth = 1) +\n  geom_smooth(se = FALSE, color = colors[3], linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - 2024\",\n    subtitle = \"Annual demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nAnnual ridership peaks at the end of summer, between the months of July and October. The least number of trips occur during winter months, such as January, declining sharply between October and January while rising at a slower rate from January to April. If multiple years of bike trips were plotted, they would likely demonstrate a sinusoidal pattern, rising and falling based on the time of year.\nThere are some notable outlying moments of high/low trips, which could be a result of major events (major sporting events or public gatherings), unseasonable temperatures (an extremely hot summer day), etc.\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPeak hourly mean bikeshare usage broadly occurs during weekdays around 7-8am and 5-6pm, demonstrating a correlation with commuting hours. Meanwhile, weekend usage has no sharp peaks in usage and instead smoothly peaks around midday hours, while also displaying greater bikeshare utilization around midnight.\n\n\nCode\n# Most popular origin stations\ntop_stations &lt;- indego %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n25,163\n\n\n3,032\n39.94527\n-75.17971\n20,128\n\n\n3,359\n39.94888\n-75.16978\n17,159\n\n\n3,295\n39.95028\n-75.16027\n16,987\n\n\n3,020\n39.94855\n-75.19007\n16,142\n\n\n3,066\n39.94561\n-75.17348\n15,681\n\n\n3,208\n39.95048\n-75.19324\n15,656\n\n\n3,244\n39.93865\n-75.16674\n15,108\n\n\n3,028\n39.94061\n-75.14958\n14,939\n\n\n3,054\n39.96250\n-75.17420\n14,793\n\n\n3,101\n39.94295\n-75.15955\n14,486\n\n\n3,012\n39.94218\n-75.17747\n14,303\n\n\n3,022\n39.95472\n-75.18323\n14,259\n\n\n3,362\n39.94816\n-75.16226\n13,982\n\n\n3,063\n39.94633\n-75.16980\n13,674\n\n\n3,185\n39.95169\n-75.15888\n13,649\n\n\n3,059\n39.96244\n-75.16121\n13,559\n\n\n3,052\n39.94732\n-75.15695\n13,517\n\n\n3,007\n39.94517\n-75.15993\n13,442\n\n\n3,161\n39.95486\n-75.18091\n13,200"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-census-data",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-census-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Census Data",
    "text": "Load Census Data\n\n\nCode\n# Get Philadelphia census tracts\nphl_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = F\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 2272)\n\n\nPerform a spatial analysis to make sure all trips are within Philly to match spatial data.\n\n\nCode\n# create a unified philadelphia geometry\nphl_boundary &lt;- phl_census %&gt;% select(-everything()) %&gt;% \n  st_union() %&gt;% \n  st_as_sf() %&gt;%\n  st_transform(2272)\n\n# create stations point shapefile \nstn_points &lt;- indego %&gt;% \n  select(start_station, start_lat, start_lon) %&gt;% \n  group_by(start_station) %&gt;% \n  slice_head(n=1) %&gt;% \n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326) %&gt;% \n  st_transform(2272) %&gt;%\n  cbind(in_phl = lengths(st_within(., phl_boundary)))\n\n# one station is not within the boundaries of philadelphia\ntable(stn_points$in_phl)\n\n\n\n  0   1 \n  1 282 \n\n\n\n\nCode\n# identify the id number of the station outside philly\nnon_phl_station &lt;- stn_points[stn_points$in_phl == 0, \"start_station\"] %&gt;% \n  st_drop_geometry() %&gt;% \n  as.numeric()\n\n# filter out trips to/from this station\nindego &lt;- indego %&gt;%\n  filter(start_station != non_phl_station & end_station != non_phl_station)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n\n\nCumulative Percent of Entries Removed: 4.29%\n\n\nCode\n# filter this station from the stn_points sf\nstn_points_filt &lt;- stn_points %&gt;% \n  filter(in_phl &gt; 0)\n\n\n\n\nCode\n# visualize stations \nggplot() +\n  geom_sf(data = phl_census, \n          color = \"grey80\",\n          fill = \"grey95\") +\n  geom_sf(data = phl_boundary,\n          color = \"grey60\",\n          fill = \"transparent\") +\n  geom_sf(data = stn_points_filt,\n          color = colors[2],\n          shape = 18,\n          alpha = 0.5) +\n  labs(title = \"Indego Bikeshare Stations in Philadelphia\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = phl_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_sf(\n    data = stn_points_filt,\n    color = \"white\", size = 0.25\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nMedian household income is not available for all census tracts. However, those that are unavailable seem to be tracts where a large concentration of commercial or industrial activity is taking place and the majority of them have no reported population. This will be explored more in the following section."
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#investigate-removing-trips-tofrom-non-residential-tracts",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#investigate-removing-trips-tofrom-non-residential-tracts",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Investigate Removing Trips To/From Non-Residential Tracts",
    "text": "Investigate Removing Trips To/From Non-Residential Tracts\n\n\nCode\nphl_tracts_valid &lt;- phl_census[complete.cases(st_drop_geometry(phl_census)), \"GEOID\"]\n\nstn_valid &lt;- stn_points %&gt;% st_filter(phl_tracts_valid, .predicate = st_within)\n\nggplot() +\n  geom_sf(data = phl_tracts_valid, color = \"grey80\", fill = \"grey95\") +\n  # Stations \n  geom_sf(\n    data = stn_points_filt,\n    aes(color = \"Non-Residential\"),\n    size = 1,\n    alpha = 0.5\n  ) +\n    geom_sf(\n    data = stn_valid,\n    aes(color = \"Residential\"), \n    size = 1,\n    alpha = 0.5\n  ) +\n  scale_color_manual(name = \"Station Type\",\n                     breaks = c(\"Residential\", \"Non-Residential\"),\n                     values = c(\"Residential\" = \"blue\", \"Non-Residential\" = \"red\")) +\n  labs(title = \"Types of Stations Based on Availability of Census Data\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nindego_valid &lt;- indego %&gt;% \n  filter(start_station %in% stn_valid$start_station & \n           end_station %in% stn_valid$start_station)\n\ncat(\"Cumulative Percent of Entries Removed: \",\n    round((1-(nrow(indego_valid)/original_trip_total))*100, 2), \"%\",\n    sep = \"\")\n\n\nCumulative Percent of Entries Removed: 25.88%\n\n\nCode\nkable(colSums(is.na(phl_census)), col.names = c(\"Variable\", \"NA Count\"))\n\n\n\n\n\nVariable\nNA Count\n\n\n\n\nGEOID\n0\n\n\nNAME\n0\n\n\nTotal_Pop\n0\n\n\nB01003_001M\n0\n\n\nMed_Inc\n25\n\n\nB19013_001M\n25\n\n\nTotal_Commuters\n0\n\n\nB08301_001M\n0\n\n\nTransit_Commuters\n0\n\n\nB08301_010M\n0\n\n\nWhite_Pop\n0\n\n\nB02001_002M\n0\n\n\nMed_Home_Value\n32\n\n\nB25077_001M\n32\n\n\ngeometry\n0\n\n\nPercent_Taking_Transit\n19\n\n\nPercent_White\n17\n\n\n\n\n\nNon-residential tracts are typically located in areas of predominately commercial, business, or industrical activity. This includes several census tracts around the city such as the Northeast Airport, University City, Center City, and South Philadelphia. In total, 28 stations (10.3% of non-virtual stations within Philadelphia) were filtered out of the dataset. When filtering out trips that started or ended at these stations, the cumulative percent of trips eliminated from the dataset jumps from 4.23% to 25.8%, a massive increase in trips lost.\nAt the risk of predicting residential trips poorer, previously identified problem variables with a large amount of NAs such as median household income and median home value will not be included in the model as is. Instead, median household income NAs for the 28 tracts in question will be reassigned values of 0 as an indicator that they are not residential. Percent taking transit and percent white variables will also get assigned zeros, since they were calculated and NAs are a result of divide by zero errors. Other economic indicators of an area such as business density will be calculated and utilized instead to supplement these in the improved model.\n\n\nCode\n# select intended census variables and replace NAs across the whole dataframe\nphl_census_select &lt;- phl_census %&gt;% \n  select(GEOID, Total_Pop, Med_Inc, Percent_Taking_Transit, Percent_White) %&gt;% \n  mutate(., across(everything(), ~ifelse(is.na(.), 0, .)))"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-additional-variables",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#load-additional-variables",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Additional Variables",
    "text": "Load Additional Variables\n\nHolidays\nDays where bikeshare usage might be higher or lower could correspond with holidays, where people either have time off and don’t need to commute or, conversely, major events could draw a lot of people to utilize the network to access them. National holidays and observances were taken from timeanddate.com while ChatGPT was consulted to aggregate multiple festival websites and develop a list of festivals, sporting events, and concerts that took place within Philadelphia in 2024.\n\n\nCode\n# load holiday xlsx sheets\nholidays_ntnl &lt;- read_xlsx(path = \"./data/holidays.xlsx\",\n                           sheet = \"national_holidays\")\nholidays_phl &lt;- read_xlsx(path = \"./data/holidays.xlsx\",\n                          sheet = \"phl_events\")\n\n# combine holiday dates and types into a single dataframe\nholidays_all &lt;- rbind(holidays_ntnl %&gt;% select(date, type),\n                      holidays_phl %&gt;% select(date, type)) %&gt;%\n  mutate(date = as.Date(date)) %&gt;%\n  arrange(date) %&gt;% \n  distinct(date, .keep_all = T)\n\n\n\n\nWeather\nComfort is an incredibly influential factor when choosing to take certain modes of transportation, and poor weather in particular can be a prominent reasons people choose to take public transportation over active transportation. Weather data will be acquired from the Philadelphia International Airport (PHL) weather station due to its proximity to Center City Philadelphia. Issues with the API call due to a corrupted CSV cell in the “metar” column for March 16th, 2024 (confirmed by directly downloading the data from Iowa Environmental Mesonet) required eliminating that variable from the call and specifying only variables that are intended to be utilized in the modeling process.\nSome measurements were taken less than 1 hr apart and had the same measurements for the selected weather variables when binned to the hour (i.e. if two measurements at 12:03pm and 12:58pm had the same temperature, when binned to 12:00pm they would be duplicated). These duplicates were removed. While no hours were missing from the dataset, some rows had NA results for wind speed that were inferred from the previous hour. Several rows exist per hour (n = 11127), so mean weather conditions for each hour were then calculated for each of the 8784 hours in the year (for 2024, a leap year: 366 days * 24 hours/day = 8784 hours).\n\n\nCode\n# get weather data from PHL airport station from Jan 1, 2024 to Dec 31, 2024\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-01-01\",\n  date_end = \"2025-04-01\", \n  data = c('tmpf', 'dwpf', 'relh', 'drct',\n           'sknt', 'p01i', 'alti', 'vsby',\n           'gust', 'wxcodes', 'feel')\n  )\n\n\n# bin to the hour, replace nas, and remove duplicated rows\nweather_processed &lt;- weather_data %&gt;% \n  mutate(interval60 = floor_date(valid, unit = \"hour\"),\n         temp = tmpf,\n         prec = ifelse(is.na(p01i), 0, p01i),\n         wspd = sknt,\n         gust = ifelse(is.na(gust), 0, gust)) %&gt;% \n  select(interval60, temp, feel, prec, relh, wspd, gust) %&gt;% \n  distinct()\n\ncat(\"Number of records after processing:\", nrow(weather_processed),\n    \"\\n\\nNumber of expected records:\", 366*24, \"\\n\\n\")\n\n\nNumber of records after processing: 13576 \n\nNumber of expected records: 8784 \n\n\nCode\n# check for missing hours and interpolate values if necessary\nweather_complete &lt;- weather_processed %&gt;% \n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;% \n  fill(temp, feel, prec, relh, wspd, gust, .direction = \"down\") %&gt;%\n  group_by(interval60) %&gt;% \n  summarise(across(everything(), mean))\n\n# check summary statistics per column\nsummary(weather_complete %&gt;% select(-interval60))\n\n\n      temp            feel             prec               relh       \n Min.   :10.00   Min.   : -5.65   Min.   :0.000000   Min.   : 16.64  \n 1st Qu.:40.00   1st Qu.: 34.73   1st Qu.:0.000000   1st Qu.: 45.05  \n Median :53.00   Median : 53.00   Median :0.000000   Median : 60.19  \n Mean   :54.79   Mean   : 52.39   Mean   :0.002949   Mean   : 61.74  \n 3rd Qu.:70.00   3rd Qu.: 70.00   3rd Qu.:0.000000   3rd Qu.: 78.94  \n Max.   :98.00   Max.   :107.23   Max.   :0.850000   Max.   :100.00  \n      wspd            gust       \n Min.   : 0.00   Min.   : 0.000  \n 1st Qu.: 5.00   1st Qu.: 0.000  \n Median : 7.00   Median : 0.000  \n Mean   : 7.84   Mean   : 3.197  \n 3rd Qu.:10.00   3rd Qu.: 0.000  \n Max.   :32.00   Max.   :55.000  \n\n\nCode\n# precipitation has a suspiciously low median and mean, isolate and confirm zero count\ncat(\"Percent of precipitation records equal to zero:\",\n    round(sum(weather_complete$prec == 0)/nrow(weather_complete)*100, 1), \"%\")\n\n\nPercent of precipitation records equal to zero: 87.9 %\n\n\n87.9% of the hourly weather records have precipitation values equal to zero. While a heavy rainstorm would likely reduce bikeshare usage more than a slight drizzle, for the purposes of this modeling exercise something like relative humidity and wind speed would together provide an indication of the strength of a storm, while precipitation can be recoded to a dummy variable indicating whether it is raining or not.\n\n\nCode\n# create a recoded column indicating whether it is raining or not\nweather_complete &lt;- weather_complete %&gt;% \n  mutate(rain = ifelse(prec &gt; 0, 1, 0) %&gt;% as.factor())\n\n\n\n\nCode\nweather_long &lt;- weather_complete %&gt;%\n  mutate(rain = as.numeric(rain)-1) %&gt;% \n  pivot_longer(cols = -c(\"interval60\", \"prec\"),\n               names_to = \"var\", values_to = \"val\")\n\n# establish labels for faceted plot\nweather_labels &lt;- c(\n  feel = \"Feels Like Temp (°F)\",\n  gust = \"Wind Gust (mph)\",\n  relh = \"Rel Humidity (%)\",\n  temp = \"Temp (°F)\",\n  wspd = \"Wind Speed (mph)\",\n  rain = \"Raining? (1=yes, 0=no)\"\n)\n\nggplot() +\n  geom_line(data = weather_long,\n            mapping = aes(x = interval60, y = val),\n            color = colors[1]) +\n  facet_wrap(facets = ~var, \n             scales = \"free_y\",\n             nrow = 3,\n             labeller = as_labeller(weather_labels)) +\n  labs(title = \"Weather Patterns for Philadelphia (2024)\",\n       x = \"Month\",\n       y = \"Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nboxplot(relh ~ rain, \n        data = weather_complete,\n        xlab = \"Raining? (1 = yes, 0 = no)\",\n        ylab = \"Relative Humidity (%)\",\n        main = \"Relative Humidity Distributions based on Rain Conditions (2024)\",\n        col = colors[1], border = \"grey30\")\n\n\n\n\n\n\n\n\n\nCode\nboxplot(wspd ~ rain, \n        data = weather_complete,\n        xlab = \"Raining? (1 = yes, 0 = no)\",\n        ylab = \"Wind Speed (mph)\",\n        main = \"Wind Speed Distributions based on Rain Conditions (2024)\",\n        col = colors[2], border = \"grey30\")\n\n\n\n\n\n\n\n\n\nCode\nplot(x = weather_complete$relh,\n     y = weather_complete$wspd,\n     pch = 16,\n     xlab = \"Relative Humidity (%)\",\n     ylab = \"Wind Speed (mph)\",\n     main = \"Wind Speed vs. Relative Humidity (2024)\",\n     col = alpha(\"black\", 0.1))\n\n\n\n\n\n\n\n\n\nWeather patterns are typically seasonal, with temperatures rising in the summer months and falling into winter months. Relative humidity, rain, and wind gusts do not seems to have any discernible relationship based on their line plots, but creating box plots of relative humidity and wind speed distributions based on rain conditions reveals that relative humidity is generally higher during rain events, while wind speed does not have any significant differences. Wind speed and relative humidity similarly do not correlate based on a scatter plot.\n\n\nOSM Commercial Density\nIn lieu of median household income and median home values as economic indicators around stations in non-residential census tracts, the density of office and shop spaces in a census tract will be used to indicate the vibrancy of economic activity within them. In a city such as Philadelphia, while there may be some enclaves of single-family detached homes with higher-than-average incomes, the vast majority of high-value homes (and therefore, high-income residents) will be located closer to Center City.\n\n\nCode\n# import a non-exhaustive list of commercial, retail, and office businesses from OSM\noffices &lt;- opq(st_bbox(phl_boundary %&gt;% st_transform(4326))) %&gt;%\n  add_osm_feature(key = \"office\") %&gt;% \n  osmdata_sf(.)\n\nshops &lt;- opq(st_bbox(phl_boundary %&gt;% st_transform(4326))) %&gt;%\n  add_osm_feature(key = \"shop\") %&gt;% \n  osmdata_sf(.)\n\n# extract points from osm object, transform to EPSG 2272, and filter to PHL boundary\noffices_pts &lt;- offices[[\"osm_points\"]] %&gt;%\n  st_transform(2272) %&gt;% \n  st_filter(phl_boundary, .predicate = st_within) %&gt;% \n  mutate(type = \"office\")\n\nshops_pts &lt;- shops[[\"osm_points\"]] %&gt;%\n  st_transform(2272) %&gt;% \n  st_filter(phl_boundary, .predicate = st_within) %&gt;% \n  mutate(type = \"shop\")\n\n# plot business locations\nggplot() +\n  geom_sf(data = phl_census, color = \"grey80\", fill = \"grey95\") +\n  geom_sf(data = offices_pts, aes(color = \"Offices\"), alpha = 0.25, size = 0.5) +\n  geom_sf(data = shops_pts, aes(color = \"Shops\"), alpha = 0.25, size = 0.5) +\n  scale_color_manual(name = \"Business Types\", values = c(\"Offices\" = colors[1], \"Shops\" = colors[2])) +\n  labs(title = \"Businesses in Philadelphia by Type\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# combine businesses into one dataset and isolate census tract geometries/GEOIDs\nbusinesses &lt;- rbind(offices_pts %&gt;% select(osm_id, name, type),\n                  shops_pts %&gt;% select(osm_id, name, type))\nphl_tracts &lt;- phl_census %&gt;% select(GEOID)\n\n# calculate business counts and densities per census tract\nbusinesses_dens &lt;- phl_tracts %&gt;% \n  mutate(business_cnt = lengths(st_intersects(., businesses)),\n         business_dens = as.numeric(business_cnt/st_area(.))*2.78784e+7)\n\n# plot business densitites spatially\nggplot() +\n  geom_sf(data = businesses_dens,\n          aes(fill = business_dens),\n          color = NA) +\n  scale_fill_viridis(name = \"Business Density (#/sqmi)\") +\n  labs(title = \"Census Tract Business Density in Philadelphia (2025)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nBusiness density is confirmed to be a decent metric for highlighting areas of high commercial activity in the city, with hot spots in Center City, Passyunk, Island Ave in West Philly, and multiple shopping districts in Northeast Philly."
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#create-space-time-panel-for-trips",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#create-space-time-panel-for-trips",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Space-Time Panel for Trips",
    "text": "Create Space-Time Panel for Trips\n\nAggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n\n[1] 777371\n\n\nCode\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n\n[1] 282\n\n\nCode\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n\n[1] 10913\n\n\n\n\nCreate Complete Panel Structure\nIn order to model over time, we need to include every hour in between the start and end date of the data. This can be accomplished by figuring out how many stations are represented in the grouped dataframe of trip counts as well as the total number of hours over the time frame (1 year).\n\n\nCode\n# find the number of stations and hours we need to represent\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(seq(min(trips_panel$interval60), max(trips_panel$interval60), by = \"hour\"))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 3,086,208 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 777,371 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 2,308,837 \n\n\nCode\n# join trip counts to expanded grid\nstudy_panel &lt;- \n  expand.grid(interval60 = seq(min(trips_panel$interval60),\n                               max(trips_panel$interval60),\n                               by = \"hour\"),\n              start_station = unique(trips_panel$start_station)\n              ) %&gt;%\n  left_join(., trips_panel %&gt;% select(-c(start_lat, start_lon)),\n            by = c(\"interval60\", \"start_station\")) %&gt;% \n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# get station lat and lon columns from indego df\nstn_coords &lt;- indego %&gt;% \n  select(start_station, start_lat, start_lon) %&gt;% \n  group_by(start_station) %&gt;% \n  slice_head(n=1) %&gt;% \n  ungroup()\n  \n# get the census tract each station is in\nstn_points_filt_panel &lt;- st_join(stn_points_filt, phl_tracts, join = st_within) %&gt;% \n  left_join(., stn_coords,\n            by = \"start_station\") %&gt;% \n  select(-in_phl) %&gt;% \n  st_drop_geometry()\n\n# fill in station-level attributes and variables\nstudy_panel &lt;- study_panel %&gt;% \n  left_join(., stn_points_filt_panel,\n            by = \"start_station\")\n\n\n\n\nCreate Time Variables\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    dotw_simple = factor(dotw,\n                         levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0) %&gt;% as.factor(),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0) %&gt;% as.factor()\n  )\n\n\n\n\nAdd Census Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;% \n  left_join(., phl_census_select %&gt;% st_drop_geometry(), by = \"GEOID\")\n\n\n\n\nAdd Business Density Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;% \n  left_join(., businesses_dens %&gt;% select(-business_cnt) %&gt;% st_drop_geometry(),\n            by = \"GEOID\")\n\n\n\n\nAdd Weather Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;% \n  left_join(weather_complete, by = \"interval60\")\n\n\n\n\nJoin Holiday Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;% \n  left_join(., holidays_all, by = \"date\") %&gt;% \n  rename(holiday = type) %&gt;% \n  mutate(holiday = replace_na(holiday, \"none\")) %&gt;% \n  mutate(holiday = factor(holiday,\n                          levels = c(\"none\",\n                                     \"holiday\",\n                                     \"festival\",\n                                     \"concert\",\n                                     \"sporting\")))"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#create-temporal-lag-variables",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#create-temporal-lag-variables",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Temporal Lag Variables",
    "text": "Create Temporal Lag Variables\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags: \",\n    format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\\n\",\n    \"Percent data loss relative to original panel dataset after removing NA lags: \",\n    round((1-nrow(study_panel_complete)/nrow(study_panel))*100, 2), \"%\", sep = \"\", \"\\n\")\n\n\nRows after removing NA lags: 3,079,440\n\nPercent data loss relative to original panel dataset after removing NA lags: 0.22%\n\n\n\n\nCode\n# isolate data from one week in a high ridership week to view lag trends\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == 3328 & week == 25)\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nRegions of the plot where overlap occurs with the current (dark blue) line are where lagged demand overlaps and is a good indicator of current demand. For example, between June 19th and 20th has significant overlap between 1 hour and 24 hour lagged demand, while between June 20th and 21st, 24-hour lagged demand is not a good predictor while 1-hour lagged demand could be."
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#temporal-traintest-split",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#temporal-traintest-split",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Train/Test Split",
    "text": "Temporal Train/Test Split\nIn order to train a model and have it predict for future data, the training dataset must have all its data points come from before the testing dataset in time. Since this is an annual dataset, and we want to train/test the model on an approximate 75/25 data split, we need to select all data from weeks 1-39 for the training dataset and from weeks 40-53 (one extra due to a leap year) in 2024. The trips from Q1 2025 also need to be isoolated in a separate dataset to test the model’s predictive ability and compare model statistics. In order for this to work, however, the datasets cannot have stations that only have trips in only one of them, and so all trip counts for these stations need to be filtered out.\n\n\nCode\nspc_2024 &lt;- study_panel_complete %&gt;% filter(year(interval60) == 2024)\nspc_2025 &lt;- study_panel_complete %&gt;% filter(year(interval60) == 2025)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- spc_2024 %&gt;%\n  filter(week &lt; 40) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- spc_2024 %&gt;%\n  filter(week &gt;= 40) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nnew_stations &lt;- spc_2025 %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;% \n  distinct(start_station) %&gt;% \n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n# eliminate trip counts from stations that only have trips in either the train/test data\nstudy_panel_complete_filt &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n  \ncat(\"Percent data loss relative to original panel dataset after removing time-limited station counts: \",\n    round((1-nrow(study_panel_complete_filt)/nrow(study_panel_complete))*100, 2), \n    \"%\", \n    sep = \"\",\n    \"\\n\")\n\n\nPercent data loss relative to original panel dataset after removing time-limited station counts: 10.64%\n\n\nCode\nspc_2024_filt &lt;- study_panel_complete_filt %&gt;% filter(year(interval60) == 2024)\nspc_2025_filt &lt;- study_panel_complete_filt %&gt;% filter(year(interval60) == 2025)\n\n# NOW create train/test split\ntrain &lt;- spc_2024_filt %&gt;%\n  filter(week &lt; 40)\n\ntest &lt;- spc_2024_filt %&gt;%\n  filter(week &gt;= 40)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\",\n    \"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\",\n    \"Training date range:\", as.character(min(train$date)), \"to\", as.character(max(train$date)), \"\\n\",\n    \"Testing date range:\", as.character(min(test$date)), \"to\", as.character(max(test$date)), \"\\n\")\n\n\nTraining observations: 1,645,056 \n Testing observations: 562,464 \n Training date range: 2024-01-02 to 2024-09-29 \n Testing date range: 2024-09-30 to 2024-12-31"
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#building-predictive-models",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#building-predictive-models",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Building Predictive Models",
    "text": "Building Predictive Models\nThis section is devoted to crafting five linear models of increasing complexity, while also exploring the addition of additional variables and conversion to a poisson regression.\n\nModel 1 - Baseline (Time + Weather)\n\n\nCode\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec,\n  data = train\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6636 -0.6086 -0.2272  0.1873 29.6544 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -4.381e-01  5.544e-03 -79.033  &lt; 2e-16 ***\nas.factor(hour)1  -4.870e-02  5.831e-03  -8.351  &lt; 2e-16 ***\nas.factor(hour)2  -6.890e-02  5.831e-03 -11.816  &lt; 2e-16 ***\nas.factor(hour)3  -8.988e-02  5.833e-03 -15.408  &lt; 2e-16 ***\nas.factor(hour)4  -7.797e-02  5.834e-03 -13.365  &lt; 2e-16 ***\nas.factor(hour)5   2.170e-02  5.835e-03   3.718 0.000201 ***\nas.factor(hour)6   2.319e-01  5.837e-03  39.737  &lt; 2e-16 ***\nas.factor(hour)7   4.666e-01  5.838e-03  79.936  &lt; 2e-16 ***\nas.factor(hour)8   7.697e-01  5.839e-03 131.820  &lt; 2e-16 ***\nas.factor(hour)9   5.678e-01  5.840e-03  97.232  &lt; 2e-16 ***\nas.factor(hour)10  4.792e-01  5.839e-03  82.070  &lt; 2e-16 ***\nas.factor(hour)11  5.066e-01  5.836e-03  86.811  &lt; 2e-16 ***\nas.factor(hour)12  5.759e-01  5.833e-03  98.730  &lt; 2e-16 ***\nas.factor(hour)13  5.783e-01  5.830e-03  99.183  &lt; 2e-16 ***\nas.factor(hour)14  5.923e-01  5.830e-03 101.589  &lt; 2e-16 ***\nas.factor(hour)15  6.818e-01  5.832e-03 116.918  &lt; 2e-16 ***\nas.factor(hour)16  8.577e-01  5.834e-03 147.006  &lt; 2e-16 ***\nas.factor(hour)17  1.114e+00  5.836e-03 190.926  &lt; 2e-16 ***\nas.factor(hour)18  8.434e-01  5.838e-03 144.483  &lt; 2e-16 ***\nas.factor(hour)19  6.192e-01  5.838e-03 106.069  &lt; 2e-16 ***\nas.factor(hour)20  3.869e-01  5.838e-03  66.279  &lt; 2e-16 ***\nas.factor(hour)21  2.600e-01  5.836e-03  44.562  &lt; 2e-16 ***\nas.factor(hour)22  1.906e-01  5.834e-03  32.664  &lt; 2e-16 ***\nas.factor(hour)23  8.924e-02  5.831e-03  15.305  &lt; 2e-16 ***\ndotw_simple2       4.842e-02  3.164e-03  15.304  &lt; 2e-16 ***\ndotw_simple3       4.205e-02  3.166e-03  13.284  &lt; 2e-16 ***\ndotw_simple4       5.635e-02  3.165e-03  17.805  &lt; 2e-16 ***\ndotw_simple5       2.052e-03  3.163e-03   0.649 0.516608    \ndotw_simple6      -7.793e-02  3.169e-03 -24.596  &lt; 2e-16 ***\ndotw_simple7      -9.705e-02  3.166e-03 -30.652  &lt; 2e-16 ***\ntemp               9.781e-03  4.923e-05 198.692  &lt; 2e-16 ***\nprec              -1.112e+00  3.396e-02 -32.731  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.079 on 1645024 degrees of freedom\nMultiple R-squared:  0.1186,    Adjusted R-squared:  0.1186 \nF-statistic:  7142 on 31 and 1645024 DF,  p-value: &lt; 2.2e-16\n\n\nThis model utilizes 0:00 (12:00am, or midnight) as the reference category for hour, with coefficients representing the relative differences in count based on whether the station is being modeled at that hour. 17:00 (5:00pm) has the greatest positive difference relative to midnight at +1.114 trips, while 04:00 (4:00am) has the greatest negative difference at -0.078 trips, indicating that midnight is an hour that does not see a lot of trips to begin with. With Monday (dotw = 1) as the reference category for days of the week, weekdays have positive coefficients while weekend days have negative coefficients, reflecting the reduction in trips that takes place over weekends on average. A 1oF increase in temperature corresponds with a 0.0098 unit increase in trips, while a 1” increase in precipitation results in a 1.11 unit decrease in trips. Every coefficient is significant except for the Friday factor variable (dotw = 5). The model’s adjusted R2 for this analysis, which will serve as a baseline value, is equal to 0.1186, indicating that 11.9% of the variation in the data can be explained by this model.\n\n\nModel 2 - Add Temporal Lag Variables\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7703 -0.4046 -0.1197  0.0930 29.6973 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -0.1776479  0.0048165 -36.883  &lt; 2e-16 ***\nas.factor(hour)1  -0.0038790  0.0050512  -0.768   0.4425    \nas.factor(hour)2   0.0082033  0.0050526   1.624   0.1045    \nas.factor(hour)3   0.0085469  0.0050560   1.690   0.0909 .  \nas.factor(hour)4   0.0298602  0.0050585   5.903 3.57e-09 ***\nas.factor(hour)5   0.1003697  0.0050607  19.833  &lt; 2e-16 ***\nas.factor(hour)6   0.2266914  0.0050669  44.740  &lt; 2e-16 ***\nas.factor(hour)7   0.3384933  0.0050778  66.662  &lt; 2e-16 ***\nas.factor(hour)8   0.4821459  0.0050966  94.602  &lt; 2e-16 ***\nas.factor(hour)9   0.2382506  0.0050891  46.816  &lt; 2e-16 ***\nas.factor(hour)10  0.2036970  0.0050720  40.161  &lt; 2e-16 ***\nas.factor(hour)11  0.2171165  0.0050719  42.808  &lt; 2e-16 ***\nas.factor(hour)12  0.2850913  0.0050685  56.248  &lt; 2e-16 ***\nas.factor(hour)13  0.2819232  0.0050683  55.624  &lt; 2e-16 ***\nas.factor(hour)14  0.2887709  0.0050687  56.972  &lt; 2e-16 ***\nas.factor(hour)15  0.3405637  0.0050745  67.113  &lt; 2e-16 ***\nas.factor(hour)16  0.4411107  0.0050896  86.668  &lt; 2e-16 ***\nas.factor(hour)17  0.5746980  0.0051173 112.304  &lt; 2e-16 ***\nas.factor(hour)18  0.3033183  0.0051136  59.316  &lt; 2e-16 ***\nas.factor(hour)19  0.1926855  0.0050901  37.855  &lt; 2e-16 ***\nas.factor(hour)20  0.0536820  0.0050890  10.549  &lt; 2e-16 ***\nas.factor(hour)21  0.0506393  0.0050688   9.990  &lt; 2e-16 ***\nas.factor(hour)22  0.0543914  0.0050588  10.752  &lt; 2e-16 ***\nas.factor(hour)23  0.0254161  0.0050512   5.032 4.86e-07 ***\ndotw_simple2       0.0013992  0.0027415   0.510   0.6098    \ndotw_simple3      -0.0160545  0.0027437  -5.851 4.88e-09 ***\ndotw_simple4      -0.0069138  0.0027428  -2.521   0.0117 *  \ndotw_simple5      -0.0456290  0.0027425 -16.638  &lt; 2e-16 ***\ndotw_simple6      -0.0840093  0.0027471 -30.581  &lt; 2e-16 ***\ndotw_simple7      -0.0715501  0.0027438 -26.077  &lt; 2e-16 ***\ntemp               0.0033068  0.0000436  75.845  &lt; 2e-16 ***\nprec              -0.5530430  0.0294434 -18.783  &lt; 2e-16 ***\nlag1Hour           0.2588848  0.0007472 346.456  &lt; 2e-16 ***\nlag3Hours          0.1100757  0.0007230 152.251  &lt; 2e-16 ***\nlag1day            0.2894345  0.0007297 396.630  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9348 on 1645021 degrees of freedom\nMultiple R-squared:  0.3388,    Adjusted R-squared:  0.3388 \nF-statistic: 2.479e+04 on 34 and 1645021 DF,  p-value: &lt; 2.2e-16\n\n\nThe adjusted R2 value for model 2 is 0.3388, a significant increase over the previous model. This was likely due to the addition of the lag variables, which represent the influence of past demand on future conditions (i.e. the demand at a station from 1 hour ago could be roughly equivalent to the demand at the station in the present).\n\n\nModel 3 - Add Demographics\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day + \n    Med_Inc + Percent_Taking_Transit + Percent_White,\n  data = train\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + temp + \n    prec + lag1Hour + lag3Hours + lag1day + Med_Inc + Percent_Taking_Transit + \n    Percent_White, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4881 -0.4097 -0.1200  0.1309 29.7623 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -2.448e-01  5.185e-03 -47.214  &lt; 2e-16 ***\nas.factor(hour)1       -5.429e-03  5.037e-03  -1.078   0.2811    \nas.factor(hour)2        5.230e-03  5.038e-03   1.038   0.2992    \nas.factor(hour)3        4.514e-03  5.042e-03   0.895   0.3706    \nas.factor(hour)4        2.527e-02  5.044e-03   5.009 5.48e-07 ***\nas.factor(hour)5        9.644e-02  5.047e-03  19.110  &lt; 2e-16 ***\nas.factor(hour)6        2.249e-01  5.053e-03  44.509  &lt; 2e-16 ***\nas.factor(hour)7        3.401e-01  5.063e-03  67.158  &lt; 2e-16 ***\nas.factor(hour)8        4.885e-01  5.083e-03  96.115  &lt; 2e-16 ***\nas.factor(hour)9        2.468e-01  5.076e-03  48.628  &lt; 2e-16 ***\nas.factor(hour)10       2.120e-01  5.058e-03  41.910  &lt; 2e-16 ***\nas.factor(hour)11       2.273e-01  5.059e-03  44.927  &lt; 2e-16 ***\nas.factor(hour)12       2.941e-01  5.055e-03  58.187  &lt; 2e-16 ***\nas.factor(hour)13       2.906e-01  5.055e-03  57.480  &lt; 2e-16 ***\nas.factor(hour)14       2.977e-01  5.055e-03  58.888  &lt; 2e-16 ***\nas.factor(hour)15       3.509e-01  5.061e-03  69.323  &lt; 2e-16 ***\nas.factor(hour)16       4.535e-01  5.077e-03  89.324  &lt; 2e-16 ***\nas.factor(hour)17       5.905e-01  5.106e-03 115.662  &lt; 2e-16 ***\nas.factor(hour)18       3.196e-01  5.102e-03  62.652  &lt; 2e-16 ***\nas.factor(hour)19       2.069e-01  5.078e-03  40.749  &lt; 2e-16 ***\nas.factor(hour)20       6.681e-02  5.076e-03  13.161  &lt; 2e-16 ***\nas.factor(hour)21       5.910e-02  5.055e-03  11.692  &lt; 2e-16 ***\nas.factor(hour)22       5.984e-02  5.045e-03  11.861  &lt; 2e-16 ***\nas.factor(hour)23       2.775e-02  5.037e-03   5.509 3.60e-08 ***\ndotw_simple2            2.933e-03  2.734e-03   1.073   0.2833    \ndotw_simple3           -1.425e-02  2.736e-03  -5.209 1.90e-07 ***\ndotw_simple4           -4.909e-03  2.735e-03  -1.795   0.0727 .  \ndotw_simple5           -4.429e-02  2.735e-03 -16.194  &lt; 2e-16 ***\ndotw_simple6           -8.418e-02  2.739e-03 -30.729  &lt; 2e-16 ***\ndotw_simple7           -7.265e-02  2.736e-03 -26.552  &lt; 2e-16 ***\ntemp                    3.533e-03  4.354e-05  81.153  &lt; 2e-16 ***\nprec                   -5.771e-01  2.936e-02 -19.655  &lt; 2e-16 ***\nlag1Hour                2.519e-01  7.486e-04 336.535  &lt; 2e-16 ***\nlag3Hours               1.019e-01  7.259e-04 140.416  &lt; 2e-16 ***\nlag1day                 2.816e-01  7.322e-04 384.589  &lt; 2e-16 ***\nMed_Inc                 3.287e-07  2.729e-08  12.045  &lt; 2e-16 ***\nPercent_Taking_Transit -2.739e-03  6.799e-05 -40.295  &lt; 2e-16 ***\nPercent_White           1.759e-03  4.181e-05  42.071  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9322 on 1645018 degrees of freedom\nMultiple R-squared:  0.3425,    Adjusted R-squared:  0.3425 \nF-statistic: 2.316e+04 on 37 and 1645018 DF,  p-value: &lt; 2.2e-16\n\n\nThe improvements in adjusted R2 after including census variables was minimal, increasing to 0.3425. Despite their statistical significance in the model, the small coefficient values for these census variables indicate that they have lesser explanatory power in the model compared to other time- and weather-based predictors.\n\n\nModel 4 - Add Station Fixed Effects\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\n\nModel 4 R-squared: 0.3637612 \n\n\nCode\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\n\nModel 4 Adj R-squared: 0.3636509 \n\n\nSlightly better improvements in adjusted R2 indicate that station fixed effects could have a meaningful influence on the model, though these small benefits might be outweighed by how computationally expensive such a model is to run.\n\n\nModel 5 - Add Rush Hour Interaction\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + temp + prec +\n    lag1Hour + lag3Hours + lag1day + rush_hour +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\n\nModel 5 R-squared: 0.367391 \n\n\nCode\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\n\nModel 5 Adj R-squared: 0.367281 \n\n\nDue to the expanded temporal range of data, the test dataset could not include all months that are represented in the train dataset. Therefore, the month predictor present in the sample code had to be removed from consideration for this analysis.\nThis model proves to not be much of an improvement (Adjusted R2 = 0.367 in model 5, compared to 0.364 in model 4). It may be reasonable to leave this interaction term between the rush hour and weekend dummy variables in the model, however, since it controls for a specific phenomenon that has a strong theoretical backing (rush hour effects on bikeshare usage are not present on the weekends).\nModel 6 - Backtracking: Additional Variables\nTo discern the potential influence of additional variables, model 6 will be an enhancement of model 3, prior to the addition of station fixed effects that make the model summary table difficult to parse.\nThe additional variables (and some variable substitutions) are as follows:\n\nBusiness Density (business_dens) - the density of shops and offices of various types per square mile\nWind Speed (wspd) - windier days make cycling more difficult, and will likely push people to other modes.\nRaining Indicator (rain) - substitution for precipitation, since any amount of rain in the day’s forecast will likely coincide with a significant reduction in bikeshare usage as people choose alternative modes.\nEvent Type (holiday) - as previously discussed, holidays could drastically reduce cycling usage as less people are commuting, while other events such as festivals and sporting events could increase it as people choose it to bypass crowded public transit or congested roads. This variable was converted to a factor to analyze the effect of each event type.\n\n\n\nCode\nmodel6 &lt;- lm(\n  Trip_Count ~ \n    as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +\n    temp + wspd + rain +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    business_dens + holiday,\n  data = train\n)\n\nsummary(model6)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + lag1Hour + \n    lag3Hours + lag1day + temp + wspd + rain + Med_Inc + Percent_Taking_Transit + \n    Percent_White + business_dens + holiday, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4893 -0.4105 -0.1196  0.1378 29.7541 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -2.222e-01  5.487e-03 -40.503  &lt; 2e-16 ***\nas.factor(hour)1       -5.637e-03  5.034e-03  -1.120  0.26274    \nas.factor(hour)2        5.741e-03  5.037e-03   1.140  0.25445    \nas.factor(hour)3        1.302e-03  5.044e-03   0.258  0.79635    \nas.factor(hour)4        2.286e-02  5.048e-03   4.529 5.92e-06 ***\nas.factor(hour)5        9.706e-02  5.051e-03  19.214  &lt; 2e-16 ***\nas.factor(hour)6        2.250e-01  5.061e-03  44.453  &lt; 2e-16 ***\nas.factor(hour)7        3.395e-01  5.070e-03  66.960  &lt; 2e-16 ***\nas.factor(hour)8        4.887e-01  5.090e-03  96.008  &lt; 2e-16 ***\nas.factor(hour)9        2.488e-01  5.083e-03  48.947  &lt; 2e-16 ***\nas.factor(hour)10       2.160e-01  5.063e-03  42.662  &lt; 2e-16 ***\nas.factor(hour)11       2.284e-01  5.058e-03  45.150  &lt; 2e-16 ***\nas.factor(hour)12       2.961e-01  5.051e-03  58.626  &lt; 2e-16 ***\nas.factor(hour)13       2.938e-01  5.051e-03  58.167  &lt; 2e-16 ***\nas.factor(hour)14       3.006e-01  5.052e-03  59.496  &lt; 2e-16 ***\nas.factor(hour)15       3.545e-01  5.060e-03  70.059  &lt; 2e-16 ***\nas.factor(hour)16       4.588e-01  5.079e-03  90.340  &lt; 2e-16 ***\nas.factor(hour)17       5.951e-01  5.112e-03 116.412  &lt; 2e-16 ***\nas.factor(hour)18       3.270e-01  5.115e-03  63.926  &lt; 2e-16 ***\nas.factor(hour)19       2.141e-01  5.092e-03  42.049  &lt; 2e-16 ***\nas.factor(hour)20       7.333e-02  5.090e-03  14.406  &lt; 2e-16 ***\nas.factor(hour)21       6.531e-02  5.067e-03  12.890  &lt; 2e-16 ***\nas.factor(hour)22       6.377e-02  5.049e-03  12.629  &lt; 2e-16 ***\nas.factor(hour)23       2.986e-02  5.036e-03   5.930 3.04e-09 ***\ndotw_simple2            7.647e-04  2.781e-03   0.275  0.78335    \ndotw_simple3           -1.149e-02  2.774e-03  -4.143 3.42e-05 ***\ndotw_simple4           -4.536e-03  2.773e-03  -1.636  0.10193    \ndotw_simple5           -4.416e-02  2.807e-03 -15.733  &lt; 2e-16 ***\ndotw_simple6           -8.201e-02  2.787e-03 -29.428  &lt; 2e-16 ***\ndotw_simple7           -6.846e-02  2.770e-03 -24.710  &lt; 2e-16 ***\nlag1Hour                2.499e-01  7.489e-04 333.722  &lt; 2e-16 ***\nlag3Hours               9.981e-02  7.263e-04 137.428  &lt; 2e-16 ***\nlag1day                 2.813e-01  7.323e-04 384.157  &lt; 2e-16 ***\ntemp                    3.376e-03  4.454e-05  75.791  &lt; 2e-16 ***\nwspd                   -1.720e-04  1.749e-04  -0.983  0.32536    \nrain1                  -9.627e-02  2.196e-03 -43.847  &lt; 2e-16 ***\nMed_Inc                 2.357e-07  2.744e-08   8.590  &lt; 2e-16 ***\nPercent_Taking_Transit -3.068e-03  6.868e-05 -44.663  &lt; 2e-16 ***\nPercent_White           1.644e-03  4.197e-05  39.176  &lt; 2e-16 ***\nbusiness_dens           8.262e-05  2.685e-06  30.773  &lt; 2e-16 ***\nholidayholiday         -4.024e-02  3.107e-03 -12.950  &lt; 2e-16 ***\nholidayfestival        -8.546e-03  3.103e-03  -2.754  0.00589 ** \nholidayconcert          2.214e-03  3.637e-03   0.609  0.54259    \nholidaysporting         3.700e-02  4.163e-03   8.889  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9313 on 1645012 degrees of freedom\nMultiple R-squared:  0.3437,    Adjusted R-squared:  0.3437 \nF-statistic: 2.003e+04 on 43 and 1645012 DF,  p-value: &lt; 2.2e-16\n\n\nUnfortunately, the results of adding additional variables to model 3 did not result in a strong increase in model performance, according to R2 values (model 6 R2 = 0.3437, model 3 R2 = 0.3425). The rain indicator variable, which was theorized to be an improvement on the continuous precipitation variable, had a significantly reduced coefficient value (model 6 “rain” estimate = -0.096, model 3 “prec” estimate = -0.57), though both are significant. The holiday variables demonstrated some predictive value, though the coefficient for concert dates was not a significant predictor (p = 0.54).\nModel 7 - Poisson Regression\n\n\nCode\nmodel7 &lt;- glm(\n  Trip_Count ~ \n    as.factor(hour) + dotw_simple + lag1Hour + lag3Hours + lag1day +\n    temp + wspd + rain +\n    Med_Inc + Percent_Taking_Transit + Percent_White +\n    business_dens + holiday,\n  family = \"poisson\",\n  data = train)\n\nsummary(model7)\n\n\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + lag1Hour + \n    lag3Hours + lag1day + temp + wspd + rain + Med_Inc + Percent_Taking_Transit + \n    Percent_White + business_dens + holiday, family = \"poisson\", \n    data = train)\n\nCoefficients:\n                         Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)            -2.863e+00  1.150e-02 -248.853  &lt; 2e-16 ***\nas.factor(hour)1       -4.520e-01  1.543e-02  -29.299  &lt; 2e-16 ***\nas.factor(hour)2       -7.527e-01  1.734e-02  -43.402  &lt; 2e-16 ***\nas.factor(hour)3       -1.350e+00  2.207e-02  -61.168  &lt; 2e-16 ***\nas.factor(hour)4       -1.202e+00  2.102e-02  -57.195  &lt; 2e-16 ***\nas.factor(hour)5       -1.793e-02  1.404e-02   -1.277  0.20144    \nas.factor(hour)6        8.510e-01  1.150e-02   73.971  &lt; 2e-16 ***\nas.factor(hour)7        1.276e+00  1.073e-02  118.867  &lt; 2e-16 ***\nas.factor(hour)8        1.556e+00  1.034e-02  150.523  &lt; 2e-16 ***\nas.factor(hour)9        1.254e+00  1.057e-02  118.621  &lt; 2e-16 ***\nas.factor(hour)10       1.164e+00  1.070e-02  108.748  &lt; 2e-16 ***\nas.factor(hour)11       1.172e+00  1.062e-02  110.317  &lt; 2e-16 ***\nas.factor(hour)12       1.293e+00  1.048e-02  123.419  &lt; 2e-16 ***\nas.factor(hour)13       1.291e+00  1.044e-02  123.625  &lt; 2e-16 ***\nas.factor(hour)14       1.287e+00  1.040e-02  123.727  &lt; 2e-16 ***\nas.factor(hour)15       1.350e+00  1.030e-02  131.162  &lt; 2e-16 ***\nas.factor(hour)16       1.431e+00  1.018e-02  140.590  &lt; 2e-16 ***\nas.factor(hour)17       1.458e+00  1.013e-02  143.935  &lt; 2e-16 ***\nas.factor(hour)18       1.239e+00  1.025e-02  120.928  &lt; 2e-16 ***\nas.factor(hour)19       1.118e+00  1.038e-02  107.658  &lt; 2e-16 ***\nas.factor(hour)20       8.558e-01  1.072e-02   79.864  &lt; 2e-16 ***\nas.factor(hour)21       7.671e-01  1.098e-02   69.838  &lt; 2e-16 ***\nas.factor(hour)22       6.671e-01  1.127e-02   59.186  &lt; 2e-16 ***\nas.factor(hour)23       4.103e-01  1.194e-02   34.347  &lt; 2e-16 ***\ndotw_simple2           -1.994e-02  3.913e-03   -5.096 3.47e-07 ***\ndotw_simple3           -4.258e-02  3.918e-03  -10.867  &lt; 2e-16 ***\ndotw_simple4           -1.134e-02  3.892e-03   -2.914  0.00356 ** \ndotw_simple5           -5.819e-02  4.016e-03  -14.489  &lt; 2e-16 ***\ndotw_simple6           -1.560e-01  4.191e-03  -37.214  &lt; 2e-16 ***\ndotw_simple7           -1.555e-01  4.203e-03  -36.984  &lt; 2e-16 ***\nlag1Hour                1.342e-01  5.601e-04  239.634  &lt; 2e-16 ***\nlag3Hours               1.021e-01  6.439e-04  158.632  &lt; 2e-16 ***\nlag1day                 1.467e-01  5.290e-04  277.311  &lt; 2e-16 ***\ntemp                    1.134e-02  7.112e-05  159.431  &lt; 2e-16 ***\nwspd                   -2.969e-03  2.640e-04  -11.243  &lt; 2e-16 ***\nrain1                  -2.407e-01  3.844e-03  -62.609  &lt; 2e-16 ***\nMed_Inc                 3.578e-07  3.746e-08    9.552  &lt; 2e-16 ***\nPercent_Taking_Transit -1.432e-02  1.291e-04 -110.938  &lt; 2e-16 ***\nPercent_White           8.373e-03  6.214e-05  134.749  &lt; 2e-16 ***\nbusiness_dens           2.844e-04  3.424e-06   83.045  &lt; 2e-16 ***\nholidayholiday         -9.309e-02  4.960e-03  -18.767  &lt; 2e-16 ***\nholidayfestival        -2.018e-03  4.798e-03   -0.420  0.67413    \nholidayconcert         -8.411e-03  4.929e-03   -1.706  0.08796 .  \nholidaysporting         1.419e-01  6.206e-03   22.866  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2600460  on 1645055  degrees of freedom\nResidual deviance: 1663486  on 1645012  degrees of freedom\nAIC: 2831298\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\n# Get predictions on test set\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test)\n  )\n\nspc_2025_filt &lt;- spc_2025_filt %&gt;% \n  mutate(\n    pred2 = predict(model2, newdata = spc_2025_filt),\n    pred6 = predict(model6, newdata = spc_2025_filt)\n  )\n  \n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Model 3 + Additional Variables\",\n    \"7. Poisson Regression of Model 6 Formula\",\n    \"8. Predicting for Q1 2025 - Model 2\",\n    \"9. Predicting for Q1 2025 - Model 6\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred2), na.rm = TRUE),\n    mean(abs(spc_2025_filt$Trip_Count - spc_2025_filt$pred6), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.63\n\n\n2. + Temporal Lags\n0.53\n\n\n3. + Demographics\n0.53\n\n\n4. + Station FE\n0.54\n\n\n5. + Rush Hour Interaction\n0.54\n\n\n6. Model 3 + Additional Variables\n0.53\n\n\n7. Poisson Regression of Model 6 Formula\n1.80\n\n\n8. Predicting for Q1 2025 - Model 2\n0.42\n\n\n9. Predicting for Q1 2025 - Model 6\n0.43\n\n\n\n\n\nCode\n# check MAE for Q1 2025\n\n\nFor 2024-data-based predictions, the poisson regression model immediately emerges as the worst in terms of Mean Absolute Error (MAE), which is likely due to the assumption of poisson regressions that the mean of the distribution equals its variance. This is not true for bikeshare trip counts, where the median and mean are close to zero but the dataset fluctuates significantly over time. The introduction of temporal lags in model 2 represent the greatest reduction in MAE across all linear model iterations. The addition of station fixed effects slightly increased MAE, highlighting how introducing complexity into the model, while increasing R2, might be a detriment to the model overall.\nModel 6 (model 3 + additional variables) was chosen as the model to predict for Q1 2025 and compare to model 2, the final model generated from the original procedure. The MAE values for both model 2 and model 6 were lower than when analyzing their respective predictions from the test dataset for 2024. It is possible that this could be a result of the limited time frame that the 2025 data represents (544,320 rows), which reduces the sample size of errors compared to the 2024 MAE values (2,207,520 rows). If either model better at predicting for late winter/early spring months due to data availability patterns that were not explored in this report, then this will be reflected in performing better for just those isolated months for 2025."
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#space-time-error-analysis",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#space-time-error-analysis",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Space-Time Error Analysis",
    "text": "Space-Time Error Analysis\n\nObserved vs. Predicted\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nModel 2 performs best for PM Rush and Evening hours, and generally better for weekdays when compared to weekends. It tends to predict worse for AM Rush and Mid-Day trips.\n\n\nCode\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred2)) %&gt;%\n  group_by(start_station, start_lat, start_lon) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = phl_census %&gt;% st_transform(4326), fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.2\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 &lt;- ggplot() +\n  geom_sf(data = phl_census %&gt;% st_transform(4326), fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = avg_demand),\n    size = 3.5,\n    alpha = 0.2\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n\n\n\n\n\n\n\n\n\nClusters of high MAE values occur towards Center City, where the average demand is greater. This is likely influenced by the large number of zeros within the dataset preventing the clean estimation of the much higher ridership stations in Center City.\n\n\nTemporal Error Patterns\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nFor the AM Rush, Evening, and PM Rush times during weekdays, model 2 has a higher MAE value. The same is true Mid-Day and Overnight times during weekends. Overnight times have the lowest MAE of all time periods, which is also likely due to the high amount of zeros present in the data (late nights have the least number of trips on average).\n\n\nError and Demographic Distributions\n\n\nCode\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(stn_points_filt_panel %&gt;% \n              select(start_station, GEOID),\n            by = \"start_station\") %&gt;% \n  left_join(phl_census_select %&gt;% \n              select(GEOID, Med_Inc, Percent_Taking_Transit, Percent_White), \n            by = \"GEOID\") %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\n\nWhile there are some trends when considering MAEs vs. demographic variables, it’s important to keep in mind that, for the most part, these relationships are very slight. For median income and percent white, high values of both tended to result in higher errors, while for stations in census tracts with lower percentages of transit users error tended to decrease."
  },
  {
    "objectID": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#final-report",
    "href": "assignments/assignment5/Sywulak-Herr_Henry_assignment5.html#final-report",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Final Report",
    "text": "Final Report\nIn general, these models - if used at all - should be used with extreme caution. A low adjusted R2 value of 0.3388 combined with relatively high MAE values of 0.53 for the top model (model 2) indicate that this model does not explain the vast majority of the existing data, nor does it predict particularly well on average. System rebalancing of the Indego bikeshare network requires accurate knowledge of where demand is highest and where supply of bikes is weakest. Prediction errors when modeling these aspects of the network would result in an incorrect distribution of bikes throughout the network. If this produces a particularly bad use experience for bikeshare users, this could push people away from utilizing the system.\nPrediction errors are loosely worse in areas with lower bikeshare usage, which here are most clearly represented as affluent, white neighborhoods that likely use other modes of transportation more frequently. This is supported by the analysis of spatial errors performed, highlighting increased prediction errors in Center City, a far more affluent and white-dominant area of the Philadelphia. Given that Indego is notoriously bad at expanding into majority-black regions of Philadelphia, and therefore this model has a deficit of input data from those communities, it’s highly likely that they are also being incorrectly modeled. This could worsen bikeshare access disparities by focusing resources in locations that wouldn’t even use bikeshare if they had access to it, while ignoring communities that could benefit from increased bikeshare access much more. A future safeguard could include weighting any models to give more emphasis to stations in or near these communities (adding a spatial lag variable to accomplish this)."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#who-we-are",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#who-we-are",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Who We Are",
    "text": "Who We Are"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#why-improve-the-model",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#why-improve-the-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Why Improve the Model?",
    "text": "Why Improve the Model?\nResearch Question\nIdentify which structural, spatial, and socio-economic predictors contribute to a more accurate Automated Valuation Model for the City of Philadelphia.\nMotivation\nImproving the accuracy of residential property tax assessment can mitigate inequity assessment methods, increase transparency in governmental processes, and make analysis more reliable and efficient"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#data-sources",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nProperty Sales: (Philadelphia, 2023-2024) City of Philadelphia - OpenDataPhilly\nSocio-Economics: United States Census - American Community Survey\nSpatial Features: City of Philadelphia - OpenDataPhilly"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#sale-prices-in-philadelphia",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#sale-prices-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Sale Prices in Philadelphia",
    "text": "Sale Prices in Philadelphia\nHigher sale prices are concentrated in Central and Northwest Philadelphia"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#factors-impacting-sale-price",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#factors-impacting-sale-price",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Factors Impacting Sale Price",
    "text": "Factors Impacting Sale Price\nThere is a notable relationship between sale price and total livable area"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-comparison",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-comparison",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nModel Performance Metrics\n\n\nModel\nRMSE\nMAE\nR²\n\n\n\n\nModel 1\n230,140.41\n15,174.13\n0.287\n\n\nModel 2\n215,430.39\n90,217.97\n0.380\n\n\nModel 3\n211,021.39\n90,144.74\n0.405\n\n\nModel 4\n196,081.17\n71,582.45\n0.490\n\n\n\n\nModel 1 (Structural) and Model 2 (Structural + Census) had the worst performance.\nSpatial features (Model 3) and interaction terms (Model 4) boosted model performance."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#top-predictors",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#top-predictors",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Top Predictors",
    "text": "Top Predictors\n\nIn a wealthy neighborhood (\\(\\beta\\) = 50,306.450, p &lt; 0.01)\nNumber of bedrooms (\\(\\beta\\) = 33,544.290, p &lt; 0.01)\nNumber of bathrooms (\\(\\beta\\) = 29,510.940, p &lt; 0.01)\n\nInterpretation: Higher home values in wealthier neighborhoods make sense, especially historically affluent areas with reputational appeal and historic housing stock. Higher numbers of bedrooms and bathrooms tend to indicate more total livable area, which aligns with our observation that total livable area impacts sale price."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance",
    "text": "Model Performance"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance-by-neighborhood-part-1",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance-by-neighborhood-part-1",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 1)",
    "text": "Model Performance by Neighborhood (Part 1)\n\nNeighborhoods in North Philadelphia and West Philadelphia (with the exception of University City), in addition to Chinatown are underpredicted.\nCenter City and parts of Northeast and Northwest Philadelphia are overpredicted."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance-by-neighborhood-part-2",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#model-performance-by-neighborhood-part-2",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 2)",
    "text": "Model Performance by Neighborhood (Part 2)\n\nThis suggests local factors not included in the model are having an impact on sale prices.\nA particular area of concern is the underpredicting of sale prices in lower-income neighborhoods in North Philadelphia."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#recommendations",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Recommendations",
    "text": "Recommendations\n\nPolicymakers should be aware that houses in lower-income neighborhoods will not be accurately predicted by this model.\nPhysical features of a structure such as the number of bedrooms/bathrooms serve as strong predictors of home sale price. Nearby attractive amenities such as proximity to transit stations as well as distance from crime also contribute."
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#limitations-next-steps",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\n\nLimitations\n\nPhiladelphia sale prices don’t have a linear relationship, particularly among lower-priced homes\nVariables used were aggregated and not weighted\n\n\nNext Steps\n\nUpdate the model to account for lower neighborhoods and tailor spatial features to add texture and depth to the model\nConsider more layered data cleaning, given that we minimally cleaned the data in order to preserve complex property types (mixed-use etc.)"
  },
  {
    "objectID": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#questions",
    "href": "assignments/midterm/Carlsen_Drake_Kakumanu_Raju_Robinson_Sywulak-Herr_Presentation.html#questions",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Questions?",
    "text": "Questions?\nThank you! from Sujan, Henry, Ryan, Kavana, Chloe, and Nina :)   Contact us at: inquiries@tnc.com"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner.\n\n\n\n\n\n\nRun the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC:\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#your-role",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#your-role",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner."
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#instructions",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#instructions",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "Run the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC:\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#consulting-team-information",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#consulting-team-information",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "Consulting Team Information",
    "text": "Consulting Team Information\nClever Team Name: _____________\nTeam Members:"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#technical-assessment",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#technical-assessment",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "1. TECHNICAL ASSESSMENT",
    "text": "1. TECHNICAL ASSESSMENT\n\nModel Performance Metrics\nAUC (Area Under ROC Curve): 0.732\nAt threshold = 0.50:\n\nSensitivity (True Positive Rate): 0.817\nSpecificity (True Negative Rate): 0.492\nPrecision (Positive Predictive Value): 0.706\nOverall Accuracy: 0.687\n\n\n\nTechnical Quality Rating\nSelect one:\n\nExcellent (AUC &gt; 0.90)\nGood (AUC 0.80-0.90)\nAcceptable (AUC 0.70-0.80)\nPoor (AUC &lt; 0.70)\n\n\n\nBrief Technical Summary (2-3 sentences)\nIs the model accurate enough for high-stakes decision-making?\nNo, it is not. Despite a high specificity, the model has a low specificity (i.e. flagging a lot of people who did not commit a crime after release as having done so) and a relatively poor accuracy of 68.7%, meaning that the outcomes of approximately one-third of the population will be incorrectly modeled."
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#equity-analysis",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#equity-analysis",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "2. EQUITY ANALYSIS",
    "text": "2. EQUITY ANALYSIS\n\nFalse Positive Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Positive Rate\nSample Size\n\n\n\n\nBlack:\n0.562\n3931\n\n\nWhite:\n0.425\n2620\n\n\nOverall:\n0.508\n6551\n\n\n\n\n\nFalse Negative Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Negative Rate\nSample Size\n\n\n\n\nBlack:\n0.154\n3931\n\n\nWhite:\n0.227\n2620\n\n\nOverall:\n0.183\n6551\n\n\n\n\n\nDisparity Analysis\nLargest disparity identified:\nBlack individuals have a false positive rate 13.7 percentage points higher than White individuals.\nOR\nWhite individuals have a false negative rate 7.3 percentage points higher than Black individuals.\n\n\nEquity Concerns Summary (3-4 sentences)\nWhat are the implications of these disparities? Who is harmed?"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "3. THRESHOLD RECOMMENDATION",
    "text": "3. THRESHOLD RECOMMENDATION\n\nIf we deploy this model, we recommend:\nSelect one:\n\nThreshold = 0.30 (Aggressive - prioritize catching recidivists)\nThreshold = 0.50 (Balanced - default)\nThreshold = 0.70 (Conservative - minimize false accusations)\nOther: ________\n\n\n\nRationale for Threshold Choice (3-4 sentences)\nWhy this threshold? What does it optimize for? What are the trade-offs?\n\n\n\n\n\n\nThis threshold prioritizes:\nSelect one:\n\nHigh Sensitivity - Catch more people who will reoffend (accept more false positives)\nHigh Specificity - Avoid false accusations (accept more false negatives)\nBalance - Try to minimize both types of errors"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "4. DEPLOYMENT RECOMMENDATION",
    "text": "4. DEPLOYMENT RECOMMENDATION\n\nOur recommendation to Georgia DOC:\nSelect one:\n\nDEPLOY - Use this model to inform parole decisions\nDO NOT DEPLOY - Do not use this model\nCONDITIONAL DEPLOY - Deploy only with specific safeguards in place\n\n\n\nKey Reasons for Our Recommendation\nProvide 3-5 bullet points supporting your decision:\n\n\n\n\n\n\n\n\n\nWhat about the equity concerns?\nHow do you justify your recommendation given the disparate impact you identified?"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "5. SAFEGUARDS OR ALTERNATIVES",
    "text": "5. SAFEGUARDS OR ALTERNATIVES\n\nIf DEPLOY - Required Safeguards\nWhat protections must be in place before deployment?\n\n\n\n\n\n\nOR\n\n\nIf DO NOT DEPLOY - Alternative Approaches\nWhat should Georgia DOC do instead?"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "6. LIMITATIONS & UNCERTAINTIES",
    "text": "6. LIMITATIONS & UNCERTAINTIES\n\nWhat we don’t know (but wish we did)\nWhat additional information would strengthen your recommendation?\n\n\n\n\n\nWeaknesses in our recommendation\nWhat’s the strongest argument AGAINST your recommendation?"
  },
  {
    "objectID": "labs/lab_week10/In_Class_Exercise_Instruction.html#bottom-line",
    "href": "labs/lab_week10/In_Class_Exercise_Instruction.html#bottom-line",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "7. BOTTOM LINE",
    "text": "7. BOTTOM LINE\n\nOne-Sentence Recommendation\nIf the Commissioner only reads one thing, what should it be?"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts from lecture\n\nBasic functionalities of GitHub\nIntroduction to Quarto as a presentation tool\nUse cases of R for Public Policy Analytics\n\nTechnical skills covered\n\nCommitting, Pushing changes to, and Pulling changes from a repository in GitHub\nIntroductory data manipulation functions in R"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nNew R functions or approaches\n\nfilter → subset rows\nselect → subset columns\nmutate → create new variable columns\nsummarize → perform operations on a grouped dataframe\ngroup_by → create a grouped dataframe based on a column\nrename → rename columns (use `newname`)\n%&gt;% (Piping) → tidyverse method of passing function outputs to another function\n\nQuarto features learned\n\nVarious markdown style shortcuts:\n\nBold = **\nItalic = *\nBold/Italic = ***\nCode Text = `\nLists = -\nHeaders = # (x2 or x3)\nLinks = [Link text] (link.com)\n\nRendering the webpage as we make edits\n\nChanging Global Options in RStudio to render on save\nUsing the “Render” button to force rendering updates"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhat I didn’t fully understand\n\nThe nuances of GitHub, but I think I’m getting most of it\n\nAreas needing more practice\n\nUsing GitHub"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nHow this week’s content applies to real policy work\n\nData manipulation in R will be useful for taking in data from web pages and restructuring it into a format that’ll be useful for modeling"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nWhat was most interesting\n\nLearning how to use GitHub and publish our pages\n\nHow I’ll apply this knowledge\n\nUsing GitHub for collaborative projects in the future"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#data-visualization",
    "href": "weekly-notes/week-03-notes.html#data-visualization",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "",
    "text": "Anscombe’s Quartet → four datasets with identical summary statistics, but in various formats (linear, scattered, parabolic, and concentrated)\nVisualizing how data is arranged is just as important as knowing the statistics behind datasets in order to see these differences\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nACS data is inherently unreliable, and bad visualizations have real consequences\n\nMisleading scales/axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "href": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\nWhen making a ggplot, the elements go as follows:\n\nData → what is your dataset?\nAesthetics → what variables map to visual properties?\nGeometries → how do you want to display the data?\nAdditional Layers → scales, themes, facets, annotations, etc.\nBroadly:\n\nggplot(data = your_data) +\n\naes(x = variable1, y = variable2) +\ngeom_something() +\nadditional_layers()\n\n\n\nArguments\n\nx, y → position\ncolor → point/line color\nfill → area fill color\nsize → point/line size\nshape → point shape\nalpha → transparency\nAethetics go inside aes(), constants go outside\nAesthetics are not for decorating, they are for changing data-related elements of your plot (WILL BE ON QUIZ)"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "href": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nIt’s like detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "href": "weekly-notes/week-03-notes.html#r-tips-from-this-week",
    "title": "Week 3 Notes - Intro to ggplot",
    "section": "R Tips from This Week",
    "text": "R Tips from This Week\n\nThe regex() function can generate regex expressions based on a string input."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes - Linear Regression",
    "section": "",
    "text": "Root Mean Squared Error →\nMAE\nCross Validation\nTrain/Testing Dataset\nLinear Regression Basics\nIndependent Variables NOT collinear\nVIF (&lt;5-10)\nConstant Variance (Heteroscedasticity)\nOutliers (Cooks Distance)\nNormality of Residuals"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#topics-discussed",
    "href": "weekly-notes/week-05-notes.html#topics-discussed",
    "title": "Week 5 Notes - Linear Regression",
    "section": "",
    "text": "Root Mean Squared Error →\nMAE\nCross Validation\nTrain/Testing Dataset\nLinear Regression Basics\nIndependent Variables NOT collinear\nVIF (&lt;5-10)\nConstant Variance (Heteroscedasticity)\nOutliers (Cooks Distance)\nNormality of Residuals"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Remove all progress bars and cluttered outputs from assignment documents\n\nFor tidycensus/tigris → “progress_bar = F” or something similar"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#homework-feedback-tips",
    "href": "weekly-notes/week-07-notes.html#homework-feedback-tips",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Remove all progress bars and cluttered outputs from assignment documents\n\nFor tidycensus/tigris → “progress_bar = F” or something similar"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#understanding-spatial-patterns-in-errors",
    "href": "weekly-notes/week-07-notes.html#understanding-spatial-patterns-in-errors",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Understanding Spatial Patterns in Errors",
    "text": "Understanding Spatial Patterns in Errors\n\nRandom errors are good, clustered errors are bad\n\nWe want to see models randomly predict poorly, not see patterns in how they predict poorly\nThe latter is an indicator of spatial autocorrelation\n\nSpatial Lag Plot\n\nChecking the relationship of variables to their 5 nearest neighbors, for example\nUse the “spdep” package in R"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#morans-i",
    "href": "weekly-notes/week-07-notes.html#morans-i",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Moran’s I",
    "text": "Moran’s I\n\nRange from -1 to +1\n\n+1 → perfect positive correlation (clustering)\n0 → random spatial pattern\n-1 → perfect negative correlation (dispersion)\n\n\n\\[I = \\frac{n \\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i \\sum_j w_{ij} \\sum_i (x_i - \\bar{x})^2}\\] - The Moran’s I formula exaggerates values that are both similarly placed relative to the mean - If there are large positive errors next to each other or large negative errors close to each other → positive numerator - If there are errors above and below the mean next to each other, multiplying them together yields a negative value - This works to bring Moran’s I back closer to 0 - Neighbors & Calculating Spatial Lag - Queen → all neighbors surrounding a feature - If a 3x3 grid, all 8 cells surrounding the central cell) - Rook → the neighbors in the four cardinal directions relative to a feature - If a 3x3 grid, the four cells touching the faces of the central cell - Spatial Lag could be simply the average value of neighbors - Computing Moran’s I - moran.mc() in R - If Moran’s I is high (errors are clustered): - Add more spatial features - Try introducing spatial fixed effects"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html",
    "href": "weekly-notes/week-10-notes.html",
    "title": "Week 10 Notes - Logistic Regression",
    "section": "",
    "text": "Logistic Regression\n\nThe logit is equal to taking the log of the odds ratio\n\nOR &gt; 1 = greater odds of outcome\nOR &lt; 1 = lower odds of outcome\n\nOutput values are probabilities of an event occurring\n\nChoosing a cutoff is an imperfect art\nIf you classify a potential spam email with a probability of 0.72 as spam, but it actually was not, what does that say about the cutoff? Confusion Matrices\n\nSensitivity = TP / TP + FN\nSpecificity = TN / TN + FP\nPrecision = TP / TP + FP\nAccuracy = TP + TN / Total ROC Curve\n“Receiver Operating Characteristic”\nAs you go up the y-axis you capture more true positives, as you go across the x-axis you are capturing more false positives (units are fractions of total = proportion)\nIf you choose a true positive rate threshold (coords: 0.35, 0.80), you can say that if you achieve an 80% true positive rate you will also have a 35% false positive rate\nGoal is to maximize AUC (area under the curve) value Handling Large Amounts of Zeros\n“Cooking the books” is to just remove the zeros"
  }
]